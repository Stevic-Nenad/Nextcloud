Directory Structure:
└── .
    ├── .github
    │   └── workflows
    │       ├── lifecycle.yml
    │       ├── main-push-deploy.yml
    │       └── reusable-deploy-app.yml
    ├── assets
    │   ├── gantt.svg
    │   ├── header.png
    │   ├── logische-gesamtarchitektur.svg
    │   └── netzwerkarchitektur.svg
    ├── backend
    │   ├── .terraform
    │   │   └── providers
    │   │       └── registry.terraform.io
    │   │           └── hashicorp
    │   │               └── aws
    │   │                   └── 5.99.1
    │   │                       └── windows_amd64
    │   │                           ├── LICENSE.txt
    │   │                           └── terraform-provider-aws_v5.99.1_x5.exe
    │   ├── .terraform.lock.hcl
    │   ├── locals.tf
    │   ├── main.tf
    │   ├── provider.tf
    │   ├── terraform.tfstate
    │   ├── terraform.tfstate.backup
    │   └── variables.tf
    ├── charts
    │   └── nextcloud-chart
    │       ├── Chart.yaml
    │       ├── templates
    │       │   ├── configmap.yaml
    │       │   ├── deployment.yaml
    │       │   ├── NOTES.txt
    │       │   ├── pvc.yaml
    │       │   ├── secret.yaml
    │       │   ├── service.yaml
    │       │   ├── tests
    │       │   │   └── test-connection.yaml
    │       │   └── _helpers.tpl
    │       └── values.yaml
    ├── docs
    │   └── templates
    │       ├── daily_scrum.md
    │       ├── sprint_planning.md
    │       ├── sprint_retro.md
    │       ├── sprint_review.md
    │       └── user_story.md
    ├── LICENSE
    ├── README.md
    ├── terraform
    │   ├── .terraform
    │   │   ├── providers
    │   │   │   └── registry.terraform.io
    │   │   │       └── hashicorp
    │   │   │           ├── aws
    │   │   │           │   └── 5.99.1
    │   │   │           │       └── windows_amd64
    │   │   │           │           ├── LICENSE.txt
    │   │   │           │           └── terraform-provider-aws_v5.99.1_x5.exe
    │   │   │           └── tls
    │   │   │               └── 4.1.0
    │   │   │                   └── windows_amd64
    │   │   │                       ├── LICENSE.txt
    │   │   │                       └── terraform-provider-tls_v4.1.0_x5.exe
    │   │   └── terraform.tfstate
    │   ├── .terraform.lock.hcl
    │   ├── backend.tf
    │   ├── ecr.tf
    │   ├── eks_addons.tf
    │   ├── eks_cluster.tf
    │   ├── eks_nodegroup.tf
    │   ├── iam_cicd.tf
    │   ├── iam_eks.tf
    │   ├── iam_irsa.tf
    │   ├── launch_template.tf
    │   ├── locals.tf
    │   ├── network.tf
    │   ├── outputs.tf
    │   ├── provider.tf
    │   ├── rds.tf
    │   ├── security_group.tf
    │   ├── terraform.tfstate
    │   ├── terraform.tfstate.backup
    │   ├── variable.tf
    │   └── versions.tf
    └── terraform.tfstate

Summary:
Total files analyzed: 59
Total directories analyzed: 28
Estimated output size: 473.13 KB
Actual analyzed size: 1418186.05 KB
Total tokens: 130072
Actual text content size: 460.06 KB

File Contents:

==================================================
File: .\.github\workflows\lifecycle.yml
==================================================
# This workflow orchestrates the entire lifecycle of the environment.
# It is triggered manually, providing a "one-click" way to build or destroy everything.

name: Full Environment Lifecycle

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform: "setup" to build infra and deploy, or "destroy" to tear down.'
        required: true
        type: choice
        options:
          - setup
          - destroy

jobs:
  # ==================================================================================
  # == SETUP PATH: Triggered when 'setup' is selected                             ==
  # ==================================================================================
  terraform-apply:
    name: "Setup: 1. Terraform Apply"
    if: github.event.inputs.action == 'setup'
    runs-on: ubuntu-latest
    outputs:
      # These outputs are passed to subsequent jobs.
      eks_cluster_name: ${{ steps.tf_outputs.outputs.eks_cluster_name }}
      rds_host: ${{ steps.tf_outputs.outputs.rds_host }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform -chdir=./terraform init

      - name: Terraform Apply
        run: terraform -chdir=./terraform apply -auto-approve

      - name: Get Terraform Outputs for next job
        id: tf_outputs
        run: |
          cd ./terraform
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "rds_host=$(terraform output -raw rds_instance_endpoint)" >> $GITHUB_OUTPUT

  deploy-application:
    name: "Setup: 2. Deploy Application"
    if: github.event.inputs.action == 'setup'
    needs: terraform-apply # Depends on the successful infrastructure creation
    uses: ./.github/workflows/reusable-deploy-app.yml@main
    with:
      EKS_CLUSTER_NAME: ${{ needs.terraform-apply.outputs.eks_cluster_name }}
      RDS_DB_HOST: ${{ needs.terraform-apply.outputs.rds_host }}
    secrets: inherit # Pass all secrets down to the reusable workflow

  # ==================================================================================
  # == DESTROY PATH: Triggered when 'destroy' is selected                           ==
  # ==================================================================================
  get-destroy-data:
    name: "Teardown: 1. Get Data"
    if: github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    outputs:
      eks_cluster_name: ${{ steps.tf_outputs.outputs.eks_cluster_name }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
      - name: Terraform Init
        run: terraform -chdir=./terraform init
      - name: Get Terraform Outputs
        id: tf_outputs
        run: |
          cd ./terraform
          # '|| true' ensures this step succeeds even if the infra doesn't exist, making the job fault-tolerant.
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name || true)" >> $GITHUB_OUTPUT

  helm-uninstall:
    name: "Teardown: 2. Uninstall Application"
    if: github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    needs: get-destroy-data
    steps:
      - name: Configure AWS Credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Configure Kubectl
        if: needs.get-destroy-data.outputs.eks_cluster_name != ''
        run: |
          aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ needs.get-destroy-data.outputs.eks_cluster_name }}
          echo "Kubeconfig configured for cluster: ${{ needs.get-destroy-data.outputs.eks_cluster_name }}"
      - name: Setup Helm
        uses: azure/setup-helm@v4
      - name: Uninstall Helm Release
        if: needs.get-destroy-data.outputs.eks_cluster_name != ''
        run: |
          # '|| true' ensures this step succeeds even if the Helm release is already gone.
          helm uninstall nextcloud -n nextcloud || true
          # IMPORTANT: Wait to allow AWS to de-register the Load Balancer before Terraform tries to delete its dependencies.
          echo "Waiting 60 seconds for Load Balancer to de-register..."
          sleep 60

  terraform-destroy:
    name: "Teardown: 3. Terraform Destroy"
    if: github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    needs: helm-uninstall
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
      - name: Terraform Init
        run: terraform -chdir=./terraform init
      - name: Terraform Destroy
        run: terraform -chdir=./terraform destroy -auto-approve

==================================================
File: .\.github\workflows\main-push-deploy.yml
==================================================
name: Deploy on Main Push

on:
  push:
    branches:
      - master

jobs:
  get-infra-data:
    runs-on: ubuntu-latest
    outputs:
      eks_cluster_name: ${{ steps.tf_outputs.outputs.eks_cluster_name }}
      rds_host: ${{ steps.tf_outputs.outputs.rds_host }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
      - name: Terraform Init
        run: terraform -chdir=./terraform init
      - name: Get Terraform Outputs
        id: tf_outputs
        run: |
          cd ./terraform
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "rds_host=$(terraform output -raw rds_instance_endpoint)" >> $GITHUB_OUTPUT

  call-reusable-deployment:
    name: "Deploy Application"
    needs: get-infra-data
    uses: ./.github/workflows/reusable-deploy-app.yml@main
    with:
      EKS_CLUSTER_NAME: ${{ needs.get-infra-data.outputs.eks_cluster_name }}
      RDS_DB_HOST: ${{ needs.get-infra-data.outputs.rds_host }}
    secrets: inherit # 'inherit' is a shortcut to pass all secrets

==================================================
File: .\.github\workflows\reusable-deploy-app.yml
==================================================
name: Reusable - Deploy Nextcloud Application

# This workflow is now triggered by other workflows, not directly by git events.
on:
  workflow_call:
    # Define the inputs this reusable workflow expects
    inputs:
      EKS_CLUSTER_NAME:
        required: true
        type: string
      RDS_DB_HOST:
        required: true
        type: string
    # Define the secrets this reusable workflow needs access to
    secrets:
      AWS_ACCOUNT_ID:
        required: true
      AWS_REGION:
        required: true
      CICD_ROLE_NAME:
        required: true
      RDS_DB_PASSWORD:
        required: true
      NEXTCLOUD_ADMIN_PASSWORD:
        required: true

jobs:
  deploy-application:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.CICD_ROLE_NAME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Configure Kubectl
        run: |
          # Use the cluster name passed as an input
          aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ inputs.EKS_CLUSTER_NAME }}
          echo "Kubeconfig configured for cluster: ${{ inputs.EKS_CLUSTER_NAME }}"
          kubectl get nodes

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: 'v3.15.2'

      - name: Lint Helm Chart
        run: helm lint ./charts/nextcloud-chart

      - name: Deploy or Upgrade Nextcloud with Helm
        run: |
          helm upgrade --install nextcloud ./charts/nextcloud-chart \
            --namespace nextcloud --create-namespace \
            --wait --timeout 10m \
            --set database.host="${{ inputs.RDS_DB_HOST }}" \
            --set database.password="${{ secrets.RDS_DB_PASSWORD }}" \
            --set nextcloud.admin.password="${{ secrets.NEXTCLOUD_ADMIN_PASSWORD }}"

      - name: Get Load Balancer Hostname and Finalize Configuration
        run: |
          HOSTNAME=""
          for i in {1..30}; do
            HOSTNAME=$(kubectl get svc nextcloud -n nextcloud -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            if [ -n "$HOSTNAME" ]; then
              echo "Load Balancer found: $HOSTNAME"
              break
            fi
            echo "Waiting for Load Balancer hostname... (Attempt $i/30)"
            sleep 10
          done
          if [ -z "$HOSTNAME" ]; then
            echo "::error::Failed to get Load Balancer hostname after 5 minutes."
            exit 1
          fi
          helm upgrade --install nextcloud ./charts/nextcloud-chart \
            --namespace nextcloud \
            --reuse-values \
            --set nextcloud.host="$HOSTNAME"

      - name: Run Helm Tests
        if: success()
        run: |
          for i in {1..3}; do
            if helm test nextcloud -n nextcloud; then
              echo "Helm tests passed on attempt $i."
              exit 0
            fi
            echo "Helm tests failed on attempt $i. Retrying in 15 seconds..."
            sleep 15
          done
          echo "::error::Helm tests failed after 3 attempts."
          exit 1

==================================================
File: .\assets\gantt.svg
==================================================
<svg id="mermaid-svg" width="100%" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1904 672" style="max-width: 1904px;" role="graphics-document document" aria-roledescription="gantt" xmlns:xlink="http://www.w3.org/1999/xlink"><style xmlns="http://www.w3.org/1999/xhtml">@import url("https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css");</style><style>#mermaid-svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#ccc;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#mermaid-svg .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#mermaid-svg .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#mermaid-svg .error-icon{fill:#a44141;}#mermaid-svg .error-text{fill:#ddd;stroke:#ddd;}#mermaid-svg .edge-thickness-normal{stroke-width:1px;}#mermaid-svg .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-svg .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg .marker{fill:lightgrey;stroke:lightgrey;}#mermaid-svg .marker.cross{stroke:lightgrey;}#mermaid-svg svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg p{margin:0;}#mermaid-svg .mermaid-main-font{font-family:"trebuchet ms",verdana,arial,sans-serif;}#mermaid-svg .exclude-range{fill:hsl(52.9411764706, 28.813559322%, 48.431372549%);}#mermaid-svg .section{stroke:none;opacity:0.2;}#mermaid-svg .section0{fill:hsl(52.9411764706, 28.813559322%, 58.431372549%);}#mermaid-svg .section2{fill:#EAE8D9;}#mermaid-svg .section1,#mermaid-svg .section3{fill:#333;opacity:0.2;}#mermaid-svg .sectionTitle0{fill:#F9FFFE;}#mermaid-svg .sectionTitle1{fill:#F9FFFE;}#mermaid-svg .sectionTitle2{fill:#F9FFFE;}#mermaid-svg .sectionTitle3{fill:#F9FFFE;}#mermaid-svg .sectionTitle{text-anchor:start;font-family:"trebuchet ms",verdana,arial,sans-serif;}#mermaid-svg .grid .tick{stroke:lightgrey;opacity:0.8;shape-rendering:crispEdges;}#mermaid-svg .grid .tick text{font-family:"trebuchet ms",verdana,arial,sans-serif;fill:#ccc;}#mermaid-svg .grid path{stroke-width:0;}#mermaid-svg .today{fill:none;stroke:#DB5757;stroke-width:2px;}#mermaid-svg .task{stroke-width:2;}#mermaid-svg .taskText{text-anchor:middle;font-family:"trebuchet ms",verdana,arial,sans-serif;}#mermaid-svg .taskTextOutsideRight{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%);text-anchor:start;font-family:"trebuchet ms",verdana,arial,sans-serif;}#mermaid-svg .taskTextOutsideLeft{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%);text-anchor:end;}#mermaid-svg .task.clickable{cursor:pointer;}#mermaid-svg .taskText.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-svg .taskTextOutsideLeft.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-svg .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-svg .taskText0,#mermaid-svg .taskText1,#mermaid-svg .taskText2,#mermaid-svg .taskText3{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%);}#mermaid-svg .task0,#mermaid-svg .task1,#mermaid-svg .task2,#mermaid-svg .task3{fill:hsl(180, 1.5873015873%, 35.3529411765%);stroke:#ffffff;}#mermaid-svg .taskTextOutside0,#mermaid-svg .taskTextOutside2{fill:lightgrey;}#mermaid-svg .taskTextOutside1,#mermaid-svg .taskTextOutside3{fill:lightgrey;}#mermaid-svg .active0,#mermaid-svg .active1,#mermaid-svg .active2,#mermaid-svg .active3{fill:#81B1DB;stroke:#ffffff;}#mermaid-svg .activeText0,#mermaid-svg .activeText1,#mermaid-svg .activeText2,#mermaid-svg .activeText3{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%)!important;}#mermaid-svg .done0,#mermaid-svg .done1,#mermaid-svg .done2,#mermaid-svg .done3{stroke:grey;fill:lightgrey;stroke-width:2;}#mermaid-svg .doneText0,#mermaid-svg .doneText1,#mermaid-svg .doneText2,#mermaid-svg .doneText3{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%)!important;}#mermaid-svg .crit0,#mermaid-svg .crit1,#mermaid-svg .crit2,#mermaid-svg .crit3{stroke:#E83737;fill:#E83737;stroke-width:2;}#mermaid-svg .activeCrit0,#mermaid-svg .activeCrit1,#mermaid-svg .activeCrit2,#mermaid-svg .activeCrit3{stroke:#E83737;fill:#81B1DB;stroke-width:2;}#mermaid-svg .doneCrit0,#mermaid-svg .doneCrit1,#mermaid-svg .doneCrit2,#mermaid-svg .doneCrit3{stroke:#E83737;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#mermaid-svg .milestone{transform:rotate(45deg) scale(0.8,0.8);}#mermaid-svg .milestoneText{font-style:italic;}#mermaid-svg .doneCritText0,#mermaid-svg .doneCritText1,#mermaid-svg .doneCritText2,#mermaid-svg .doneCritText3{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%)!important;}#mermaid-svg .activeCritText0,#mermaid-svg .activeCritText1,#mermaid-svg .activeCritText2,#mermaid-svg .activeCritText3{fill:hsl(28.5714285714, 17.3553719008%, 86.2745098039%)!important;}#mermaid-svg .titleText{text-anchor:middle;font-size:18px;fill:#F9FFFE;font-family:"trebuchet ms",verdana,arial,sans-serif;}#mermaid-svg :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g/><g class="grid" transform="translate(300, 622)" fill="none" font-size="10" font-family="sans-serif" text-anchor="middle"><path class="domain" stroke="currentColor" d="M0.5,-577V0.5H1529.5V-577"/><g class="tick" opacity="1" transform="translate(139.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 11.05.</text></g><g class="tick" opacity="1" transform="translate(301.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 18.05.</text></g><g class="tick" opacity="1" transform="translate(463.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 25.05.</text></g><g class="tick" opacity="1" transform="translate(626.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 01.06.</text></g><g class="tick" opacity="1" transform="translate(788.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 08.06.</text></g><g class="tick" opacity="1" transform="translate(950.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 15.06.</text></g><g class="tick" opacity="1" transform="translate(1112.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 22.06.</text></g><g class="tick" opacity="1" transform="translate(1274.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 29.06.</text></g><g class="tick" opacity="1" transform="translate(1436.5,0)"><line stroke="currentColor" y2="-577"/><text fill="#000" y="3" dy="1em" stroke="none" font-size="10" style="text-anchor: middle;"> 06.07.</text></g></g><g><rect x="0" y="58" width="1866.5" height="46" class="section section0"/><rect x="0" y="104" width="1866.5" height="46" class="section section0"/><rect x="0" y="150" width="1866.5" height="46" class="section section0"/><rect x="0" y="196" width="1866.5" height="46" class="section section1"/><rect x="0" y="242" width="1866.5" height="46" class="section section1"/><rect x="0" y="288" width="1866.5" height="46" class="section section1"/><rect x="0" y="334" width="1866.5" height="46" class="section section2"/><rect x="0" y="380" width="1866.5" height="46" class="section section3"/><rect x="0" y="426" width="1866.5" height="46" class="section section3"/><rect x="0" y="472" width="1866.5" height="46" class="section section0"/><rect x="0" y="518" width="1866.5" height="46" class="section section0"/><rect x="0" y="564" width="1866.5" height="46" class="section section0"/></g><g><rect id="approval" rx="3" ry="3" x="285" y="60" width="30" height="30" transform-origin="300px 75px" class="task milestone  done0 "/><rect id="sprint0" rx="3" ry="3" x="300" y="106" width="116" height="30" transform-origin="358px 121px" class="task active0 "/><rect id="meet1" rx="3" ry="3" x="389.5" y="152" width="30" height="30" transform-origin="404.5px 167px" class="task milestone  task0 "/><rect id="sprint1" rx="3" ry="3" x="416" y="198" width="255" height="30" transform-origin="543.5px 213px" class="task task1 "/><rect id="sprint2" rx="3" ry="3" x="671" y="244" width="255" height="30" transform-origin="798.5px 259px" class="task task1 "/><rect id="meet2" rx="3" ry="3" x="922.5" y="290" width="30" height="30" transform-origin="937.5px 305px" class="task milestone  task1 "/><rect id="sprint3" rx="3" ry="3" x="926" y="336" width="231" height="30" transform-origin="1041.5px 351px" class="task task2 "/><rect id="sprint4" rx="3" ry="3" x="1157" y="382" width="232" height="30" transform-origin="1273px 397px" class="task task3 "/><rect id="meet3" rx="3" ry="3" x="1362.5" y="428" width="30" height="30" transform-origin="1377.5px 443px" class="task milestone  task3 "/><rect id="sprint5" rx="3" ry="3" x="1389" y="474" width="255" height="30" transform-origin="1516.5px 489px" class="task task0 "/><rect id="sprint6" rx="3" ry="3" x="1644" y="520" width="185" height="30" transform-origin="1736.5px 535px" class="task task0 "/><rect id="handoff" rx="3" ry="3" x="1791" y="566" width="30" height="30" transform-origin="1806px 581px" class="task milestone  task0 "/><text id="approval-text" font-size="11" x="320" y="78.5" text-height="30" class=" taskTextOutsideRight taskTextOutside0  doneText0 milestoneText width-121.0625">Freigabe Semesterarbeit             </text><text id="sprint0-text" font-size="11" x="358" y="124.5" text-height="30" class=" taskText taskText0 activeText0 width-37.921875">Sprint 0 </text><text id="meet1-text" font-size="11" x="424.5" y="170.5" text-height="30" class=" taskTextOutsideRight taskTextOutside0  milestoneText width-102.75">Einzel­besprechung 1                </text><text id="sprint1-text" font-size="11" x="543.5" y="216.5" text-height="30" class=" taskText taskText1  width-143.32470703125">Sprint 1 AWS Account &amp; VPC          </text><text id="sprint2-text" font-size="11" x="798.5" y="262.5" text-height="30" class=" taskText taskText1  width-149.625">Sprint 2 Terraform EKS &amp; ECR        </text><text id="meet2-text" font-size="11" x="957.5" y="308.5" text-height="30" class=" taskTextOutsideRight taskTextOutside1  milestoneText width-102.75">Einzel­besprechung 2                </text><text id="sprint3-text" font-size="11" x="1041.5" y="354.5" text-height="30" class=" taskText taskText2  width-149.171875">Sprint 3 RDS/IAM &amp; Nextcloud        </text><text id="sprint4-text" font-size="11" x="1273" y="400.5" text-height="30" class=" taskText taskText3  width-148.5625">Sprint 4 Nextcloud Helm Chart       </text><text id="meet3-text" font-size="11" x="1397.5" y="446.5" text-height="30" class=" taskTextOutsideRight taskTextOutside3  milestoneText width-102.75">Einzel­besprechung 3                </text><text id="sprint5-text" font-size="11" x="1516.5" y="492.5" text-height="30" class=" taskText taskText0  width-153.146484375">Sprint 5 GitHub Actions &amp; Tests     </text><text id="sprint6-text" font-size="11" x="1736.5" y="538.5" text-height="30" class=" taskText taskText0  width-103.34375">Sprint 6 Finalisierung      </text><text id="handoff-text" font-size="11" x="1786" y="584.5" text-height="30" class=" taskTextOutsideLeft taskTextOutside0  milestoneText">Abgabe &amp; Schluss­präsentation       </text></g><g><text dy="0em" x="10" y="129" font-size="11" class="sectionTitle sectionTitle0"><tspan alignment-baseline="central" x="10">Phase 1 – Vorbereitung &amp; Planung</tspan></text><text dy="0em" x="10" y="267" font-size="11" class="sectionTitle sectionTitle1"><tspan alignment-baseline="central" x="10">Phase 2 – Infrastruktur Grundlagen (Terraform)</tspan></text><text dy="0em" x="10" y="359" font-size="11" class="sectionTitle sectionTitle2"><tspan alignment-baseline="central" x="10">Phase 3 – DB &amp; manuelles Deployment</tspan></text><text dy="0em" x="10" y="428" font-size="11" class="sectionTitle sectionTitle3"><tspan alignment-baseline="central" x="10">Phase 4 – Helm-Chart Entwicklung</tspan></text><text dy="0em" x="10" y="543" font-size="11" class="sectionTitle sectionTitle0"><tspan alignment-baseline="central" x="10">Phase 5 – CI/CD &amp; Abschluss</tspan></text></g><g class="today"><line x1="391" x2="391" y1="30" y2="642" class="today"/></g><text x="952" y="30" class="titleText">Semesterarbeit: Nextcloud CI/CD auf EKS</text></svg>

==================================================
File: .\assets\logische-gesamtarchitektur.svg
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" type="text/css"?>
<svg aria-roledescription="flowchart-v2" role="graphics-document document" style="overflow: hidden; max-width: 100%; touch-action: none; user-select: none;" class="flowchart" xmlns="http://www.w3.org/2000/svg" width="100%" id="graph-1835" height="100%" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ev="http://www.w3.org/2001/xml-events"><g id="viewport-20250702192649672" class="svg-pan-zoom_viewport" transform="matrix(1.060095191001892,0,0,1.060095191001892,-20.55942726135254,370.10382080078125)" style="transform: matrix(1.0601, 0, 0, 1.0601, -20.5594, 370.104);"><style>#graph-1835{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#ccc;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#graph-1835 .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#graph-1835 .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#graph-1835 .error-icon{fill:#a44141;}#graph-1835 .error-text{fill:#ddd;stroke:#ddd;}#graph-1835 .edge-thickness-normal{stroke-width:1px;}#graph-1835 .edge-thickness-thick{stroke-width:3.5px;}#graph-1835 .edge-pattern-solid{stroke-dasharray:0;}#graph-1835 .edge-thickness-invisible{stroke-width:0;fill:none;}#graph-1835 .edge-pattern-dashed{stroke-dasharray:3;}#graph-1835 .edge-pattern-dotted{stroke-dasharray:2;}#graph-1835 .marker{fill:lightgrey;stroke:lightgrey;}#graph-1835 .marker.cross{stroke:lightgrey;}#graph-1835 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#graph-1835 p{margin:0;}#graph-1835 .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#ccc;}#graph-1835 .cluster-label text{fill:#F9FFFE;}#graph-1835 .cluster-label span{color:#F9FFFE;}#graph-1835 .cluster-label span p{background-color:transparent;}#graph-1835 .label text,#graph-1835 span{fill:#ccc;color:#ccc;}#graph-1835 .node rect,#graph-1835 .node circle,#graph-1835 .node ellipse,#graph-1835 .node polygon,#graph-1835 .node path{fill:#1f2020;stroke:#ccc;stroke-width:1px;}#graph-1835 .rough-node .label text,#graph-1835 .node .label text,#graph-1835 .image-shape .label,#graph-1835 .icon-shape .label{text-anchor:middle;}#graph-1835 .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#graph-1835 .rough-node .label,#graph-1835 .node .label,#graph-1835 .image-shape .label,#graph-1835 .icon-shape .label{text-align:center;}#graph-1835 .node.clickable{cursor:pointer;}#graph-1835 .root .anchor path{fill:lightgrey!important;stroke-width:0;stroke:lightgrey;}#graph-1835 .arrowheadPath{fill:lightgrey;}#graph-1835 .edgePath .path{stroke:lightgrey;stroke-width:2.0px;}#graph-1835 .flowchart-link{stroke:lightgrey;fill:none;}#graph-1835 .edgeLabel{background-color:hsl(0, 0%, 34.4117647059%);text-align:center;}#graph-1835 .edgeLabel p{background-color:hsl(0, 0%, 34.4117647059%);}#graph-1835 .edgeLabel rect{opacity:0.5;background-color:hsl(0, 0%, 34.4117647059%);fill:hsl(0, 0%, 34.4117647059%);}#graph-1835 .labelBkg{background-color:rgba(87.75, 87.75, 87.75, 0.5);}#graph-1835 .cluster rect{fill:hsl(180, 1.5873015873%, 28.3529411765%);stroke:rgba(255, 255, 255, 0.25);stroke-width:1px;}#graph-1835 .cluster text{fill:#F9FFFE;}#graph-1835 .cluster span{color:#F9FFFE;}#graph-1835 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(20, 1.5873015873%, 12.3529411765%);border:1px solid rgba(255, 255, 255, 0.25);border-radius:2px;pointer-events:none;z-index:100;}#graph-1835 .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#ccc;}#graph-1835 rect.text{fill:none;stroke-width:0;}#graph-1835 .icon-shape,#graph-1835 .image-shape{background-color:hsl(0, 0%, 34.4117647059%);text-align:center;}#graph-1835 .icon-shape p,#graph-1835 .image-shape p{background-color:hsl(0, 0%, 34.4117647059%);padding:2px;}#graph-1835 .icon-shape rect,#graph-1835 .image-shape rect{opacity:0.5;background-color:hsl(0, 0%, 34.4117647059%);fill:hsl(0, 0%, 34.4117647059%);}#graph-1835 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="5" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-1835_flowchart-v2-pointEnd"><path style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="4.5" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-1835_flowchart-v2-pointStart"><path style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-1835_flowchart-v2-circleEnd"><circle style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-1835_flowchart-v2-circleStart"><circle style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="graph-1835_flowchart-v2-crossEnd"><path style="stroke-width: 2px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="graph-1835_flowchart-v2-crossStart"><path style="stroke-width: 2px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"><g data-look="classic" id="EndUser" class="cluster"><rect height="129" width="223.81666564941406" y="370" x="880.5749969482422" style=""></rect><g transform="translate(963.4583282470703, 370)" class="cluster-label"><foreignObject height="24" width="58.05000305175781"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>EndUser</p></span></div></foreignObject></g></g><g data-look="classic" id="DockerHub" class="cluster"><rect height="129" width="307.51666259765625" y="549" x="8" style=""></rect><g transform="translate(123.0250015258789, 549)" class="cluster-label"><foreignObject height="24" width="77.46665954589844"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>DockerHub</p></span></div></foreignObject></g></g><g data-look="classic" id="AWS" class="cluster"><rect height="308" width="525.0583343505859" y="370" x="335.51666259765625" style=""></rect><g transform="translate(583.3624954223633, 370)" class="cluster-label"><foreignObject height="24" width="29.366668701171875"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>AWS</p></span></div></foreignObject></g></g><g data-look="classic" id="GitHub" class="cluster"><rect height="312" width="399.9000015258789" y="8" x="399.8999938964844" style=""></rect><g transform="translate(574.9166603088379, 8)" class="cluster-label"><foreignObject height="24" width="49.866668701171875"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>GitHub</p></span></div></foreignObject></g></g><g data-look="classic" id="VPC" class="cluster"><rect height="258" width="233.14166259765625" y="395" x="355.51666259765625" style=""></rect><g transform="translate(458.1458282470703, 395)" class="cluster-label"><foreignObject height="24" width="27.883331298828125"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>VPC</p></span></div></foreignObject></g></g></g><g class="edgePaths"><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_B_0" d="M599.85,87L599.85,91.167C599.85,95.333,599.85,103.667,599.85,111.333C599.85,119,599.85,126,599.85,129.5L599.85,133"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_B_C_0" d="M599.85,191L599.85,195.167C599.85,199.333,599.85,207.667,599.85,215.333C599.85,223,599.85,230,599.85,233.5L599.85,237"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_C_I_0" d="M664.633,295L674.63,299.167C684.627,303.333,704.622,311.667,714.619,320C724.617,328.333,724.617,336.667,724.617,345C724.617,353.333,724.617,361.667,724.617,370C724.617,378.333,724.617,386.667,724.617,394.333C724.617,402,724.617,409,724.617,412.5L724.617,416"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_C_F_0" d="M537.149,295L527.472,299.167C517.796,303.333,498.444,311.667,488.768,320C479.092,328.333,479.092,336.667,479.092,345C479.092,353.333,479.092,361.667,479.092,370C479.092,378.333,479.092,386.667,479.092,394.333C479.092,402,479.092,409,479.092,412.5L479.092,416"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_H_0" d="M473.899,474L473.098,478.167C472.297,482.333,470.694,490.667,469.893,499C469.092,507.333,469.092,515.667,469.092,524C469.092,532.333,469.092,540.667,469.092,548.333C469.092,556,469.092,563,469.092,566.5L469.092,570"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_J_0" d="M508.577,474L513.128,478.167C517.678,482.333,526.779,490.667,531.329,499C535.879,507.333,535.879,515.667,535.879,524C535.879,532.333,535.879,540.667,550.427,548.842C564.976,557.017,594.072,565.033,608.62,569.041L623.169,573.05"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_K_0" d="M444.414,474L439.062,478.167C433.71,482.333,423.007,490.667,417.656,499C412.304,507.333,412.304,515.667,412.304,524C412.304,532.333,412.304,540.667,390.992,549.257C369.681,557.846,327.057,566.693,305.745,571.116L284.433,575.539"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_L_J_0" d="M992.483,474L992.483,478.167C992.483,482.333,992.483,490.667,947.839,499C903.194,507.333,813.906,515.667,769.261,524C724.617,532.333,724.617,540.667,724.617,548.333C724.617,556,724.617,563,724.617,566.5L724.617,570"></path><path marker-end="url(#graph-1835_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_J_F_0" d="M627.025,576.688L608.501,572.074C589.976,567.459,552.928,558.229,534.403,549.448C515.879,540.667,515.879,532.333,515.879,524C515.879,515.667,515.879,507.333,513.316,499.544C510.754,491.755,505.628,484.51,503.066,480.888L500.503,477.265"></path></g><g class="edgeLabels"><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(599.8499946594238, 60)" id="flowchart-A-0" class="node default"><rect height="54" width="132.28334045410156" y="-27" x="-66.14167022705078" style="fill:#9f7 !important;stroke:#333 !important;stroke-width:2px !important" class="basic label-container"></rect><g transform="translate(-36.14167022705078, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="72.28334045410156"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Developer</p></span></div></foreignObject></g></g><g transform="translate(599.8499946594238, 164)" id="flowchart-B-1" class="node default"><rect height="54" width="161.53334045410156" y="-27" x="-80.76667022705078" style="" class="basic label-container"></rect><g transform="translate(-50.76667022705078, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="101.53334045410156"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Git Repository</p></span></div></foreignObject></g></g><g transform="translate(599.8499946594238, 268)" id="flowchart-C-2" class="node default"><rect height="54" width="166.76666259765625" y="-27" x="-83.38333129882812" style="" class="basic label-container"></rect><g transform="translate(-53.383331298828125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="106.76666259765625"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>GitHub Actions</p></span></div></foreignObject></g></g><g transform="translate(479.09165954589844, 447)" id="flowchart-F-3" class="node default"><rect height="54" width="141.11666870117188" y="-27" x="-70.55833435058594" style="" class="basic label-container"></rect><g transform="translate(-40.55833435058594, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="81.11666870117188"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>EKS Cluster</p></span></div></foreignObject></g></g><g transform="translate(469.09165954589844, 601)" id="flowchart-H-4" class="node default"><rect height="54" width="157.14999389648438" y="-27" x="-78.57499694824219" style="" class="basic label-container"></rect><g transform="translate(-48.57499694824219, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="97.14999389648438"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>RDS Database</p></span></div></foreignObject></g></g><g transform="translate(724.6166610717773, 447)" id="flowchart-I-5" class="node default"><rect height="54" width="201.9166717529297" y="-27" x="-100.95833587646484" style="" class="basic label-container"></rect><g transform="translate(-70.95833587646484, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="141.9166717529297"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>OIDC Provider / IAM</p></span></div></foreignObject></g></g><g transform="translate(724.6166610717773, 601)" id="flowchart-J-6" class="node default"><rect height="54" width="195.18333435058594" y="-27" x="-97.59166717529297" style="" class="basic label-container"></rect><g transform="translate(-67.59166717529297, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="135.18333435058594"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>AWS Load Balancer</p></span></div></foreignObject></g></g><g transform="translate(161.75833129882812, 601)" id="flowchart-K-7" class="node default"><rect height="54" width="237.51666259765625" y="-27" x="-118.75833129882812" style="" class="basic label-container"></rect><g transform="translate(-88.75833129882812, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="177.51666259765625"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Official Nextcloud Image</p></span></div></foreignObject></g></g><g transform="translate(992.4833297729492, 447)" id="flowchart-L-8" class="node default"><rect height="54" width="153.81666564941406" y="-27" x="-76.90833282470703" style="fill:#9f7 !important;stroke:#333 !important;stroke-width:2px !important" class="basic label-container"></rect><g transform="translate(-46.90833282470703, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="93.81666564941406"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>User Browser</p></span></div></foreignObject></g></g></g></g></g></g></svg>

==================================================
File: .\assets\netzwerkarchitektur.svg
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" type="text/css"?>
<svg aria-roledescription="flowchart-v2" role="graphics-document document" style="overflow: hidden; max-width: 100%; touch-action: none; user-select: none;" class="flowchart" xmlns="http://www.w3.org/2000/svg" width="100%" id="graph-2527" height="100%" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ev="http://www.w3.org/2001/xml-events"><g id="viewport-20250702192803035" class="svg-pan-zoom_viewport" transform="matrix(0.7289819717407227,0,0,0.7289819717407227,-157.52725219726562,340.4006652832031)" style="transform: matrix(0.728982, 0, 0, 0.728982, -157.527, 340.401);"><style>#graph-2527{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#ccc;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#graph-2527 .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#graph-2527 .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#graph-2527 .error-icon{fill:#a44141;}#graph-2527 .error-text{fill:#ddd;stroke:#ddd;}#graph-2527 .edge-thickness-normal{stroke-width:1px;}#graph-2527 .edge-thickness-thick{stroke-width:3.5px;}#graph-2527 .edge-pattern-solid{stroke-dasharray:0;}#graph-2527 .edge-thickness-invisible{stroke-width:0;fill:none;}#graph-2527 .edge-pattern-dashed{stroke-dasharray:3;}#graph-2527 .edge-pattern-dotted{stroke-dasharray:2;}#graph-2527 .marker{fill:lightgrey;stroke:lightgrey;}#graph-2527 .marker.cross{stroke:lightgrey;}#graph-2527 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#graph-2527 p{margin:0;}#graph-2527 .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#ccc;}#graph-2527 .cluster-label text{fill:#F9FFFE;}#graph-2527 .cluster-label span{color:#F9FFFE;}#graph-2527 .cluster-label span p{background-color:transparent;}#graph-2527 .label text,#graph-2527 span{fill:#ccc;color:#ccc;}#graph-2527 .node rect,#graph-2527 .node circle,#graph-2527 .node ellipse,#graph-2527 .node polygon,#graph-2527 .node path{fill:#1f2020;stroke:#ccc;stroke-width:1px;}#graph-2527 .rough-node .label text,#graph-2527 .node .label text,#graph-2527 .image-shape .label,#graph-2527 .icon-shape .label{text-anchor:middle;}#graph-2527 .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#graph-2527 .rough-node .label,#graph-2527 .node .label,#graph-2527 .image-shape .label,#graph-2527 .icon-shape .label{text-align:center;}#graph-2527 .node.clickable{cursor:pointer;}#graph-2527 .root .anchor path{fill:lightgrey!important;stroke-width:0;stroke:lightgrey;}#graph-2527 .arrowheadPath{fill:lightgrey;}#graph-2527 .edgePath .path{stroke:lightgrey;stroke-width:2.0px;}#graph-2527 .flowchart-link{stroke:lightgrey;fill:none;}#graph-2527 .edgeLabel{background-color:hsl(0, 0%, 34.4117647059%);text-align:center;}#graph-2527 .edgeLabel p{background-color:hsl(0, 0%, 34.4117647059%);}#graph-2527 .edgeLabel rect{opacity:0.5;background-color:hsl(0, 0%, 34.4117647059%);fill:hsl(0, 0%, 34.4117647059%);}#graph-2527 .labelBkg{background-color:rgba(87.75, 87.75, 87.75, 0.5);}#graph-2527 .cluster rect{fill:hsl(180, 1.5873015873%, 28.3529411765%);stroke:rgba(255, 255, 255, 0.25);stroke-width:1px;}#graph-2527 .cluster text{fill:#F9FFFE;}#graph-2527 .cluster span{color:#F9FFFE;}#graph-2527 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(20, 1.5873015873%, 12.3529411765%);border:1px solid rgba(255, 255, 255, 0.25);border-radius:2px;pointer-events:none;z-index:100;}#graph-2527 .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#ccc;}#graph-2527 rect.text{fill:none;stroke-width:0;}#graph-2527 .icon-shape,#graph-2527 .image-shape{background-color:hsl(0, 0%, 34.4117647059%);text-align:center;}#graph-2527 .icon-shape p,#graph-2527 .image-shape p{background-color:hsl(0, 0%, 34.4117647059%);padding:2px;}#graph-2527 .icon-shape rect,#graph-2527 .image-shape rect{opacity:0.5;background-color:hsl(0, 0%, 34.4117647059%);fill:hsl(0, 0%, 34.4117647059%);}#graph-2527 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="5" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-2527_flowchart-v2-pointEnd"><path style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="4.5" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-2527_flowchart-v2-pointStart"><path style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-2527_flowchart-v2-circleEnd"><circle style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart-v2" id="graph-2527_flowchart-v2-circleStart"><circle style="stroke-width: 1px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="graph-2527_flowchart-v2-crossEnd"><path style="stroke-width: 2px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="graph-2527_flowchart-v2-crossStart"><path style="stroke-width: 2px; stroke-dasharray: 1px, 0px;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"><g data-look="classic" id="AWS_Cloud" class="cluster"><rect height="1166" width="1326.2166595458984" y="8" x="342" style=""></rect><g transform="translate(920.9833297729492, 8)" class="cluster-label"><foreignObject height="24" width="168.25"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>AWS Cloud eu-central-1</p></span></div></foreignObject></g></g><g data-look="classic" id="Internet" class="cluster"><rect height="979" width="216.76666259765625" y="183" x="8" style=""></rect><g transform="translate(87.21666717529297, 183)" class="cluster-label"><foreignObject height="24" width="58.33332824707031"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Internet</p></span></div></foreignObject></g></g><g data-look="classic" id="VPC" class="cluster"><rect height="998" width="1276.2166595458984" y="28" x="367" style=""></rect><g transform="translate(946.3999938964844, 28)" class="cluster-label"><foreignObject height="24" width="117.41667175292969"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>VPC 10.0.0.0/16</p></span></div></foreignObject></g></g><g data-look="classic" id="AZ_B" class="cluster"><rect height="318" width="990.5833282470703" y="584" x="392" style=""></rect><g transform="translate(820.783332824707, 584)" class="cluster-label"><foreignObject height="24" width="133.01666259765625"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Availability Zone B</p></span></div></foreignObject></g></g><g data-look="classic" id="AZ_A" class="cluster"><rect height="412" width="990.5833282470703" y="152" x="392" style=""></rect><g transform="translate(820.591667175293, 152)" class="cluster-label"><foreignObject height="24" width="133.39999389648438"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Availability Zone A</p></span></div></foreignObject></g></g><g data-look="classic" id="PrivateSubnetB" class="cluster"><rect height="134" width="545.4333343505859" y="604" x="812.1499938964844" style=""></rect><g transform="translate(1025.7916641235352, 604)" class="cluster-label"><foreignObject height="24" width="118.14999389648438"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Private Subnet B</p></span></div></foreignObject></g></g><g data-look="classic" id="PublicSubnetB" class="cluster"><rect height="124" width="940.5833282470703" y="758" x="417" style=""></rect><g transform="translate(831.4583282470703, 758)" class="cluster-label"><foreignObject height="24" width="111.66667175292969"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Public Subnet B</p></span></div></foreignObject></g></g><g data-look="classic" id="PrivateSubnetA" class="cluster"><rect height="228" width="277.8666687011719" y="316" x="812.1499938964844" style=""></rect><g transform="translate(891.8166580200195, 316)" class="cluster-label"><foreignObject height="24" width="118.53334045410156"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Private Subnet A</p></span></div></foreignObject></g></g><g data-look="classic" id="PublicSubnetA" class="cluster"><rect height="124" width="940.5833282470703" y="172" x="417" style=""></rect><g transform="translate(831.2666625976562, 172)" class="cluster-label"><foreignObject height="24" width="112.05000305175781"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Public Subnet A</p></span></div></foreignObject></g></g></g><g class="edgePaths"><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_User_LBA_0" d="M126.203,505L142.631,459.833C159.058,414.667,191.912,324.333,218.109,279.167C244.306,234,263.844,234,283.383,234C302.922,234,322.461,234,336.397,234C350.333,234,358.667,234,367,234C375.333,234,383.667,234,392,234C400.333,234,408.667,234,419.957,234C431.247,234,445.494,234,452.618,234L459.742,234"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_User_LBB_0" d="M126.544,559L142.915,602.5C159.285,646,192.026,733,218.166,776.5C244.306,820,263.844,820,283.383,820C302.922,820,322.461,820,336.397,820C350.333,820,358.667,820,367,820C375.333,820,383.667,820,392,820C400.333,820,408.667,820,419.989,820C431.311,820,445.622,820,452.778,820L459.933,820"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_LBA_NodeA_0" d="M680.258,234L693.061,234C705.864,234,731.469,234,753.451,234C775.433,234,793.792,234,821.322,253.02C848.852,272.04,885.554,310.081,903.905,329.101L922.256,348.121"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_LBB_NodeB_0" d="M680.067,820L692.901,820C705.736,820,731.406,820,753.419,820C775.433,820,793.792,820,821.322,800.98C848.852,781.96,885.554,743.919,903.905,724.899L922.256,705.879"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NodeA_NATA_0" d="M1065.017,368.979L1069.183,368.649C1073.35,368.32,1081.683,367.66,1090.017,341.663C1098.35,315.667,1106.683,264.333,1116.329,239.615C1125.975,214.897,1136.934,216.795,1142.413,217.744L1147.892,218.692"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NodeB_NATB_0" d="M1064.825,693.192L1069.024,693.827C1073.222,694.461,1081.619,695.731,1089.985,696.365C1098.35,697,1106.683,697,1126.159,712.525C1145.634,728.051,1176.251,759.101,1191.56,774.626L1206.868,790.152"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NATA_IGW_0" d="M1320.767,234L1326.903,234C1333.039,234,1345.311,234,1355.614,234C1365.917,234,1374.25,234,1382.583,234C1390.917,234,1399.25,234,1422.017,326.513C1444.783,419.026,1481.983,604.052,1500.583,696.565L1519.183,789.078"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NATB_IGW_0" d="M1320.575,820L1326.743,820C1332.911,820,1345.247,820,1355.582,820C1365.917,820,1374.25,820,1382.583,820C1390.917,820,1399.25,820,1406.917,820C1414.583,820,1421.583,820,1425.083,820L1428.583,820"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_RT_Public_IGW_0" d="M1332.583,964L1336.75,964C1340.917,964,1349.25,964,1357.583,964C1365.917,964,1374.25,964,1382.583,964C1390.917,964,1399.25,964,1418.949,945.016C1438.648,926.032,1469.712,888.064,1485.244,869.08L1500.776,850.096"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_RT_PrivateA_NATA_0" d="M973.818,117L993.184,140C1012.551,163,1051.284,209,1074.817,232C1098.35,255,1106.683,255,1116.329,254.051C1125.975,253.103,1136.934,251.205,1142.413,250.256L1147.892,249.308"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_RT_PrivateB_NATB_0" d="M977.133,937L995.947,917.5C1014.761,898,1052.389,859,1075.369,839.5C1098.35,820,1106.683,820,1116.351,820C1126.019,820,1137.022,820,1142.524,820L1148.025,820"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" marker-start="url(#graph-2527_flowchart-v2-pointStart)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NodeA_RDSB_0" d="M1006.011,406.861L1020.012,414.217C1034.013,421.574,1062.015,436.287,1080.182,443.643C1098.35,451,1106.683,451,1128.198,481.753C1149.712,512.505,1184.408,574.011,1201.756,604.763L1219.104,635.516"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" marker-start="url(#graph-2527_flowchart-v2-pointStart)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_NodeB_RDSB_0" d="M1068.815,667.526L1072.348,667.272C1075.882,667.017,1082.949,666.509,1090.65,666.254C1098.35,666,1106.683,666,1118.199,666C1129.714,666,1144.411,666,1151.76,666L1159.108,666"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_GHA_EKS_CP_0" d="M199.767,1100L203.933,1100C208.1,1100,216.433,1100,230.369,1100C244.306,1100,263.844,1100,283.383,1100C302.922,1100,322.461,1100,336.397,1100C350.333,1100,358.667,1100,367,1100C375.333,1100,383.667,1100,392,1100C400.333,1100,408.667,1100,416.333,1100C424,1100,431,1100,434.5,1100L438,1100"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_EKS_CP_NodeA_0" d="M581.997,1061L611.177,947.167C640.356,833.333,698.716,605.667,737.075,491.833C775.433,378,793.792,378,806.471,378C819.15,378,826.15,378,829.65,378L833.15,378"></path><path marker-end="url(#graph-2527_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_EKS_CP_NodeB_0" d="M589.023,1061L617.032,996.833C645.041,932.667,701.058,804.333,738.246,740.167C775.433,676,793.792,676,806.503,676C819.214,676,826.278,676,829.81,676L833.342,676"></path></g><g class="edgeLabels"><g transform="translate(283.3833312988281, 234)" class="edgeLabel"><g transform="translate(-33.616668701171875, -12)" class="label"><foreignObject height="24" width="67.23333740234375"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>HTTPS 80</p></span></div></foreignObject></g></g><g transform="translate(283.3833312988281, 820)" class="edgeLabel"><g transform="translate(-33.616668701171875, -12)" class="label"><foreignObject height="24" width="67.23333740234375"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>HTTPS 80</p></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g transform="translate(283.3833312988281, 1100)" class="edgeLabel"><g transform="translate(-26.550003051757812, -12)" class="label"><foreignObject height="24" width="53.100006103515625"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>EKS API</p></span></div></foreignObject></g></g><g transform="translate(757.0749969482422, 378)" class="edgeLabel"><g transform="translate(-30.074996948242188, -12)" class="label"><foreignObject height="24" width="60.149993896484375"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>Manages</p></span></div></foreignObject></g></g><g transform="translate(757.0749969482422, 676)" class="edgeLabel"><g transform="translate(-30.074996948242188, -12)" class="label"><foreignObject height="24" width="60.149993896484375"><div class="labelBkg" xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>Manages</p></span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(116.38333129882812, 532)" id="flowchart-User-0" class="node default"><rect height="54" width="91.81666564941406" y="-27" x="-45.90833282470703" style="fill:#9f7 !important;stroke:#333 !important;stroke-width:2px !important" class="basic label-container"></rect><g transform="translate(-15.908332824707031, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="31.816665649414062"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>User</p></span></div></foreignObject></g></g><g transform="translate(116.38333129882812, 1100)" id="flowchart-GHA-1" class="node default"><rect height="54" width="166.76666259765625" y="-27" x="-83.38333129882812" style="" class="basic label-container"></rect><g transform="translate(-53.383331298828125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="106.76666259765625"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>GitHub Actions</p></span></div></foreignObject></g></g><g transform="translate(572, 234)" id="flowchart-LBA-2" class="node default"><rect height="54" width="216.51666259765625" y="-27" x="-108.25833129882812" style="" class="basic label-container"></rect><g transform="translate(-78.25833129882812, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="156.51666259765625"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Load Balancer Node A</p></span></div></foreignObject></g></g><g transform="translate(1236.2999954223633, 234)" id="flowchart-NATA-3" class="node default"><rect height="54" width="168.93333435058594" y="-27" x="-84.46666717529297" style="" class="basic label-container"></rect><g transform="translate(-54.46666717529297, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="108.93333435058594"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>NAT Gateway A</p></span></div></foreignObject></g></g><g transform="translate(951.0833282470703, 378)" id="flowchart-NodeA-4" class="node default"><rect height="54" width="227.86666870117188" y="-27" x="-113.93333435058594" style="" class="basic label-container"></rect><g transform="translate(-83.93333435058594, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="167.86666870117188"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>EKS Worker Node A EC2</p></span></div></foreignObject></g></g><g transform="translate(951.0833282470703, 482)" id="flowchart-RDSA-5" class="node default"><rect height="54" width="148.56666564941406" y="-27" x="-74.28333282470703" style="" class="basic label-container"></rect><g transform="translate(-44.28333282470703, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="88.56666564941406"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>RDS Standby</p></span></div></foreignObject></g></g><g transform="translate(572, 820)" id="flowchart-LBB-6" class="node default"><rect height="54" width="216.13333129882812" y="-27" x="-108.06666564941406" style="" class="basic label-container"></rect><g transform="translate(-78.06666564941406, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="156.13333129882812"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Load Balancer Node B</p></span></div></foreignObject></g></g><g transform="translate(1236.2999954223633, 820)" id="flowchart-NATB-7" class="node default"><rect height="54" width="168.5500030517578" y="-27" x="-84.2750015258789" style="" class="basic label-container"></rect><g transform="translate(-54.275001525878906, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="108.55000305175781"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>NAT Gateway B</p></span></div></foreignObject></g></g><g transform="translate(951.0833282470703, 676)" id="flowchart-NodeB-8" class="node default"><rect height="54" width="227.48333740234375" y="-27" x="-113.74166870117188" style="" class="basic label-container"></rect><g transform="translate(-83.74166870117188, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="167.48333740234375"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>EKS Worker Node B EC2</p></span></div></foreignObject></g></g><g transform="translate(1236.2999954223633, 666)" id="flowchart-RDSB-9" class="node default"><rect height="54" width="146.38333129882812" y="-27" x="-73.19166564941406" style="" class="basic label-container"></rect><g transform="translate(-43.19166564941406, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="86.38333129882812"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>RDS Primary</p></span></div></foreignObject></g></g><g transform="translate(1525.3999938964844, 820)" id="flowchart-IGW-10" class="node default"><rect height="54" width="185.63333129882812" y="-27" x="-92.81666564941406" style="" class="basic label-container"></rect><g transform="translate(-62.81666564941406, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="125.63333129882812"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Internet Gateway</p></span></div></foreignObject></g></g><g transform="translate(1236.2999954223633, 964)" id="flowchart-RT_Public-11" class="node default"><rect height="54" width="192.56666564941406" y="-27" x="-96.28333282470703" style="" class="basic label-container"></rect><g transform="translate(-66.28333282470703, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="132.56666564941406"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Public Route Table</p></span></div></foreignObject></g></g><g transform="translate(951.0833282470703, 90)" id="flowchart-RT_PrivateA-12" class="node default"><rect height="54" width="213.3000030517578" y="-27" x="-106.6500015258789" style="" class="basic label-container"></rect><g transform="translate(-76.6500015258789, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="153.3000030517578"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Private Route Table A</p></span></div></foreignObject></g></g><g transform="translate(951.0833282470703, 964)" id="flowchart-RT_PrivateB-13" class="node default"><rect height="54" width="212.9166717529297" y="-27" x="-106.45833587646484" style="" class="basic label-container"></rect><g transform="translate(-76.45833587646484, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="152.9166717529297"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Private Route Table B</p></span></div></foreignObject></g></g><g transform="translate(572, 1100)" id="flowchart-EKS_CP-14" class="node default"><rect height="78" width="260" y="-39" x="-130" style="" class="basic label-container"></rect><g transform="translate(-100, -24)" style="" class="label"><rect></rect><foreignObject height="48" width="200"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;"><span class="nodeLabel"><p>EKS Control Plane AWS Managed</p></span></div></foreignObject></g></g></g></g></g></g></svg>

==================================================
File: .\backend\locals.tf
==================================================
locals {
  common_tags = {
    Project   = var.project_name
    Purpose   = "Terraform-State-Backend"
    ManagedBy = "Terraform"
  }
}

==================================================
File: .\backend\main.tf
==================================================
resource "aws_s3_bucket" "terraform_state" {
  bucket = var.terraform_s3_backend_bucket_name

  lifecycle {
    prevent_destroy = true
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-tfstate-bucket"
    }
  )
}

resource "aws_s3_bucket_versioning" "terraform_state_versioning" {
  bucket = aws_s3_bucket.terraform_state.id

  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state_sse" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "terraform_state_public_access" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_dynamodb_table" "terraform_state_lock" {
  name         = var.terraform_dynamodb_lock_table_name
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-tfstate-lock-table"
    }
  )
}

==================================================
File: .\backend\provider.tf
==================================================
provider "aws" {
  region = var.aws_region
  profile = "nextcloud-project"
}

==================================================
File: .\backend\variables.tf
==================================================
variable "aws_region" {
  default = "eu-central-1"
}

variable "project_name" {
  default = "Nextcloud-Backend"
}

variable "terraform_s3_backend_bucket_name" {
  description = "A globally unique name for the S3 bucket that will store Terraform state."
  type        = string
  default     = "nenad-stevic-nextcloud-tfstate"
}

variable "terraform_dynamodb_lock_table_name" {
  description = "Name for the DynamoDB table used for Terraform state locking."
  type        = string
  default     = "nenad-stevic-nextcloud-tfstate-lock"
}


==================================================
File: .\charts\nextcloud-chart\Chart.yaml
==================================================
apiVersion: v2
name: nextcloud
description: A Helm chart for deploying Nextcloud on Kubernetes
type: application
version: 0.1.0
appVersion: "latest"

==================================================
File: .\charts\nextcloud-chart\templates\configmap.yaml
==================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "nextcloud.fullname" . }}-config
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
data:
  # This injects a custom configuration file into Nextcloud.
  # It is crucial for setting the trusted domain and the overwrite URL,
  # which prevents being redirected to the internal Pod IP or localhost.
  nextcloud-config.php: |
    <?php
    $CONFIG = array (
      'overwrite.cli.url' => 'http://{{ .Values.nextcloud.host }}',
      'overwritehost' => '{{ .Values.nextcloud.host }}',
      'trusted_domains' =>
      array (
        0 => 'localhost',
        1 => '{{ .Release.Name }}-{{ .Chart.Name }}',
        2 => '{{ .Values.nextcloud.host }}',
      ),
    );

==================================================
File: .\charts\nextcloud-chart\templates\deployment.yaml
==================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "nextcloud.fullname" . }}
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "nextcloud.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "nextcloud.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ include "nextcloud.name" . }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            # Determine which secret to use. If an existing one is provided, use it.
            # Otherwise, use the one created by this chart.
            {{- $secretName := "" }}
            {{- if .Values.database.existingSecret }}
            {{- $secretName = .Values.database.existingSecret }}
            {{- else }}
            {{- $secretName = printf "%s-db" (include "nextcloud.fullname" .) }}
            {{- end }}
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: POSTGRES_HOST
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: POSTGRES_DB
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: POSTGRES_USER
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: POSTGRES_PASSWORD
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: NEXTCLOUD_ADMIN_USER
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ $secretName }}
                  key: NEXTCLOUD_ADMIN_PASSWORD
          volumeMounts:
            - name: nextcloud-data
              mountPath: /var/www/html
            - name: nextcloud-config
              mountPath: /var/www/html/config/autoconfig.php
              subPath: nextcloud-config.php
      volumes:
        - name: nextcloud-data
          {{- if .Values.persistence.enabled }}
          persistentVolumeClaim:
            claimName: {{ include "nextcloud.fullname" . }}-data
          {{- else }}
          emptyDir: {}
          {{- end }}
        - name: nextcloud-config
          configMap:
            name: {{ include "nextcloud.fullname" . }}-config

==================================================
File: .\charts\nextcloud-chart\templates\NOTES.txt
==================================================
{{- /*
This NOTES.txt file is displayed to the user after a successful `helm install` or `helm upgrade`.
*/}}
{{- $fullName := include "nextcloud.fullname" . -}}
{{- $servicePort := .Values.service.port -}}
{{- $serviceType := .Values.service.type -}}

Nextcloud has been deployed. Congratulations!

Your release is named: {{ .Release.Name }}

 _____  ___    _______  ___  ___  ___________  ______   ___        ______    ____  ____  ________
(\"   \|"  \  /"     "||"  \/"  |("     _   ")/" _  "\ |"  |      /    " \  ("  _||_ " ||"      "\
|.\\   \    |(: ______) \   \  /  )__/  \\__/(: ( \___)||  |     // ____  \ |   (  ) : |(.  ___  :)
|: \.   \\  | \/    |    \\  \/      \\_ /    \/ \     |:  |    /  /    ) :)(:  |  | . )|: \   ) ||
|.  \    \. | // ___)_   /\.  \      |.  |    //  \ _   \  |___(: (____/ //  \\ \__/ // (| (___\ ||
|    \    \ |(:      "| /  \   \     \:  |   (:   _) \ ( \_|:  \\        /   /\\ __ //\ |:       :)
 \___|\____\) \_______)|___/\___|     \__|    \_______) \_______)\"_____/   (__________)(________/


**IMPORTANT NEXT STEPS:**

{{- if eq $serviceType "LoadBalancer" }}
1. **Get the Nextcloud URL:**

   The Load Balancer is being created by your cloud provider. It may take a few minutes.
   Run the following command to get the EXTERNAL-IP (it will change from <pending> to a hostname):

   kubectl get svc --namespace {{ .Release.Namespace }} -w {{ $fullName }}

2. **Update Nextcloud Host Configuration:**

   Once you have the external hostname from step 1, you MUST upgrade the Helm release
   so Nextcloud knows its own address. This prevents redirect errors.

   Replace `YOUR_LOADBALANCER_HOSTNAME` with the address from step 1:

   helm upgrade --namespace {{ .Release.Namespace }} {{ .Release.Name }} . \
     --set nextcloud.host=YOUR_LOADBALANCER_HOSTNAME

   After the upgrade, you can access your Nextcloud instance at:
   http://YOUR_LOADBALANCER_HOSTNAME

{{- else if eq $serviceType "NodePort" }}
1. **Get the Nextcloud URL:**

   Get the IP address of any node in your cluster:
   export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")

   Your Nextcloud instance is accessible at:
   http://$NODE_IP:{{ .Values.service.nodePort }}

   **NOTE:** You still need to run `helm upgrade` with the correct `nextcloud.host` value (e.g., http://$NODE_IP) to ensure Nextcloud works correctly.

{{- else if eq $serviceType "ClusterIP" }}
1. **Accessing Nextcloud:**

   Your service is of type `ClusterIP`, so it is not exposed outside the cluster by default.
   You can access it using `kubectl port-forward`:

   kubectl port-forward --namespace {{ .Release.Namespace }} svc/{{ $fullName }} 8080:{{ $servicePort }}

   Then open your browser to http://127.0.0.1:8080

{{- end }}

**Admin Credentials:**

{{- if not .Values.database.existingSecret }}
   Username: {{ .Values.nextcloud.admin.user }}
   Password: {{ .Values.nextcloud.admin.password }}

   **WARNING:** This is the default password from your values.yaml. Please change it and use a more secure method for production environments.
{{- else }}
   The admin credentials are held in the existing secret you provided: '{{ .Values.database.existingSecret }}'.
   Please retrieve them from there.
{{- end }}

==================================================
File: .\charts\nextcloud-chart\templates\pvc.yaml
==================================================
{{- if .Values.persistence.enabled }}
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ include "nextcloud.fullname" . }}-data
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: {{ .Values.persistence.storageClassName | quote }}
  resources:
    requests:
      storage: {{ .Values.persistence.size | quote }}
{{- end }}

==================================================
File: .\charts\nextcloud-chart\templates\secret.yaml
==================================================
{{- if and .Values.database.enabled (not .Values.database.existingSecret) -}}
apiVersion: v1
kind: Secret
metadata:
  name: {{ include "nextcloud.fullname" . }}-db
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
type: Opaque
data:
  # General Nextcloud Admin Credentials
  NEXTCLOUD_ADMIN_USER: {{ .Values.nextcloud.admin.user | b64enc | quote }}
  NEXTCLOUD_ADMIN_PASSWORD: {{ .Values.nextcloud.admin.password | b64enc | quote }}

  # Database Credentials
  POSTGRES_HOST: {{ .Values.database.host | b64enc | quote }}
  POSTGRES_USER: {{ .Values.database.user | b64enc | quote }}
  POSTGRES_PASSWORD: {{ .Values.database.password | b64enc | quote }}
  POSTGRES_DB: {{ .Values.database.database | b64enc | quote }}
{{- end -}}

==================================================
File: .\charts\nextcloud-chart\templates\service.yaml
==================================================
apiVersion: v1
kind: Service
metadata:
  name: {{ include "nextcloud.fullname" . }}
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "nextcloud.selectorLabels" . | nindent 4 }}

==================================================
File: .\charts\nextcloud-chart\templates\tests\test-connection.yaml
==================================================
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "nextcloud.fullname" . }}-test-connection"
  labels:
    {{- include "nextcloud.labels" . | nindent 4 }}
  annotations:
    # This is the magic annotation that makes this a Helm test.
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      # We use a busybox image because it's small and contains wget.
      image: busybox
      command: ['wget']
      # We target the service internally via its Kubernetes service name.
      # The /status.php endpoint is a good health check for Nextcloud.
      # The '--spider' flag tells wget to check for existence without downloading.
      # The '-T 10' sets a 10-second timeout.
      args: ['-q', '--spider', '-T', '10', 'http://{{ include "nextcloud.fullname" . }}:{{ .Values.service.port }}/status.php']
  # Tests should not run forever. If they fail, they should just fail.
  restartPolicy: Never

==================================================
File: .\charts\nextcloud-chart\templates\_helpers.tpl
==================================================
{{/*
Expand the name of the chart.
*/}}
{{- define "nextcloud.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
This is the full name of a release.
*/}}
{{- define "nextcloud.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}

{{/*
Create common chart labels.
*/}}
{{- define "nextcloud.labels" -}}
helm.sh/chart: {{ include "nextcloud.name" . }}-{{ .Chart.Version }}
app.kubernetes.io/name: {{ include "nextcloud.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
{{- end }}

==================================================
File: .\charts\nextcloud-chart\values.yaml
==================================================
replicaCount: 1

image:
  repository: nextcloud
  pullPolicy: IfNotPresent
  tag: ""

service:
  type: LoadBalancer
  port: 80

persistence:
  enabled: true
  storageClassName: "ebs-sc"
  size: 10Gi

nextcloud:
  host: ""
  admin:
    user: "admin"
    # It is strongly recommended to override this password via --set or external secrets.
    # Storing plain-text passwords in values.yaml is not a secure practice.
    password: "SuperSecurePassword!"

database:
  # If you want the chart to create a secret for you, set enabled to true.
  enabled: true
  # If using an existing secret, provide its name here. `enabled` should be false if you use this.
  existingSecret: ""

  # Values for the secret if it is created by the chart.
  # It is strongly recommended to override these via --set or external secrets.
  user: "nextcloudadmin"
  password: "AnotherSuperSecurePassword!"
  database: "nextclouddb"
  host: "your-rds-or-db-host.com"

==================================================
File: .\docs\templates\daily_scrum.md
==================================================
# Daily Scrum Log - Sprint [Nummer]

---
**Datum:** [Datum]
**Teilnehmer:** Nenad Stevic (Dev Team, SM)

**Was wurde seit dem letzten Daily Scrum erreicht, um das Sprint-Ziel zu unterstützen?**
*   ...
*   ...

**Was wird bis zum nächsten Daily Scrum getan, um das Sprint-Ziel zu unterstützen?**
*   ...
*   ...

**Gibt es Hindernisse (Impediments), die den Fortschritt blockieren oder verlangsamen?**
*   [ ] Nein
*   [ ] Ja: [Beschreibung des Impediments und ggf. erste Lösungsansätze oder wer helfen kann]
    *   Status: [Offen / In Klärung / Gelöst]

**Aktualisierung Project Board:**
*   [ ] Tasks auf dem Board aktualisiert (To Do -> In Progress -> Done).
---
*(Neuer Eintrag pro Arbeitstag am Projekt)*

==================================================
File: .\docs\templates\sprint_planning.md
==================================================
# Sprint Planning - Sprint [Nummer]

**Datum:** [Datum des Plannings]
**Dauer:** [z.B. 1 Stunde]

## Teilnehmer
*   Product Owner: Nenad Stevic
*   Scrum Master: Nenad Stevic
*   Development Team: Nenad Stevic

## 1. Was kann in diesem Sprint geliefert werden? (Sprint Goal Definition)
*   Diskussion der wichtigsten Product Backlog Items (präsentiert durch PO).
*   Kapazitätsbetrachtung des Development Teams.
*   **Sprint-Ziel:** [Hier das gemeinsam definierte, spezifische Sprint-Ziel eintragen]

## 2. Wie wird die gewählte Arbeit erledigt? (Sprint Backlog Erstellung)
*   **Ausgewählte Product Backlog Items (User Stories) für diesen Sprint:**
    *   `[ID-US1]`: [Titel der User Story 1] - *Geschätzter Aufwand (optional): [z.B. S, M, L oder Story Points]*
    *   `[ID-US2]`: [Titel der User Story 2] - *Geschätzter Aufwand (optional):*
    *   ...
*   **Aufbrechen in Tasks (optional, kann direkt auf dem Board erfolgen):**
    *   Für `[ID-US1]`:
        *   Task 1.1: ...
        *   Task 1.2: ...
*   **Identifizierte Abhängigkeiten oder Risiken für diesen Sprint:**
    *   ...
*   **Plan zur Erreichung des Sprint-Ziels:**
    *   *(Kurze Beschreibung der Vorgehensweise oder wichtiger technischer Überlegungen)*

## Ergebnis
*   Definiertes Sprint-Ziel.
*   Committetes Sprint Backlog (Liste der User Stories für den Sprint).
*   (Optional) Erste Tasks für die User Stories definiert.

==================================================
File: .\docs\templates\sprint_retro.md
==================================================
# Sprint Retrospektive - Sprint [Nummer]

**Datum:** [Datum der Retro, üblicherweise nach dem Review]
**Dauer:** [z.B. 30-45 Minuten]

## Teilnehmer
*   Product Owner: Nenad Stevic
*   Scrum Master: Nenad Stevic
*   Development Team: Nenad Stevic

## Zweck
*   Reflexion über den vergangenen Sprint hinsichtlich Personen, Beziehungen, Prozessen und Werkzeugen.
*   Identifikation und Priorisierung von Aspekten, die gut gelaufen sind und Verbesserungspotenzial haben.
*   Erstellung eines Plans zur Umsetzung von Verbesserungen im nächsten Sprint.

## Diskussion
*   **Was lief gut in diesem Sprint? (Keep doing)**
    *   ...
    *   ...
*   **Was lief nicht so gut / könnte verbessert werden? (Stop doing / Start doing / Improve)**
    *   ...
    *   ...
*   **Welche konkreten Massnahmen werden wir im nächsten Sprint umsetzen, um uns zu verbessern? (Action Items)**
    *   Massnahme 1: [Beschreibung der Massnahme, verantwortlich: Dev-Team/PO/SM, ggf. als Task im nächsten Sprint anlegen]
    *   Massnahme 2: ...

## Ergebnis
*   Identifizierte Verbesserungspunkte.
*   Konkreter Aktionsplan für den nächsten Sprint.

==================================================
File: .\docs\templates\sprint_review.md
==================================================
# Sprint Review - Sprint [Nummer]

**Datum:** [Datum des Reviews]
**Dauer:** [z.B. 30-60 Minuten]

## Teilnehmer
*   Product Owner: Nenad Stevic
*   Scrum Master: Nenad Stevic
*   Development Team: Nenad Stevic
*   (Optional, falls Besprechung mit Experten: Experten als Stakeholder)

## 1. Einleitung
*   Scrum Master begrüsst und erklärt den Zweck des Meetings.
*   Product Owner erläutert das Sprint-Ziel und welche Product Backlog Items (PBIs) committet wurden.

## 2. Präsentation des Inkrements ("Done" Work)
*   Development Team demonstriert die im Sprint fertiggestellten PBIs.
    *   PBI 1 (`[ID-USX]`): [Titel] - Kurze Demo / Erklärung des Ergebnisses.
    *   PBI 2 (`[ID-USY]`): [Titel] - Kurze Demo / Erklärung des Ergebnisses.
    *   ...
*   Diskussion und Beantwortung von Fragen zum Inkrement.

## 3. Product Backlog Status und Ausblick
*   Product Owner gibt einen Überblick über den aktuellen Stand des Product Backlogs.
*   Diskussion über mögliche nächste Schritte und Prioritäten für kommende Sprints.
*   (Ggf. Anpassung des Product Backlogs basierend auf Feedback oder neuen Erkenntnissen).

## 4. Feedback der Stakeholder (falls zutreffend, z.B. von Experten)
*   [Notizen zum erhaltenen Feedback]
*   [Ggf. abgeleitete neue PBIs oder Anpassungen]

## Ergebnis
*   Feedback zum Produktinkrement erhalten.
*   Product Backlog ggf. aktualisiert.
*   Basis für das nächste Sprint Planning geschaffen.

==================================================
File: .\docs\templates\user_story.md
==================================================
## User Story: [Kurzer, prägnanter Titel der User Story]

**Als** [Rolle/Persona, z.B. Student, Entwickler, System]
**Möchte ich** [Ziel/Bedürfnis, das erreicht werden soll]
**Damit** [Nutzen/Wert, der dadurch entsteht]

---

### Akzeptanzkriterien:
*   [ ] Kriterium 1: [Spezifische, messbare Bedingung, die erfüllt sein muss]
*   [ ] Kriterium 2: ...
*   [ ] Kriterium 3: ...
*   [ ] (Optional) Kriterium X: Dokumentation der Umsetzung im Haupt-README ist erfolgt.

---

### Notizen / Technische Überlegungen (optional):
*   ...
*   ...

---

### Zugehöriges Epic:
*   `[EPIC-ID]`

### Geschätzter Aufwand (optional):
*   [z.B. S, M, L oder Story Points]

==================================================
File: .\README.md
==================================================
# Semesterarbeit: End-to-End CI/CD-Pipeline mit Terraform, Helm und GH Actions für Nextcloud auf Kubernetes (AWS EKS)

![Header Bild](assets/header.png)

[![Deploy Nextcloud to EKS](https://github.com/Stevic-Nenad/Nextcloud/actions/workflows/deploy.yml/badge.svg?branch=master)](https://github.com/Stevic-Nenad/Nextcloud/actions/workflows/deploy.yml)

**Student:** Nenad Stevic<br>
**TBZ Lehrgang dipl. Informatiker/in HF - 3. Semester**<br>
**Abgabedatum:** 09.07.2025

# Inhaltsverzeichnis

- [1. Einleitung](#1-einleitung)
    - [1.1 Problemstellung](#11-problemstellung)
    - [1.2 Projektziele](#12-projektziele)
    - [1.3 Vorgehensweise](#13-vorgehensweise)
    - [1.4 Zusammenfassung](#14-zusammenfassung)
    - [1.5 Scope](#15-scope)
- [2. Projektmanagement](#2-projektmanagement)
    - [2.1 Scrum](#21-scrum)
        - [2.1.1 Rollen](#211-rollen)
        - [2.1.2 Artefakte](#212-artefakte)
        - [2.1.3 Zeremonien](#213-zeremonien)
        - [2.1.4 Definition of Done (DoD)](#214-definition-of-done-dod)
        - [2.1.5 Definition of Ready (DoR)](#215-definition-of-ready-dor)
    - [2.2 Projektplanung](#22-projektplanung)
        - [2.2.1 Der Grobplan](#221-der-grobplan)
        - [2.2.2 Strukturierung](#222-strukturierung)
        - [2.2.3 Von Epics zu Sprints: Die iterative Feinplanung](#223-von-epics-zu-sprints-die-iterative-feinplanung)
    - [2.3 Sprint-Durchführung und Dokumentation](#23-sprint-durchführung-und-dokumentation)
    - [2.4 Risiken](#24-risiken)
    - [2.5 Stakeholder und Kommunikation](#25-stakeholder-und-kommunikation)
- [3. Evaluation](#3-evaluation)
    - [3.1 Evaluation von Lösungen](#31-evaluation-von-lösungen)
        - [3.1.1 Cloud Provider (AWS)](#311-cloud-provider-aws)
        - [3.1.2 Container Orchestrierung (Kubernetes - EKS)](#312-container-orchestrierung-kubernetes---eks)
        - [3.1.3 Infrastructure as Code (Terraform)](#313-infrastructure-as-code-terraform)
        - [3.1.4 Application Configuration Management (Helm)](#314-application-configuration-management-helm)
        - [3.1.5 CI/CD Werkzeug (GitHub Actions)](#315-cicd-werkzeug-github-actions)
        - [3.1.6 Entwicklungswerkzeuge und Versionen](#316-entwicklungswerkzeuge-und-versionen)
    - [3.2 Theoretische Grundlagen](#32-theoretische-grundlagen)
        - [3.2.1 Infrastructure as Code (IaC) - Prinzipien](#321-infrastructure-as-code-iac---prinzipien)
        - [3.2.2 CI/CD - Konzepte und Phasen](#322-cicd---konzepte-und-phasen)
        - [3.2.3 Kubernetes - Kernkomponenten](#323-kubernetes---kernkomponenten)
        - [3.2.4 Helm - Charts, Releases, Templates](#324-helm---charts-releases-templates)
        - [3.2.5 Nextcloud auf Kubernetes - Architekturüberlegungen](#325-nextcloud-auf-kubernetes---architekturüberlegungen)
    - [3.3 System-Design / Architektur](#33-system-design--architektur)
        - [3.3.1 Logische Gesamtarchitektur](#331-logische-gesamtarchitektur)
        - [3.3.2 AWS Netzwerkarchitektur (VPC Detail)](#332-aws-netzwerkarchitektur-vpc-detail)
        - [3.3.3 Komponenten und Datenflüsse](#333-komponenten-und-datenflüsse)
        - [3.3.4 AWS EKS Architektur Detail](#334-aws-eks-architektur-detail)
- [4. Implementierung und Technische Umsetzung](#4-implementierung-und-technische-umsetzung)
    - [4.1 Infrastruktur-Provisionierung mit Terraform](#41-infrastruktur-provisionierung-mit-terraform)
        - [4.1.1 Terraform Code-Struktur und Module](#411-terraform-code-struktur-und-module)
        - [4.1.2 Provisionierung des Netzwerks (VPC)](#412-provisionierung-des-netzwerks-vpc)
        - [4.1.3 Provisionierung des EKS Clusters und der ECR](#413-provisionierung-des-eks-clusters-und-der-ecr)
        - [4.1.4 Provisionierung der RDS Datenbank und IAM-Rollen](#414-provisionierung-der-rds-datenbank-und-iam-rollen)
        - [4.1.5 Secrets Management für Terraform](#415-secrets-management-für-terraform)
    - [4.2 Nextcloud Helm Chart Entwicklung](#42-nextcloud-helm-chart-entwicklung)
        - [4.2.1 Helm Chart Struktur](#421-helm-chart-struktur)
        - [4.2.2 Wichtige Templates](#422-wichtige-templates)
        - [4.2.3 Konfigurationsmöglichkeiten über `values.yaml`](#423-konfigurationsmöglichkeiten-über-valuesyaml)
    - [4.3 CI/CD Pipeline mit GitHub Actions](#43-cicd-pipeline-mit-github-actions)
        - [4.3.1 Workflow-Definition](#431-workflow-definition)
        - [4.3.2 Authentifizierung gegenüber AWS (OIDC)](#432-authentifizierung-gegenüber-aws-oidc)
        - [4.3.3 Integrationsschritte (Terraform, Helm)](#433-integrationsschritte-terraform-helm)
        - [4.3.4 Secrets Management in der Pipeline](#434-secrets-management-in-der-pipeline)
    - [4.4 Installation und Inbetriebnahme der Gesamtlösung](#44-installation-und-inbetriebnahme-der-gesamtlösung)
        - [4.4.1 Voraussetzungen](#441-voraussetzungen)
        - [4.4.2 Klonen des Repositorys](#442-klonen-des-repositorys)
        - [4.4.3 Konfiguration von Umgebungsvariablen/Secrets](#443-konfiguration-von-umgebungsvariablensecrets)
        - [4.4.4 Ausführen der Pipeline / Manuelle Schritte](#444-ausführen-der-pipeline--manuelle-schritte)
        - [4.4.5 Zugriff auf die Nextcloud Instanz](#445-zugriff-auf-die-nextcloud-instanz)
    - [4.5 Anpassung von Software / Konfiguration von Geräten](#45-anpassung-von-software--konfiguration-von-geräten)
        - [4.5.1 Nextcloud-spezifische Konfigurationen (via Helm)](#451-nextcloud-spezifische-konfigurationen-via-helm)
        - [4.5.2 Wichtige AWS Service-Konfigurationen](#452-wichtige-aws-service-konfigurationen)
- [5. Testing und Qualitätssicherung](#5-testing-und-qualitätssicherung)
    - [5.1 Teststrategie](#51-teststrategie)
        - [5.1.1 Statische Code-Analyse (Linting)](#511-statische-code-analyse-linting)
        - [5.1.2 Validierung der Infrastruktur-Konfiguration](#512-validierung-der-infrastruktur-konfiguration)
        - [5.1.3 Manuelle Funktionstests der Nextcloud Instanz](#513-manuelle-funktionstests-der-nextcloud-instanz)
        - [5.1.4 End-to-End Tests der CI/CD Pipeline](#514-end-to-end-tests-der-cicd-pipeline)
    - [5.2 Testfälle und Protokolle](#52-testfälle-und-protokolle)
        - [5.2.1 Nachweise der Testergebnisse](#521-nachweise-der-testergebnisse)
- [6. Projektdokumentation (Zusammenfassung)](#6-projektdokumentation-zusammenfassung)
    - [6.1 Verzeichnisse und Zusammenfassungen](#61-verzeichnisse-und-zusammenfassungen)
    - [6.2 Quellenangaben und verwendete Werkzeuge](#62-quellenangaben-und-verwendete-werkzeuge)
- [7. Reflexion und Erkenntnisse](#7-reflexion-und-erkenntnisse)
    - [7.1 Abgleich von Theorie und Praxis](#71-abgleich-von-theorie-und-praxis)
    - [7.2 Eigene Erfahrungen und persönlicher Lernprozess](#72-eigene-erfahrungen-und-persönlicher-lernprozess)
    - [7.3 Bewertung der eigenen Lösung und Verbesserungspotenzial](#73-bewertung-der-eigenen-lösung-und-verbesserungspotenzial)
    - [7.4 Handlungsempfehlungen für das weitere Vorgehen](#74-handlungsempfehlungen-für-das-weitere-vorgehen)
- [8. Anhänge](#8-anhänge)
    - [8.1 Verwendete Scrum-Vorlagen (Templates)](#81-verwendete-scrum-vorlagen-templates)
    - [8.2 Weitere Referenzen](#82-weitere-referenzen)
    - [8.3 Link zum GitHub Repository](#83-link-zum-github-repository)
    - [8.4 Link zum GitHub Project Board](#84-link-zum-github-project-board)

---

## 1. Einleitung

*In diesem Kapitel wird das Projekt, die Kriterien und die Vorgehensweise genauer unter die Lupe genommen. Damit wird
ein Überblick über die geplante Arbeit geschaffen, was die Auswertung der Ergebnisse am Schluss vereinfachen soll.*

### 1.1 Problemstellung

Das Hosten und Verwalten von Webanwendungen mit Datenbankanbindung stellt in der heutigen IT-Welt mehrere
Herausforderungen dar. Mit High-Availability Infrastrukturen, welche heute in der Industrie weit verbreitet sind, muss
man deutlich mehr konfigurieren und beachten, als bei "Bare Metal" Servern vor einem Jahrzehnt. Wie kann man mehrere
Instanzen einer Applikation laufen lassen, und trotzdem Datenintegrität auf allen Replicas gewährleisten? Wie kann man
diese verschiedenen Komponenten und Abhängigkeiten erfolgreich konfigurieren, ohne dass Fehler oder Unachtsamkeiten
unterlaufen?

Diese Fragen oder Probleme sind welche, die wir auch in meinem Betrieb begegnen, besonders wenn es darum geht,
Webanwendungen für unsere Kunden zu hosten. Deswegen ist dieses Thema äusserst interessant und motivierend,
Lösungsansätze zu finden, und Erfahrungen zu sammeln die einen messbaren Beitrag zum Berufsleben bringen werden.

### 1.2 Projektziele

Die Ziele der Arbeit wurden nach dem SMART-Prinzip definiert:

1. **Automatisierte Infrastruktur Verwaltung via IaC (Terraform):**<br>
   Die erforderliche Cloud-Infrastruktur auf AWS – bestehend aus einem Elastic Kubernetes Service (EKS) Cluster, einem
   Relational Database Service (RDS) und einer Elastic Container Registry (ECR) – wird vollständig mittels
   Terraform-Code automatisiert erstellt und versioniert. Das Ergebnis ist eine betriebsbereite, aber initial leere
   Kubernetes- und Datenbankumgebung.<br>
   **Deadline: Ende Sprint 3** *(VPC in Sprint 1, EKS/ECR in Sprint 2, RDS/IAM in Sprint 3)*
2. **Entwicklung eines funktionalen Nextcloud Helm Charts:**<br>
   Ein eigenständiges, funktionales Helm Chart für die Nextcloud-Anwendung entwickeln. Dieses Chart ermöglicht die
   Konfiguration von Kubernetes-Deployments, Services, Persistent Volume Claims (PVCs), Datenbankverbindungs-Secrets und
   weiterer Anwendungsparameter über eine `values.yaml`-Datei. Die Funktionalität wird durch `helm template` und
   `helm install` (lokal oder auf EKS) verifiziert.<br>
   **Deadline: Ende Sprint 4**
3. **Implementierung einer CI/CD-Pipeline mit GitHub Actions:**<br>
   Eine automatisierte Continuous Integration / Continuous Deployment (CI/CD) Pipeline unter Verwendung von GitHub
   Actions einrichten. Diese Pipeline wird bei Änderungen im Git-Repository (z.B. Aktualisierung des Nextcloud-Images
   oder der Helm-Chart-Konfiguration) den vollständigen Deployment-Prozess auslösen. Dies beinhaltet optionale Schritte
   wie Image Build/Push und Helm Linting/Packaging.<br>
   **Deadline: Ende Sprint 5**
4. **Bereitstellung einer funktionalen Nextcloud Instanz (via CI/CD-Pipeline):**<br>
   Eine Nextcloud-Instanz mittels der CI/CD-Pipeline auf dem Kubernetes-Cluster bereitstellen. Die Instanz ist extern
   erreichbar, erfolgreich mit der provisionierten Datenbank verbunden und weist Persistenz für Benutzerdaten auf. Dies
   wird durch einen erfolgreichen Login sowie exemplarische Datei-Upload- und Download-Vorgänge demonstriert.<br>
   **Deadline: Ende Sprint 5** *(Manuelles Test-Deployment in Sprint 3, finales Deployment via Pipeline in Sprint 5)*
5. **Umfassende Dokumentation und Code-Bereitstellung:**<br>
   Bis zum Projektende sind die Systemarchitektur, die Terraform-Module, das Helm Chart, die CI/CD-Pipeline, getroffene
   Sicherheitsüberlegungen sowie der gesamte Setup- und Deployment-Prozess detailliert dokumentiert. Der gesamte
   Quellcode (Terraform, Helm, GitHub Actions Workflows) ist in einem Git-Repository versioniert und für die Experten
   zugänglich.<br>
   **Deadline: Ende Sprint 6 (zur Projektabgabe)**

Was diese Arbeit besonders attraktiv macht, ist die hohe Relevanz der verwendeten Technologien und Prinzipien im
beruflichen Alltag eines DevOps Engineers. Der Zeitrahmen von ca. 50 Stunden ist realistisch, vor allem durch vorhandene
Erfahrung in diesem Bereich.

### 1.3 Vorgehensweise

Die geplante Lösung zielt auf die Erstellung einer vollständigen End-to-End-Automatisierung für das Hosten von Nextcloud
auf einem Kubernetes-Cluster (AWS EKS) ab. Der gesamte Prozess basiert auf dem "Infrastructure as Code" (IaC) Prinzip,
bei dem alle Konfigurationen und Komponenten als Code definiert und versioniert werden.

* **Infrastruktur-Provisionierung:** Die Cloud-Infrastruktur (EKS-Cluster, Datenbank, Container Registry etc.) wird
  mittels **Terraform** deklariert und verwaltet.
* **Anwendungs-Deployment auf Kubernetes:** Die Nextcloud-Anwendung selbst wird mithilfe eines eigens entwickelten *
  *Helm Charts** auf dem Kubernetes-Cluster konfiguriert und bereitgestellt. Helm dient hierbei als Paketmanager für
  Kubernetes-Anwendungen.
* **Automatisierung des Lifecycles:** Der gesamte Prozess von der Code-Änderung bis zum Deployment wird durch eine *
  *CI/CD-Pipeline, implementiert mit GitHub Actions**, automatisiert.
* **Methodisches Vorgehen:** Die Umsetzung erfolgt nach agilen Prinzipien, angelehnt an das **Scrum-Framework** (in
  einer auf die Einzelarbeit angepassten Form). Dies beinhaltet eine iterative Entwicklung, regelmässige Reflexion und
  eventuelle Anpassung der Planung, um Flexibilität sicherzustellen. Eine inhaltlich aktuelle Dokumentation begleitet
  den gesamten Prozess.

### 1.4 Zusammenfassung

Diese Semesterarbeit realisiert eine durchgängig automatisierte Pipeline zur Bereitstellung und Verwaltung der
Webanwendung _Nextcloud_ auf einem Kubernetes-Cluster (AWS EKS). Die Lösung nutzt Terraform für die Definition der
Cloud-Infrastruktur, Helm für die Paketierung und Konfiguration von Nextcloud auf Kubernetes sowie GitHub Actions für
die CI/CD-Automatisierung. Das Ziel ist es, eine robuste, wiederholbare und moderne Bereitstellungsmethode zu
implementieren und dabei Kernkompetenzen im Bereich DevOps und Cloud-native Technologien zu vertiefen.

### 1.5 Scope

Zur Sicherstellung der Realisierbarkeit innerhalb des vorgegebenen Zeitrahmens werden folgende Aspekte klar definiert
und abgegrenzt:

* **Im Projektumfang enthalten (In Scope):**
    * Automatisierte Erstellung der Kern-Infrastrukturkomponenten (EKS, RDS, ECR) mittels Terraform.
    * Entwicklung eines funktionalen Helm Charts für Nextcloud, das grundlegende Konfigurationen, Persistenz und
      Datenbankanbindung abdeckt.
    * Implementierung einer CI/CD-Pipeline mit GitHub Actions für das Deployment des Helm Charts auf dem EKS-Cluster.
    * Sichere Handhabung von Secrets für Datenbankzugangsdaten und Pipeline-Authentifizierung.
    * Bereitstellung einer funktionierenden, extern erreichbaren Nextcloud-Instanz mit Datenpersistenz.
    * Dokumentation der gewählten Architektur, der Konfigurationen und des Inbetriebnahme-Prozesses.
* **Nicht im Projektumfang enthalten (Out of Scope):**
    * Implementierung hochkomplexer oder anwendungsspezifischer Nextcloud-Konfigurationen (z.B. Integration externer
      Authentifizierungssysteme, spezifische Nextcloud-Apps über die Basisinstallation hinaus).
    * Entwicklung und Implementierung automatisierter Backup- und Restore-Strategien für Nextcloud-Daten oder die
      Datenbank.
    * Ausgefeilte Monitoring- und Logging-Lösungen für die Nextcloud-Instanz, die über die Standardfunktionalitäten von
      Kubernetes und AWS hinausgehen.
    * Detaillierte Performance-Optimierungen und umfangreiche Lasttests.
    * Unterstützung für Multi-Cloud-Szenarien oder andere Kubernetes-Distributionen als AWS EKS.
    * Erstellung eines benutzerdefinierten Nextcloud Docker-Images (Verwendung des offiziellen Images, sofern nicht
      zwingend anders erforderlich).
    * Tiefgehende Betrachtung von Compliance-Anforderungen oder rechtlichen Aspekten, die über allgemeine Best Practices
      der IT-Sicherheit hinausgehen.

---

## 2. Projektmanagement

*In diesem Kapitel wird das methodische Vorgehen zur Planung, Durchführung und Steuerung des Projekts detailliert
erläutert. Der Fokus liegt auf der konsequenten Anwendung agiler Prinzipien nach dem Scrum-Framework, um eine iterative
Entwicklung, kontinuierliche Verbesserung und transparente Nachvollziehbarkeit des Projektfortschritts zu
gewährleisten.*

### 2.1 Scrum

Für die Durchführung dieser Semesterarbeit wird das agile Framework **Scrum** angewandt. Scrum ermöglicht eine flexible
Reaktion auf sich ändernde Anforderungen, fördert die konstante Lieferung von Fortschritten und legt einen starken Fokus
auf Transparenz. Obwohl Scrum primär für Teams konzipiert ist, werden die Prinzipien und Praktiken hier konsquent in
einer Einzelarbeit simuliert. Scrum ist heutzutage der Standard im IT-Umfeld, da es eine strukturierte Herangehensweise
an komplexe Projekte bietet und die iterative Entwicklung des Produkts unterstützt.

#### Product Goal

Bis zum 09.07.2025 eine vollautomatisierte End-to-End CI/CD-Pipeline mit Terraform, Helm und GitHub Actions zu
implementieren, die eine funktionale, extern erreichbare und persistent datenspeichernde Nextcloud-Instanz auf einem AWS
EKS Kubernetes-Cluster bereitstellt und verwaltet. Die gesamte Lösung ist als Infrastructure as Code versioniert und der
Entwicklungsprozess folgt konsequent den Scrum-Prinzipien.

#### 2.1.1 Rollen

Im Rahmen dieser Semesterarbeit werden alle Scrum-Rollen durch den Studierenden (Nenad Stevic) wahrgenommen. Die klare
Abgrenzung und Erfüllung der jeweiligen Verantwortlichkeiten ist für die Integrität der Arbeit entscheidend:

* **Product Owner (PO):** Verantwortlich für die Definition der Produktvision (basierend auf dem Einreichungsformular
  und den Projektzielen) sowie der Erstellung und Priorisierung des Product Backlogs. Der PO stellt sicher, dass die
  entwickelten Inkremente den Anforderungen entsprechen.
* **Scrum Master (SM):** Verantwortlich für die Einhaltung und korrekte Anwendung des Scrum-Prozesses. Der SM moderiert
  die Scrum Events, beseitigt Hindernisse (Impediments), coacht den Entwicklungsprozess und stellt sicher, dass das
  Team (in diesem Fall der Studierende als Entwickler) effektiv arbeiten kann.
* **Development Team (Dev-Team):** Verantwortlich für die Umsetzung der im Sprint Backlog ausgewählten Product Backlog
  Items (PBIs) in ein funktionsfähiges Inkrement. Das Dev-Team organisiert sich selbst und ist für die technische
  Qualität der Lieferung zuständig.

#### 2.1.2 Artefakte

Die folgenden Scrum Artefakte werden in diesem Projekt eingesetzt:

* **Product Backlog:** Eine dynamische, geordnete Liste aller bekannten Anforderungen, Funktionalitäten, Verbesserungen
  und Fehlerbehebungen, die für das Produkt erforderlich sind. Das Product Backlog wird
  als [GitHub Project Board]([https://github.com/users/Stevic-Nenad/projects/1]) geführt und kontinuierlich gepflegt.
  Jedes Product Backlog Item (PBI) wird als User Story formuliert und enthält Akzeptanzkriterien.
* **Sprint Backlog:** Eine Auswahl von Product Backlog Items, die für einen spezifischen Sprint committet wurden,
  ergänzt um einen Plan zur Lieferung des Produktinkrements und zur Erreichung des Sprint-Ziels. Das Sprint Backlog wird
  ebenfalls auf dem GitHub Project Board visualisiert (z.B. in einer "Sprint Backlog" oder "To Do" Spalte für den
  aktuellen Sprint).
* **Increment:** Die Summe aller im aktuellen Sprint fertiggestellten Product Backlog Items, integriert mit den
  Inkrementen aller vorherigen Sprints. Jedes Inkrement muss potenziell auslieferbar sein und der Definition of Done
  entsprechen. Das Inkrement besteht aus dem lauffähigen Code (Terraform, Helm, GitHub Actions) und der aktualisierten
  Dokumentation.

#### 2.1.3 Zeremonien

Alle Scrum Zeremonien werden zeitlich begrenzt (Time-boxed) und konsequent durchgeführt, um den Inspektions- und
Adaptionszyklus von Scrum zu leben:

* **Sprint Planning:** Zu Beginn jedes Sprints wird das Sprint Planning durchgeführt. Der Product Owner (PO) präsentiert
  die priorisierten Product Backlog Items. Das Development Team (Dev-Team) wählt die Items aus, die es im Sprint
  umsetzen kann, definiert das Sprint-Ziel und plant die konkreten Aufgaben zur Erreichung dieses Ziels.
* **Daily Scrum:** Ein tägliches, maximal 15-minütiges Meeting des Dev-Teams (und des SM), um den Fortschritt in
  Richtung Sprint-Ziel zu synchronisieren und Impediments zu identifizieren. Es werden die drei Fragen beantwortet: Was
  wurde gestern erreicht? Was wird heute getan? Gibt es Hindernisse?
* **Sprint Review:** Am Ende jedes Sprints wird das Inkrement den Stakeholdern (hier den Fachexperten und dem
  Projektmanagement-Experten, repräsentiert durch den PO in der Vorbereitung) präsentiert. Es wird Feedback eingeholt,
  und das Product Backlog wird bei Bedarf angepasst.
* **Sprint Retrospective:** Nach dem Sprint Review und vor dem nächsten Sprint Planning reflektiert das Scrum Team (PO,
  SM, Dev-Team) den vergangenen Sprint. Ziel ist es, den Prozess kontinuierlich zu verbessern, indem positive Aspekte
  identifiziert und Massnahmen zur Optimierung für den nächsten Sprint abgeleitet werden.
* **Backlog Refinement (Product Backlog Grooming):** Obwohl nicht immer als formale Zeremonie bei Einzelarbeiten
  durchgeführt, wird im Laufe jedes Sprints kontinuierlich Zeit für das Product Backlog Refinement eingeplant. Dies
  beinhaltet das Detaillieren und Schätzen von User Stories für kommende Sprints sowie das ggf. Aufteilen grosser
  Stories, um das Product Backlog stets in einem "ready" Zustand zu halten.
  Die detaillierten Protokolle und Ergebnisse jedes Scrum Events werden im
  Abschnitt [2.3 Sprint-Durchführung und Dokumentation](#23-sprint-durchführung-und-dokumentation) für den jeweiligen
  Sprint dokumentiert. Vorlagen für diese Protokolle finden sich
  im [Anhang 8.1](#81-verwendete-scrum-vorlagen-templates).

#### 2.1.4 Definition of Done (DoD)

Die Definition of Done (DoD) ist ein gemeinsames Verständnis darüber, wann ein Product Backlog Item als "fertig" gilt
und somit Teil des Inkrements werden kann. Für diese Semesterarbeit gilt folgende initiale Definition of Done (diese
kann im Laufe des Projekts angepasst und erweitert werden):

* Der Code für das PBI wurde geschrieben und ist auf einem Feature-Branch committet.
* Ein Pull Request (PR) wurde erstellt, vom Entwickler selbst sorgfältig anhand der Akzeptanzkriterien und Anforderungen
  geprüft (Self-Review) und anschliessend in den `main`-Branch gemerged.
* Alle automatisierten Prüfungen (sofern bereits implementiert, z.B. `terraform validate`, `helm lint`) sind erfolgreich
  durchlaufen.
* Infrastrukturänderungen wurden erfolgreich mittels `terraform apply` angewendet und die Funktionalität wurde
  verifiziert.
* Anwendungs-Deployments mittels `helm upgrade --install` waren erfolgreich und die Kernfunktionalität der Anwendung
  wurde überprüft.
* Alle Akzeptanzkriterien des zugehörigen User Story sind erfüllt.
* Die relevante Dokumentation (dieses README, Diagramme, Setup-Anleitungen) wurde aktualisiert, um die Änderungen
  widerzuspiegeln.
* Das PBI wurde auf dem GitHub Project Board in die Spalte "Done" verschoben.

#### 2.1.5 Definition of Ready (DoR)

Die Definition of Ready (DoR) beschreibt, wann ein Product Backlog Item (PBI) – in diesem Projekt eine User Story –
bereit ist, in ein Sprint Planning Meeting aufgenommen und potenziell für einen Sprint ausgewählt zu werden. Sie stellt
sicher, dass die User Story ausreichend vorbereitet und verstanden ist, um eine effiziente Planung und Umsetzung im
Sprint zu ermöglichen.

Für diese Semesterarbeit gilt folgende initiale Definition of Ready:

* **Klar formuliert:** Das PBI ist als User Story im Format "Als [Rolle] möchte ich [Ziel], damit [Nutzen]" formuliert.
* **Verstanden:** Die User Story ist vom Development Team (Student in Entwicklerrolle) inhaltlich verstanden.
  Unklarheiten wurden im Vorfeld (z.B. im Backlog Refinement) geklärt.
* **Akzeptanzkriterien definiert:** Klare, spezifische und testbare Akzeptanzkriterien sind für die User Story
  formuliert.
* **Abhängigkeiten bekannt:** Mögliche Abhängigkeiten zu anderen User Stories oder externen Faktoren sind identifiziert
  und soweit möglich geklärt.
* **Geschätzt:** Die User Story wurde vom Development Team (Student) mit Story Points (oder einer anderen vereinbarten
  Einheit) geschätzt.
* **Klein genug (INVEST - Small):** Die User Story ist so zugeschnitten, dass sie realistischerweise innerhalb eines
  Sprints vom Development Team (Student) abgeschlossen werden kann. Ist sie zu gross, wurde sie im Backlog Refinement
  bereits in kleinere, handhabbare User Stories aufgeteilt.
* **Wertstiftend (INVEST - Valuable):** Die User Story liefert einen erkennbaren Wert für das Produkt oder das
  Projektziel.
* **Testbar (INVEST - Testable):** Es ist klar, wie die Erfüllung der User Story und ihrer Akzeptanzkriterien überprüft
  werden kann.
* **Priorisiert:** Die User Story wurde vom Product Owner (Student in PO-Rolle) im Product Backlog priorisiert.

### 2.2 Projektplanung

*Eine gute Planung ist das A und O, auch wenn man agil unterwegs ist. In diesem Kapitel wird behandelt, wie die Roadmap
zur fertigen Nextcloud-Pipeline auszusehen hat.*

#### 2.2.1 Der Grobplan

Den zeitlichen Rahmen für das Projekt geben die offiziellen Termine der TBZ vor, insbesondere die Einzelbesprechungen
und der finale Abgabetermin. Basierend darauf und auf einer ersten Schätzung der Arbeitsaufwände für die
Hauptkomponenten des Projekts wurde folgender visueller Zeitplan (Gantt-Diagramm) erstellt. Er zeigt die übergeordneten
Projektphasen und die geplanten Sprints im Überblick:

![gantt](assets/gantt.svg)
Gantt Diagramm
*(Stand: 09.05.2025 – Dieser Plan dient als Orientierung und wird iterativ im Detail verfeinert)*

#### 2.2.2 Strukturierung

Um die in [Kapitel 1.2](#12-projektziele) definierten Projektziele greifbar zu machen und die Arbeit sinnvoll zu
bündeln, wurden grössere thematische Arbeitspakete, sogenannte **Epics**, definiert. Jedes Epic repräsentiert einen
wesentlichen Baustein auf dem Weg zum fertigen Produkt und kann sich über mehrere Sprints erstrecken.

Die folgenden Epics bilden das Rückgrat des Product Backlogs für dieses Projekt:

* **`EPIC-PROJMGMT`: Projektinitialisierung & Durchgängige Scrum-Prozessdokumentation**
    * *Ziel:* Schaffung der organisatorischen und dokumentarischen Grundlagen (Repository, `README.md`, Project Board)
      und Sicherstellung der korrekten, nachvollziehbaren Anwendung des Scrum-Frameworks über die gesamte Projektdauer.
* **`EPIC-TF-NET`: AWS Netzwerk-Infrastruktur mit Terraform**
    * *Ziel:* Aufbau eines sicheren und skalierbaren Fundaments in der AWS Cloud mittels Terraform, inklusive VPC,
      Subnetzen, Routing und Internet-Anbindung.
* **`EPIC-TF-K8S`: Kubernetes (EKS) Cluster & Container Registry (ECR) mit Terraform**
    * *Ziel:* Automatisierte Bereitstellung eines managed Kubernetes-Clusters (AWS EKS) und einer privaten Container
      Registry (AWS ECR) mittels Terraform.
* **`EPIC-TF-DB-IAM`: Datenbank (RDS) & zugehörige IAM-Rollen mit Terraform**
    * *Ziel:* Automatisierte Provisionierung einer managed relationalen Datenbank (AWS RDS) und der notwendigen
      IAM-Rollen für den Zugriff durch Anwendungen und Kubernetes-Komponenten.
* **`EPIC-NC-DEPLOY`: Nextcloud Grundlagen (Manuelles Deployment, Persistenz-Konfiguration)**
    * *Ziel:* Manuelle Installation und Konfiguration von Nextcloud auf dem EKS-Cluster, um die grundlegende
      Funktionalität, Datenbankanbindung und Datenpersistenz zu validieren, bevor die Automatisierung mit Helm erfolgt.
* **`EPIC-HELM`: Nextcloud Helm Chart Entwicklung**
    * *Ziel:* Erstellung eines eigenen, robusten und konfigurierbaren Helm Charts für die Nextcloud-Anwendung zur
      Vereinfachung des Deployments und Managements auf Kubernetes.
* **`EPIC-CICD`: CI/CD Pipeline (GitHub Actions) Implementierung**
    * *Ziel:* Aufbau einer vollautomatisierten CI/CD-Pipeline mit GitHub Actions, die Änderungen am Code oder an der
      Konfiguration erkennt und Nextcloud automatisch auf dem EKS-Cluster bereitstellt oder aktualisiert.
* **`EPIC-ABSCHLUSS`: Testing, Finale Dokumentation & Projektabschluss**
    * *Ziel:* Umfassendes Testen der Gesamtlösung, Finalisierung der Projektdokumentation gemäss den Vorgaben und
      Vorbereitung der Abschlusspräsentation.

Alle Epics sind als Issues mit dem Label `epic` auf
dem [GitHub Project Board]([https://github.com/users/Stevic-Nenad/projects/1]) erfasst.

#### 2.2.3 Von Epics zu Sprints: Die iterative Feinplanung

Mit der groben Roadmap (Gantt) und den thematischen Wegweisern (Epics) erfolgt die detaillierte Planung iterativ für
jeden einzelnen Sprint im **Sprint Planning Meeting**. Für jeden Sprint wird ein klares **Sprint-Ziel** definiert. Aus
den für dieses Ziel relevanten Epics werden dann konkrete **User Stories** abgeleitet oder ausgewählt. Diese User
Stories beschreiben eine kleine, wertstiftende Funktionalität aus Nutzersicht und werden mit spezifischen *
*Akzeptanzkriterien** versehen.

Die User Stories für den jeweils aktuellen Sprint bilden das **Sprint Backlog**. Alle weiteren, noch nicht für einen
spezifischen Sprint eingeplanten User Stories verbleiben im **Product Backlog** und werden kontinuierlich durch den
Product Owner (also mich) gepflegt, priorisiert und verfeinert (Backlog Refinement).

Dieser Ansatz – vom Groben ins Feine – stellt sicher, dass das Projekt einerseits eine klare Richtung hat, andererseits
aber die Flexibilität bewahrt wird, auf Erkenntnisse aus vorherigen Sprints reagieren und die Planung anpassen zu
können. Es wird also **nicht das gesamte Projekt mit allen User Stories für alle sechs Sprints im Voraus bis ins letzte
Detail durchgeplant.** Vielmehr wird der Fokus auf eine exzellente Planung des unmittelbar bevorstehenden Sprints
gelegt, während das Product Backlog eine Vorschau auf mögliche Inhalte der Folgesprints bietet.

Die konkrete Umsetzung und Dokumentation der einzelnen Sprints ist im nachfolgenden
Abschnitt [2.3 Sprint-Durchführung](#23-sprint-durchführung) detailliert beschrieben.

### 2.3 Sprint-Durchführung und Dokumentation

*Dieser Abschnitt fasst die Durchführung und die wesentlichen Ergebnisse jedes Sprints zusammen. Jeder Sprint folgt dem
definierten Scrum-Zyklus (Planning, Daily Scrums, Increment-Erstellung, Review, Retrospektive). Die detaillierte
Ausarbeitung der User Stories, ihrer Akzeptanzkriterien und die tägliche Aufgabenverfolgung erfolgen auf
dem [GitHub Project Board](https://github.com/users/Stevic-Nenad/projects/1/views/1). Die hier skizzierten Inhalte für
Sprints 2-6 sind vorläufig und werden im jeweiligen Sprint Planning Meeting finalisiert und committet.*

---

#### **Sprint 0: Bootstrap & Initialplanung**

* **Dauer:** 05. Mai 2025 - 09. Mai 2025 *(Datum ggf. anpassen, wenn Sprint 0 länger dauerte)*
* **Zugehöriges Epic (primär):** `EPIC-PROJEKT`
* **Sprint Planning (simuliert am 05.05.2025):** Basierend auf den Projektanforderungen und den TBZ-Vorgaben wurden das
  untenstehende Sprint-Ziel und die folgenden User Stories als Sprint Backlog für Sprint 0 committet, um die
  Projektbasis zu schaffen.
* **Sprint-Ziel (committet für Sprint 0):**
    * "Die grundlegende Projektinfrastruktur (Repository, Scrum-Board, initiale Dokumentation) ist etabliert, das
      Scrum-Rahmenwerk für das Projekt ist definiert und dokumentiert, und eine erste Grobplanung (Epics, Roadmap) sowie
      die Detailplanung für Sprint 1 sind vorhanden, um eine solide Basis für die erfolgreiche Durchführung der
      Semesterarbeit zu schaffen."
* **Sprint Backlog (committete User Stories für Sprint 0 – siehe
  auch [Sprint 0 auf GitHub Board](DEIN_LINK_ZUM_GITHUB_PROJECT_BOARD_HIER_ODER_FILTER_S0)):**
    * `Nextcloud#33`: GitHub Repository initialisieren
    * `Nextcloud#34`: Scrum-Rahmenwerk im README definieren
    * `Nextcloud#35`: Initiale Projekt- und Sprintplanung durchführen
    * `Nextcloud#36`: Initiale Risikoanalyse durchführen und dokumentieren
    * `Nextcloud#1`: GitHub Projekt-Board einrichten
    * `Nextcloud#3`: GitHub Issue-Vorlagen konfigurieren
* **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    * Krankheitsbedingter Ausfall am 06. & 07.05. erforderte eine Priorisierung der Kernaufgaben für das Sprint-Ziel.
* **Erreichtes Inkrement / Ergebnisse (Stand 09.05.2025 oder aktuelles Enddatum Sprint 0):**
    * Projekt-Repository (`https://github.com/Stevic-Nenad/Nextcloud`) und `README.md` Grundstruktur mit initialen
      Planungsartefakten (Scrum-Prozess inkl. Product Goal, DoD/DoR; Epics-Liste; Risikomatrix; initiale
      Technologie-Evaluation und Architekturskizze in Kap. 3) sind erstellt und committet.
    * GitHub Project Board (https://github.com/users/Stevic-Nenad/projects/1/views/1) mit Spalten, Product Goal, Links
      zu DoD/DoR, Epic-Labels, User Story Vorlagen und initialen User Stories (für Sprint 0 und Product Backlog) ist
      eingerichtet. (Story Points Feld hinzugefügt und Stories initial geschätzt, falls `Nextcloud#Y` erledigt).
    * Sprint 1 ist detailliert geplant und die entsprechenden User Stories sind im Product Backlog angelegt.
* **Sprint Review (Kurzfazit, Stand 09.05.2025):**
    * Die Projektbasis ist erfolgreich gelegt und dokumentiert. Alle oben im Sprint Backlog für Sprint 0 committeten und
      als erledigt markierten User Stories wurden abgeschlossen. Grundlage für die erste Besprechung mit den Experten
      und den Start von Sprint 1 ist geschaffen.
* **Sprint Retrospektive (Wichtigste Aktion, Stand 09.05.2025 oder aktuelles Enddatum Sprint 0):**
    * Die Notwendigkeit von Puffern für Unvorhergesehenes in der Zeitplanung wurde durch den Ausfall verdeutlicht.
      Zukünftige Sprintplanungen werden versuchen, dies besser zu berücksichtigen. Die detaillierte Vorab-Planung der
      User Stories hat geholfen, trotz Zeitdruck den Überblick zu behalten. Klare Checklisten (wie die
      Akzeptanzkriterien) sind sehr hilfreich.

---

#### **Sprint 1: AWS Account, Lokale Umgebung & Terraform Basis-Netzwerk (VPC)**

* **Dauer:** ca. 10. Mai 2025 - 24. Mai 2025
* **Zugehörige Epics (Labels):** `EPIC-PROJEKT`, `EPIC-NETZ`
* **Sprint Planning (simuliert am 09.05.2025):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als Product Owner, Scrum Master, Development Team).
    * **Kontext & Ziel des Plannings:** Nach erfolgreichem Abschluss von Sprint 0, in dem die Projektgrundlagen und das
      Scrum-Rahmenwerk etabliert wurden, zielt dieses erste "echte" Sprint Planning darauf ab, den ersten operativen
      Sprint zu definieren. Das Hauptziel ist es, ein klares Sprint-Ziel für Sprint 1 zu formulieren und die dafür
      notwendigen User Stories aus dem Product Backlog auszuwählen und zu committen.
    * **Input für das Planning:**
        * **Product Goal:** "Bis zum 09.07.2025 eine vollautomatisierte End-to-End CI/CD-Pipeline [...] bereitzustellen
          und zu verwalten."
        * **Priorisiertes Product Backlog:** Die User Stories aus den Epics `EPIC-PROJEKT` (Restarbeiten zur Umgebung)
          und `EPIC-TF-NET` (Netzwerk-Grundlagen) wurden im Vorfeld als am höchsten priorisiert identifiziert, da sie
          die Basis für alle weiteren technischen Arbeiten legen. Alle für dieses Planning relevanten User Stories
          erfüllten die Definition of Ready (DoR).
        * **Kapazität für Sprint 1:** Geschätzte Arbeitszeit für die kommenden zwei Wochen (ca. 10-12 Stunden effektive
          Projektarbeitszeit, unter Berücksichtigung anderer Verpflichtungen).
        * **Definition of Done (DoD):** Die in Sprint 0 definierte DoD dient als Richtlinie für den Abschluss jeder User
          Story.
        * **Erkenntnisse aus Sprint 0:** Die Notwendigkeit einer soliden Basis und einer klaren Umgebungskonfiguration
          wurde unterstrichen.
    * **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):**
        * Der Product Owner (PO) betonte, dass ohne einen sicheren AWS-Zugang, eine funktionierende lokale
          Entwicklungsumgebung und eine grundlegende Netzwerkstruktur keine weiteren Infrastrukturkomponenten sinnvoll
          aufgesetzt werden können.
        * Das Development Team (Dev) stimmte zu und schlug vor, das Sprint-Ziel so zu formulieren, dass es diese
          fundamentalen Bausteine abdeckt, inklusive der wichtigen Aspekte der Kostenkontrolle (Tagging) und der
          zentralen State-Verwaltung für Terraform, um von Anfang an Best Practices zu folgen.
        * Gemeinsam wurde das folgende Sprint-Ziel formuliert und committet:
            * *"Ein sicherer AWS Account und eine lokale Entwicklungsumgebung sind eingerichtet, das Terraform Remote
              Backend ist konfiguriert, und ein grundlegendes, korrekt getaggtes AWS VPC-Netzwerk ist mittels Terraform
              Code definiert, versioniert und erfolgreich provisioniert."*
    * **Diskussion – Das "Was" (Auswahl der Sprint Backlog Items):**
        * Basierend auf dem Sprint-Ziel wurden die folgenden User Stories aus dem Product Backlog als essentiell für
          dessen Erreichung identifiziert:
            * `Nextcloud#37` (AWS Account sicher konfigurieren): Absolute Grundvoraussetzung.
            * `Nextcloud#38` (Lokale Entwicklungsumgebung einrichten): Notwendig für jegliche Code-Entwicklung und
              Tests.
            * `Nextcloud#7` (Kosten-Tags & initiale TF-Provider-Konfig): Wichtig, um von Beginn an Kostenkontrolle zu
              gewährleisten und eine saubere Terraform-Struktur aufzusetzen.
            * `Nextcloud#6` (Terraform Remote Backend konfigurieren): Kritisch für sichere und kollaborative
              State-Verwaltung, bevor komplexe Ressourcen erstellt werden.
            * `Nextcloud#5` (VPC mit Subnetzen via Terraform erstellen): Das Kernstück der Netzwerkinfrastruktur für
              diesen Sprint.
        * Die Aufwände für diese Stories wurden als realistisch für die Sprint-Kapazität eingeschätzt. Jede Story wurde
          kurz durchgegangen, um sicherzustellen, dass die Akzeptanzkriterien klar sind und keine unmittelbaren Blocker
          bestehen.
    * **Diskussion – Das "Wie" (Grobe Planung der Umsetzung):**
        * Das Dev-Team plant, mit der Einrichtung des AWS Accounts und der lokalen Umgebung zu beginnen, da diese die
          Basis für alle Terraform-Arbeiten bilden.
        * Anschliessend wird die initiale Terraform-Provider-Konfiguration mit den Tags und dem Remote Backend
          angegangen.
        * Zuletzt erfolgt die Implementierung des VPC-Netzwerks.
        * Es wurde beschlossen, die Terraform-Struktur von Anfang an modular im `src/terraform/` Verzeichnis aufzubauen.
        * Die Dokumentation (dieses README) wird parallel zur Umsetzung der User Stories aktualisiert.
    * **Commitment:** Das Development Team committet sich zum Erreichen des Sprint-Ziels und zur Fertigstellung der
      ausgewählten Sprint Backlog Items bis zum Ende des Sprints.
* **Sprint-Ziel (committet für Sprint 1):**
    * "Ein sicherer AWS Account und eine lokale Entwicklungsumgebung sind eingerichtet, das Terraform Remote Backend ist
      konfiguriert, und ein grundlegendes, korrekt getaggtes AWS VPC-Netzwerk ist mittels Terraform Code definiert,
      versioniert und erfolgreich provisioniert."
* **Sprint Backlog (committete User Stories für Sprint 1 – siehe
  auch [Sprint 1 auf GitHub Board](LINK_ZU_SPRINT_1_FILTER_ODER_BOARD)):**
    * `Nextcloud#37`: AWS Account sicher konfigurieren
    * `Nextcloud#38`: Lokale Entwicklungsumgebung einrichten
    * `Nextcloud#6`: Terraform Remote Backend konfigurieren
    * `Nextcloud#5`: VPC mit Subnetzen via Terraform erstellen
    * `Nextcloud#7`: Kosten-Tags für AWS Ressourcen definieren und initiale Terraform-Provider-Konfiguration erstellen
* **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    * AWS Free Tier Limits mussten genau geprüft werden um Kosten zu vermeiden
    * Unterschiedliche Installationsmethoden je nach OS erforderten flexible Dokumentation
* **Erreichtes Inkrement / Ergebnisse:**
    * AWS Root Account mit MFA gesichert
    * IAM User "terraform-admin" mit AdministratorAccess Policy erstellt
    * AWS CLI Profile "nextcloud-project" konfiguriert für lokale Entwicklung
    * Access Keys sicher in ~/.aws/credentials gespeichert
    * AWS Budget von $20/Monat mit Benachrichtigungen bei 80% und 100% eingerichtet
    * AWS Region eu-central-1 als Standard festgelegt und in allen Konfigurationen verwendet
    * **Lokale Entwicklungsumgebung vollständig eingerichtet (User Story #38 ✓):**
      - AWS CLI v2.x mit Profile "nextcloud-project"
      - Terraform v1.9.x
      - kubectl v1.30.x
      - Helm v3.15.x
      - IntelliJ Ultimate mit allen Extensions
    * Alle Tools erfolgreich getestet und verifiziert
    * **Initiale Terraform-Konfiguration mit globaler Tagging-Strategie implementiert:**
        * Grundlegende Terraform-Dateistruktur (`versions.tf`, `provider.tf`, `variables.tf`, `locals.tf`) im
          Verzeichnis `src/terraform/` erstellt.
        * AWS Provider konfiguriert, inklusive der Festlegung einer Standardregion (`var.aws_region`).
        * Standard-Tags (`Projekt: Semesterarbeit-Nextcloud`, `Student: NenadStevic`, `ManagedBy: Terraform`) wurden als
          lokale Terraform-Variable (`local.common_tags`) definiert.
        * Diese Tags werden mittels des `default_tags` Blocks in der AWS Provider-Konfiguration automatisch an alle
          zukünftig erstellten, unterstützten Ressourcen propagiert.
        * `terraform init` erfolgreich ausgeführt, um Provider-Plugins zu laden.
        * Diese Konfiguration stellt die Basis für die nachfolgende Erstellung der VPC (User Story #5) dar, welche dann
          diese Tags automatisch erhalten wird. Die Sichtbarkeit der Tags auf Ressourcen wird im Rahmen der
          VPC-Erstellung verifiziert.
    * **Grundlegendes AWS VPC-Netzwerk via Terraform provisioniert (User Story #5 ✓):**
    * Konfigurierbarer VPC CIDR-Block (`10.0.0.0/16` als Standard).
    * Jeweils mindestens zwei öffentliche und zwei private Subnetze über zwei Availability Zones (`eu-central-1a`,
      `eu-central-1b` als Standard) erstellt.
    * Internet Gateway erstellt und der VPC zugeordnet.
    * Routing-Tabellen für öffentliche Subnetze mit Route zum IGW konfiguriert und mit den öffentlichen Subnetzen
      assoziiert.
    * Für Hochverfügbarkeit wurde **ein NAT Gateway pro Availability Zone** in den jeweiligen öffentlichen Subnetzen
      erstellt, jeweils mit einer zugehörigen Elastic IP.
    * **Dedizierte Routing-Tabellen für private Subnetze pro Availability Zone** konfiguriert. Jede dieser Tabellen
      leitet ausgehenden Internetverkehr (`0.0.0.0/0`) über das NAT Gateway in derselben AZ. Die privaten Subnetze sind
      entsprechend mit ihrer AZ-spezifischen privaten Routing-Tabelle assoziiert.
    * `terraform validate`, `plan` und `apply` erfolgreich ausgeführt und Ressourcen verifiziert.
    * Netzwerkarchitektur-Diagramm in Abschnitt [3.3.2](#332-aws-netzwerkarchitektur-vpc-detail) dokumentiert und
      aktualisiert, um die NAT-Gateway-pro-AZ-Architektur darzustellen.
    * Alle Ressourcen sind korrekt mit den globalen Tags (`Projekt`, `Student`, `ManagedBy`) versehen (verifiziert im
      Rahmen des Testfalls für User Story #7).
* **Terraform Remote Backend konfiguriert (User Story #6 ✓):**
    * Die Terraform-Konfiguration der Hauptanwendung (`src/terraform/`) wurde so eingerichtet, dass sie einen extern
      verwalteten S3 Bucket (`nenad-stevic-nextcloud-tfstate`) für die zentrale und sichere Speicherung des Terraform
      States nutzt. Für diesen Bucket sind Versionierung, serverseitige Verschlüsselung (SSE-S3) und die Blockierung des
      öffentlichen Zugriffs vorausgesetzt (und durch die separate Backend-Infrastruktur-Konfiguration sichergestellt).
    * Ebenso wird eine extern verwaltete DynamoDB-Tabelle (`nenad-stevic-nextcloud-tfstate-lock`) für das State Locking
      verwendet, um konkurrierende State-Änderungen zu verhindern.
    * Die Terraform Backend-Konfiguration (`backend "s3" {}`) wurde in `src/terraform/backend.tf` hinzugefügt und
      committet, um diese externen Ressourcen zu nutzen.
    * `terraform init` wurde erfolgreich ausgeführt, um das Remote Backend für die Hauptanwendung zu initialisieren und
      sich mit dem S3 Bucket zu verbinden. *(Anmerkung: Falls die AWS-Probleme dies verhindert haben, muss hier der
      tatsächliche Stand dokumentiert werden...)*
    * Es wurden keine AWS Keys im Code hardcodiert; die Authentifizierung erfolgt über das konfigurierte AWS CLI Profil.
    * Alle projektspezifischen DoD-Punkte für diese User Story (bezogen auf die Konfiguration des Backends in
      `src/terraform/`) sind erfüllt.
* **Sprint Review (durchgeführt am 24.05.2025 – simuliert):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als Product Owner, Scrum Master, Development Team). Die Fachexperten
      werden als die primären Stakeholder betrachtet, denen das Inkrement präsentiert wird.
    * **Sprint-Ziel von Sprint 1 (rekapituliert):** "Ein sicherer AWS Account und eine lokale Entwicklungsumgebung sind
      eingerichtet, das Terraform Remote Backend ist konfiguriert, und ein grundlegendes, korrekt getaggtes AWS
      VPC-Netzwerk ist mittels Terraform Code definiert, versioniert und erfolgreich provisioniert."
    * **Präsentation des Inkrements:**
        * Alle für Sprint 1 committeten User Stories (`#37`, `#38`, `#6`, `#5`, `#7`) wurden erfolgreich abgeschlossen
          und erfüllen ihre jeweiligen Akzeptanzkriterien sowie die Definition of Done.
        * Die sichere AWS-Account-Konfiguration wurde demonstriert (MFA, IAM User, Budget Alerts).
        * Die eingerichtete lokale Entwicklungsumgebung mit allen notwendigen Tools wurde vorgestellt.
        * Die Implementierung der initialen Terraform-Provider-Konfiguration inklusive der globalen Tagging-Strategie
          wurde gezeigt.
        * Die erfolgreiche Provisionierung des VPC-Netzwerks mit öffentlichen/privaten Subnetzen und der
          NAT-Gateway-pro-AZ-Architektur via Terraform wurde demonstriert und die erstellten Ressourcen in der AWS
          Konsole gezeigt (inkl. Verifizierung der Tags).
        * Die Konfiguration des Terraform Remote Backends (S3 & DynamoDB) für die Hauptanwendung wurde erläutert und die
          erfolgreiche Initialisierung (`terraform init`) bestätigt. *(Anmerkung: Falls AWS-Probleme die volle
          Verifizierung des Remote Backend `apply` verhindert haben, sollte dies hier transparent erwähnt werden.)*
        * Die "Self-Review"-Notizen für jede User Story (siehe "Erreichtes Inkrement / Ergebnisse") dienten als
          detaillierte Nachweise der Erfüllung.
    * **Diskussion & Feedback (simuliert):**
        * Das Sprint-Ziel wurde vollständig erreicht.
        * Die klare Struktur des Terraform-Codes und die detaillierte Dokumentation der Einrichtungsschritte wurden
          positiv hervorgehoben.
        * Die NAT-Gateway-pro-AZ-Strategie wurde als gute Designentscheidung für Hochverfügbarkeit bewertet.
        * Es wurde angemerkt, dass die initiale Nutzung von `AdministratorAccess` für den IAM User zwar für den Start
          praktikabel war, aber für spezifischere Aufgaben in späteren Sprints (z.B. CI/CD Pipeline) feingranularere
          Berechtigungen (Least Privilege) empfohlen werden (bereits in der Retrospektive als Punkt aufgenommen).
    * **Anpassungen am Product Backlog:**
        * Aufgrund des Feedbacks und der Ergebnisse dieses Reviews sind aktuell keine unmittelbaren Änderungen oder
          neuen Items für das Product Backlog erforderlich. Die bestehende Priorisierung bleibt gültig.
    * **Fazit:** Der Sprint war erfolgreich. Das Inkrement bildet eine solide Grundlage für die weiteren Sprints.
* **Sprint Retrospektive (durchgeführt am 24.05.2025 – simuliert):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als Product Owner, Scrum Master, Development Team).
    * **Ziel der Retrospektive:** Den abgelaufenen Sprint 1 reflektieren, um positive Aspekte zu identifizieren,
      Verbesserungspotenziale aufzudecken und konkrete Massnahmen für den nächsten Sprint abzuleiten, um den Prozess und
      die Zusammenarbeit (auch in der Einzelarbeit) kontinuierlich zu optimieren.
    * **Diskussion – Was lief gut in Sprint 1?**
        * **Klare Zielsetzung:** Das Sprint-Ziel war von Anfang an klar und half, den Fokus während des Sprints zu
          behalten.
        * **Strukturierte Planung:** Die detaillierte User Story Aufteilung und die Akzeptanzkriterien aus dem Sprint
          Planning waren sehr hilfreich bei der Umsetzung und dem Self-Review.
        * **Frühe Etablierung von Best Practices:** Die Entscheidung, das Terraform Remote Backend (`Nextcloud#6`) und
          die globale Tagging-Strategie (`Nextcloud#7`) früh im Projekt zu implementieren, wurde als positiv bewertet,
          da dies eine solide und nachvollziehbare Basis schafft.
        * **Problemlösungskompetenz:** Die Herausforderungen bei der Recherche der AWS Free Tier Limits und der
          unterschiedlichen Installationsmethoden für die lokale Umgebung (`Nextcloud#38`) konnten erfolgreich
          gemeistert werden.
        * **Dokumentationsdisziplin:** Die parallele Aktualisierung des `README.md` mit den Fortschritten und
          technischen Details hat sich bewährt.
    * **Diskussion – Was könnte in Sprint 2 verbessert werden?**
        * **Genauigkeit der Aufwandsschätzung:** Obwohl die User Stories für Sprint 1 abgeschlossen wurden, war der
          tatsächliche Zeitaufwand für die Recherche und Implementierung des VPC-Netzwerks (`Nextcloud#5`) mit der
          NAT-Gateway-pro-AZ-Lösung etwas höher als initial grob geschätzt. Hier könnte für zukünftige Sprints eine noch
          detailliertere Vorab-Recherche für komplexere Tasks helfen.
        * **IAM-Berechtigungen (Least Privilege):** Die Nutzung der `AdministratorAccess` Policy für den initialen IAM
          User (`Nextcloud#37`) war zwar pragmatisch für den Start, entspricht aber nicht dem "Least Privilege"-Prinzip.
          Dies birgt unnötige Sicherheitsrisiken, auch wenn es sich um eine Entwicklungsumgebung handelt.
        * **Proaktive Impediment-Dokumentation:** Kleinere "Stolpersteine" (z.B. spezifische Terraform-Provider-Version
          Kompatibilitäten) wurden gelöst, aber nicht immer sofort als potenzielles Impediment im Daily Scrum (
          simuliert) festgehalten. Eine konsequentere Erfassung könnte helfen, Muster zu erkennen.
    * **Abgeleitete Action Items für Sprint 2 (und darüber hinaus):**
        1. **IAM Policy Verfeinerung (Höchste Priorität):** Für alle neu zu erstellenden IAM-Rollen (insbesondere für
           EKS, EBS CSI Driver in Sprint 2 und später die CI/CD-Pipeline) werden von Beginn an spezifische, auf den
           tatsächlichen Bedarf zugeschnittene IAM Policies erstellt und verwendet, anstatt pauschale Admin-Rechte zu
           vergeben. Dies wird das primäre Learning sein, das in Sprint 2 umgesetzt wird.
        2. **Detailliertere Recherche bei komplexen Tasks:** Vor dem Commit zu User Stories, die absehbar hohe
           Komplexität oder viele unbekannte Variablen haben (z.B. EKS Cluster Setup), wird etwas mehr Zeit für eine
           fokussierte Vorab-Recherche eingeplant, um die Schätzgenauigkeit zu verbessern und potenzielle Fallstricke
           früher zu identifizieren.
        3. **Konsequentere Impediment-Erfassung:** Auch kleinere technische Hürden oder unerwartete Verhaltensweisen von
           Tools werden bewusster als (potenzielle) Impediments im (simulierten) Daily Scrum festgehalten, um den
           Lernprozess und die Transparenz zu fördern.

---

#### **Sprint 2: Terraform für EKS Cluster & ECR**

* **Dauer:** ca. 25. Mai 2025 - 01. Juni 2025
* **Zugehörige Epics:** `EPIC-TF-K8S`
* **Sprint Planning (durchgeführt am 24.05.2025 – simuliert):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als Product Owner, Scrum Master, Development Team).
    * **Ziel des Plannings:** Definition des Sprint-Ziels für Sprint 2, Auswahl der User Stories aus dem Product Backlog
      und Planung der Umsetzung.
    * **Input:**
        * Product Backlog (priorisierte User Stories für `EPIC-TF-K8S`).
        * Aktuelles Produktinkrement (Ergebnisse aus Sprint 1, insbesondere das provisionierte VPC).
        * Voraussichtliche Kapazität für Sprint 2 (ca. 6 Stunden).
        * Erkenntnisse aus der Sprint 1 Retrospektive.
* **Sprint-Ziel (committet für Sprint 2):**
    * "Ein funktionsfähiger AWS EKS Kubernetes-Cluster mit konfigurierten Node Groups und einem IAM OIDC Provider ist
      mittels Terraform automatisiert provisioniert. Zusätzlich ist ein AWS ECR Repository für Docker-Images via
      Terraform erstellt und der AWS EBS CSI Driver im EKS Cluster für persistente Volumes installiert und
      konfiguriert."
* **Sprint Backlog (committete User Stories für Sprint 2 – siehe
  auch [Sprint 2 auf GitHub Board]([DEIN_AKTUALISIERTER_LINK_HIER])): **
    * `Nextcloud#8`: EKS Cluster mit Node Groups provisionieren (via Terraform).
    * `Nextcloud#9`: ECR Repository via Terraform erstellen.
    * `Nextcloud#10`: IAM OIDC Provider für EKS konfigurieren (via Terraform).
    * `Nextcloud#11`: AWS EBS CSI Driver im EKS Cluster installieren und konfigurieren.
* **Plan für die Umsetzung (grob):**
    * Priorität 1: EKS Cluster Grundgerüst (#8) und IAM OIDC Provider (#10), da diese fundamental sind und voneinander
      abhängen könnten.
    * Priorität 2: ECR Repository (#9), wichtig für spätere CI/CD und kann parallel vorbereitet werden.
    * Priorität 3: EBS CSI Driver (#11), baut auf dem funktionierenden EKS Cluster und OIDC Provider auf, um
      Persistenz-Tests vorzubereiten.
    * Die Terraform-Konfigurationen werden im bestehenden Verzeichnis `src/terraform/` in logisch getrennten Dateien (
      z.B. `eks.tf`, `ecr.tf`, `iam_oidc.tf`) erweitert.
    * Dokumentation (dieses README, insbesondere Abschnitte 4.1.3 und ggf. relevante Teile von 3.3) wird parallel zur
      Umsetzung jeder User Story aktualisiert.
* **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    * Die korrekte JSON-Syntax für die ECR Lifecycle Policy (`Nextcloud#9`) erforderte eine kurze, aber fokussierte Recherche in der AWS/Terraform-Dokumentation, war aber dank der Beispiele schnell umsetzbar. Ansonsten keine Blocker.
    * Die `sts:AssumeRoleWithWebIdentity`-Aktion und die genaue Struktur der Trust Policy für IRSA (`Nextcloud#10`) waren anfangs komplex. Das Verständnis der `Condition`-Klausel, die die Rolle an einen spezifischen Kubernetes Service Account bindet, war der Schlüssel zum Erfolg.
* **Erreichtes Inkrement / Ergebnisse:**
    * **EKS Cluster und Node Groups provisioniert (User Story #8 ✓):**
        * Die EKS Control Plane wurde mit der Kubernetes-Version `1.29` (konfigurierbar via Terraform-Variable
          `var.eks_cluster_version`) erfolgreich erstellt.
        * Die notwendige IAM Rolle (`${var.project_name}-eks-cluster-role`) für den EKS Cluster mit der angehängten
          `AmazonEKSClusterPolicy` wurde erstellt und korrekt konfiguriert.
        * Mindestens eine EKS Managed Node Group (`${var.project_name}-main-nodes`) wurde erstellt.
            * Instanztyp: `t3.medium` (konfigurierbar via `var.eks_node_instance_types`).
            * Skalierungsparameter: Min: 2, Max: 2, Desired: 2 für grundlegende HA.
        * Die Worker Nodes wurden erfolgreich in den privaten Subnetzen (`aws_subnet.private[*].id`) der in Sprint 1
          erstellten VPC platziert.
        * Die notwendige IAM Rolle (`${var.project_name}-eks-node-role`) für die Node Groups (inkl. der Policies:
          `AmazonEKSWorkerNodePolicy`, `AmazonEC2ContainerRegistryReadOnly`, `AmazonEKS_CNI_Policy`) wurde erstellt und
          korrekt konfiguriert.
        * Der Befehl `terraform apply` provisionierte den Cluster und die Node Groups erfolgreich, nachdem initiale
          Probleme mit der `assume_role_policy` der Cluster-Rolle behoben wurden.
        * Die `kubeconfig`-Datei wurde mittels
          `aws eks update-kubeconfig --region $(terraform output -raw aws_region) --name $(terraform output -raw eks_cluster_name) --profile nextcloud-project`
          aktualisiert.
        * Der Befehl `kubectl get nodes -o wide` zeigte die 2 Worker Nodes im 'Ready'-Status mit privaten IP-Adressen
          an.
        * Die Dokumentation der EKS-Architektur wurde in Abschnitt [3.3.4](#334-aws-eks-architektur-detail) (neu)
          und [4.1.3](#413-provisionierung-des-eks-clusters-und-der-ecr) aktualisiert.
        * Alle Akzeptanzkriterien für User Story #8 sind erfüllt.
    * **Privates ECR Repository via Terraform erstellt (User Story #9 ✓):**
        * Ein privates AWS ECR Repository mit dem konfigurierbaren Namen `nextcloud-app` wurde erfolgreich mittels Terraform provisioniert (`aws_ecr_repository`).
        * Die Funktion "Image-Scanning bei Push" wurde aktiviert, um Images automatisch auf bekannte Sicherheitslücken zu prüfen.
        * Eine Lifecycle Policy (`aws_ecr_lifecycle_policy`) wurde konfiguriert, die ungetaggte Images nach 30 Tagen automatisch löscht, um die Kostenkontrolle zu gewährleisten.
        * Der URI des Repositories (`repository_url`) wird nun als Terraform-Output ausgegeben, um ihn in nachfolgenden CI/CD-Schritten einfach referenzieren zu können.
        * `terraform apply` wurde erfolgreich ausgeführt und das Repository in der AWS Konsole verifiziert.
        * Alle DoD-Punkte für diese User Story sind erfüllt.
    * **IAM OIDC Provider für EKS konfiguriert (User Story #10 ✓):**
        * Der IAM OIDC Identity Provider wurde mittels der Ressource `aws_iam_openid_connect_provider` in AWS IAM erstellt und ist korrekt mit dem OIDC-Issuer des EKS-Clusters verknüpft.
        * Der Root-CA-Thumbprint des OIDC-Endpunkts wurde dynamisch und sicher über die `tls_certificate` Datenquelle ermittelt.
        * Eine beispielhafte IAM-Rolle (`${var.project_name}-ebs-csi-driver-role`) für den AWS EBS CSI Driver wurde erstellt.
        * Die Trust Policy dieser Rolle wurde so konfiguriert, dass sie nur vom Kubernetes Service Account `ebs-csi-controller-sa` im Namespace `kube-system` übernommen werden kann. Dies legt den Grundstein für sichere, anmeldeinformationsfreie AWS-API-Aufrufe aus Pods (IRSA).
        * Die AWS-verwaltete `AmazonEBSCSIDriverPolicy` wurde an die Rolle angehängt.
        * `terraform apply` wurde erfolgreich ausgeführt und die Konfiguration in der AWS Konsole verifiziert.
    * **AWS EBS CSI Driver installiert und konfiguriert (User Story #11 ✓):**
        * Der AWS EBS CSI Driver wurde als EKS-verwaltetes Add-on (`aws_eks_addon`) via Terraform installiert. Diese Methode wurde wegen ihrer einfachen Verwaltung und direkten Integration in EKS und Terraform gewählt.
        * Das Add-on wurde so konfiguriert, dass es die in User Story #10 erstellte IAM-Rolle (`${var.project_name}-ebs-csi-driver-role`) via IRSA verwendet. Dies stellt sicher, dass der Treiber die nötigen Berechtigungen hat, um EBS Volumes zu verwalten.
        * Die erfolgreiche Installation wurde durch die Erstellung einer `StorageClass` und eines Test-`PersistentVolumeClaim` (PVC) verifiziert. Der PVC wurde erfolgreich an ein dynamisch provisioniertes `PersistentVolume` (PV) und ein zugrunde liegendes AWS EBS Volume gebunden.
        * Alle DoD-Punkte für diese User Story sind erfüllt.

*(Sprint 2 ist damit hinsichtlich der technischen Umsetzung vollständig abgeschlossen.)*
* **Sprint Review (durchgeführt am 01.06.2025 – simuliert):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team). Die Stakeholder (Fachexperten) werden gedanklich als Publikum einbezogen.
    * **Sprint-Ziel von Sprint 2 (rekapituliert):** "Ein funktionsfähiger AWS EKS Kubernetes-Cluster mit konfigurierten Node Groups und einem IAM OIDC Provider ist mittels Terraform automatisiert provisioniert. Zusätzlich ist ein AWS ECR Repository für Docker-Images via Terraform erstellt und der AWS EBS CSI Driver im EKS Cluster für persistente Volumes installiert und konfiguriert."
    * **Präsentation des Inkrements (Demo-Highlights):**
        * **EKS Cluster:** Es wurde demonstriert, wie `terraform apply` den vollständigen EKS-Cluster mit zwei Worker Nodes in den privaten Subnetzen erstellt. Der erfolgreiche Zugriff via `kubectl get nodes` wurde live gezeigt.
        * **ECR Repository:** Das via Terraform erstellte private ECR-Repository wurde in der AWS-Konsole präsentiert, inklusive der aktivierten Konfiguration für "Scan on push" und der Lifecycle Policy.
        * **IRSA & EBS CSI Driver:** Die Kernfunktionalität wurde durch einen Live-Test nachgewiesen:
            1. Anwenden eines Test-`PersistentVolumeClaim` (PVC) via `kubectl apply`.
            2. Beobachtung des PVC-Status, der in Echtzeit auf `Bound` wechselte (`kubectl get pvc`).
            3. Verifizierung des dynamisch erstellten EBS-Volumes im AWS EC2 Dashboard.
        * Dieses Ergebnis bestätigt, dass der EKS-Cluster, der OIDC Provider, die IRSA-Rolle und der EBS CSI Driver perfekt zusammenspielen.
    * **Feedback & Diskussion (simuliert):** Das Sprint-Ziel wurde vollumfänglich erreicht. Alle User Stories (`#8`, `#9`, `#10`, `#11`) wurden erfolgreich abgeschlossen. Das Inkrement stellt eine robuste und sichere Basis für die Anwendungs-Deployments in den nächsten Sprints dar. Die Wahl des EKS Add-ons für den CSI-Treiber wurde als kluge, stabile Entscheidung gewürdigt.
    * **Product Backlog:** Keine unmittelbaren Änderungen am Backlog erforderlich. Die Priorisierung für Sprint 3 (RDS-Datenbank) bleibt bestehen und ist der logische nächste Schritt.
* **Sprint Retrospektive (durchgeführt am 01.06.2025 – simuliert):**
    * **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    * **Diskussion – Was lief gut in Sprint 2?**
        * **Logische Aufteilung:** Die Aufteilung der komplexen EKS-Einrichtung in separate, fokussierte User Stories (#8, #10, #11) hat sich als äusserst effektiv erwiesen. Jedes Teilproblem konnte isoliert gelöst und verifiziert werden.
        * **Proaktive Problemlösung:** Der `no such host`-Fehler bei `kubectl` wurde schnell als typisches `kubeconfig`-Problem identifiziert und souverän gelöst.
        * **Stabile Basis:** Das Inkrement aus Sprint 1 (VPC) war eine fehlerfreie und solide Grundlage, auf der reibungslos aufgebaut werden konnte.
        * **Dokumentationsdisziplin:** Die proaktive Dokumentation von Architekturentscheidungen (EKS Add-on vs. Helm) im `README.md` wurde als wertvoll für die Nachvollziehbarkeit erachtet.
    * **Diskussion – Was könnte verbessert werden?**
        * **Komplexität von IRSA:** Die Einarbeitung in die genaue Funktionsweise von IRSA und die Syntax der Trust Policy war zeitaufwändiger als erwartet. Ein initialer "Spike" (kurzes Forschungs-Ticket) hätte hier den Aufwand vielleicht genauer vorhersagbar gemacht.
        * **Terraform-Plan-Dauer:** Mit zunehmender Anzahl an Ressourcen dauert `terraform plan` und `apply` länger. Dies ist normal, muss aber in der Zeitplanung für zukünftige Sprints berücksichtigt werden.
    * **Abgeleitete Action Items für Sprint 3:**
        1. **Komplexitätsbewertung beibehalten:** Bei neuen, unbekannten AWS-Diensten (wie RDS in Sprint 3) bewusst eine kurze Recherchephase einplanen, bevor die Implementierung beginnt, um die Aufwandsschätzung zu verbessern.
        2. **Modulare Terraform-Struktur im Auge behalten:** Auch wenn noch keine eigenen Module erstellt werden, wird weiterhin auf saubere, in thematische Dateien aufgeteilte Terraform-Konfigurationen geachtet, um die Übersichtlichkeit zu wahren.

---

#### **Sprint 3: Terraform für RDS/IAM & Manuelles Nextcloud Deployment**

*   **Dauer:** ca. 03. Juni 2025 - 14. Juni 2025
*   **Zugehörige Epics:** `EPIC-TF-DB-IAM`, `EPIC-NC-DEPLOY`
*   **Sprint Planning (durchgeführt am 02.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Kontext & Ziel des Plannings:** Nach Abschluss von Sprint 2, in dem die komplette EKS-Infrastruktur inkl. Persistenz-Fähigkeit bereitgestellt wurde, fokussiert sich dieser Sprint darauf, die letzte grosse Infrastruktur-Abhängigkeit – die Datenbank – zu provisionieren und die Funktionsfähigkeit der Gesamtplattform mit einem manuellen Test-Deployment zu validieren.
    *   **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):**
        *   Der Product Owner betonte, dass vor der Automatisierung des Deployments (Sprint 4 & 5) absolute Sicherheit bestehen muss, dass die Plattform (VPC, EKS, EBS, RDS) eine stateful Anwendung wie Nextcloud überhaupt tragen kann. Ein manuelles Proof-of-Concept-Deployment ist der schnellste Weg, dies zu validieren und unerwartete Integrationsprobleme frühzeitig aufzudecken.
        *   Gemeinsam wurde das folgende, präzisierte Sprint-Ziel formuliert:
            *   *"Eine ausfallsichere AWS RDS PostgreSQL-Instanz ist via Terraform provisioniert und sicher konfiguriert, sodass nur der EKS-Cluster darauf zugreifen kann. Die erfolgreiche Integration der gesamten Infrastruktur wird durch ein manuelles Deployment einer funktionalen, datenbank-angebundenen und persistenten Nextcloud-Instanz nachgewiesen."*
    *   **Diskussion – Das "Was" (Auswahl der Sprint Backlog Items):**
        *   Basierend auf dem Ziel wurden die User Stories `#12` (RDS), `#13` (Security Group), `#14` (Manuelles Deployment) und `#15` (Doku) als Kernbestandteile identifiziert.
        *   Während der Diskussion wurde klar, dass ein kritischer Schritt fehlt: Wie gelangen die Datenbank-Credentials sicher in den Cluster? Das Dev-Team schlug vor, hierfür eine neue User Story zu erstellen (`#16`), die das manuelle Erstellen eines Kubernetes Secrets für die Credentials abdeckt. Dies wurde vom PO akzeptiert und dem Sprint Backlog hinzugefügt.
        *   Es wurde ebenfalls beschlossen, das RDS-Master-Passwort nicht im Terraform-Code zu hardcoden, sondern es im AWS Secrets Manager zu speichern und von Terraform nur zu referenzieren. Diese Anforderung wird Teil von User Story `#12`.
    *   **Diskussion – Das "Wie" (Grobe Planung der Umsetzung):**
        1.  **AWS-Infrastruktur zuerst:** Zuerst wird das RDS-Passwort im AWS Secrets Manager angelegt. Danach werden die Terraform-Konfigurationen für RDS (`#12`) und die Security Group (`#13`) erstellt und angewendet.
        2.  **Kubernetes-Vorbereitung:** Sobald die Datenbank läuft, wird das Kubernetes-Secret mit den Credentials manuell im Cluster erstellt (`#16`).
        3.  **Proof-of-Concept:** Anschliessend wird das manuelle Deployment von Nextcloud mit einfachen YAML-Manifesten (Deployment, Service, PVC) durchgeführt (`#14`).
        4.  **Dokumentation:** Die für das manuelle Deployment notwendigen Schritte und Befehle werden parallel dokumentiert (`#15`).
*   **Sprint-Ziel (committet für Sprint 3):**
    *   "Eine ausfallsichere AWS RDS PostgreSQL-Instanz ist via Terraform provisioniert und sicher konfiguriert, sodass nur der EKS-Cluster darauf zugreifen kann. Die erfolgreiche Integration der gesamten Infrastruktur wird durch ein manuelles Deployment einer funktionalen, datenbank-angebundenen und persistenten Nextcloud-Instanz nachgewiesen."
*   **Sprint Backlog (committete User Stories für Sprint 3):**
    *   `Nextcloud#12`: RDS PostgreSQL Instanz via Terraform provisionieren & Master-Passwort via Secrets Manager verwalten.
    *   `Nextcloud#13`: RDS Security Group konfigurieren, um Zugriff nur vom EKS-Cluster zu erlauben.
    *   `Nextcloud#14`: Nextcloud manuell auf EKS deployen (als Proof-of-Concept).
    *   `Nextcloud#15`: Die Schritte des manuellen Deployments im `README.md` dokumentieren.
    *   `Nextcloud#39`: **(NEU)** Manuell ein Kubernetes Secret für die RDS-Datenbank-Credentials erstellen.
*   **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    *   **Impediment 1 (Blocker):** Nach dem initialen manuellen Deployment blieb der `PersistentVolumeClaim` (PVC) für Nextcloud im Status `Pending`. Die `kubectl describe pod` Ausgabe zeigte keine Events, was auf ein Problem vor dem Scheduling hindeutete.
        *   **Analyse:** Die Logs des `ebs-csi-node` Pods zeigten einen Timeout-Fehler bei der Abfrage des EC2 Instance Metadata Service (IMDS). Dies verhinderte, dass der CSI-Treiber die notwendigen Topologie-Labels auf den Worker-Nodes setzen konnte, wodurch der Scheduler die Nodes für die Volume-Provisionierung ignorierte.
        *   **Lösung 1 (Versuch):** Zuerst wurde eine fehlende IAM-Berechtigung vermutet. Dies stellte sich als falsch heraus.
        *   **Lösung 2 (Erfolgreich):** Die Recherche ergab, dass die Standard-Hop-Limit von `1` für den IMDS in containerisierten Umgebungen nicht ausreicht. Die Lösung war die Erstellung einer dedizierten `aws_launch_template` für die EKS Node Group, in der die `http_put_response_hop_limit` auf `2` gesetzt wurde.
    *   **Impediment 2 (Blocker):** Nachdem das IMDS-Problem gelöst war, schlug die Volume-Provisionierung mit einem `AccessDenied`-Fehler fehl.
        *   **Analyse:** Die PVC-Events zeigten klar, dass der CSI-Treiber nicht berechtigt war, die `sts:AssumeRoleWithWebIdentity`-Aktion auszuführen. Dies deutete auf eine fehlerhafte IAM Trust Policy für die IRSA-Rolle hin.
        *   **Lösung:** Die Trust Policy der `ebs_csi_driver_role` wurde überarbeitet und robuster gestaltet, indem sie explizit das `audience` (`aud`) und `subject` (`sub`) des OIDC-Tokens validiert.
    *   **Erkenntnis:** Die Fehlersuche in einem verteilten System erfordert eine schichtweise Analyse. Ein `Pending`-Status kann von der Anwendung, über Kubernetes-Komponenten, bis hin zu tiefen Cloud-Infrastruktur-Einstellungen (IAM, EC2) verursacht werden. Die `describe`- und `logs`-Befehle sind hierbei die wichtigsten Werkzeuge.
*   **Erreichtes Inkrement / Ergebnisse:**
    *   **Korrektur & Härtung der EKS-Worker-Node-Konfiguration (Ungeplant, aber notwendig):**
        *   Eine dedizierte `aws_launch_template` wurde erstellt und mit der EKS Node Group verknüpft. Diese setzt die IMDS Hop-Limit auf `2`, um Konnektivitätsprobleme von Pods zum EC2 Metadatendienst zu beheben.
        *   Die IAM Trust Policy für die EBS CSI Driver Rolle (`ebs_csi_driver_role`) wurde überarbeitet, um die Sicherheit und Zuverlässigkeit der IRSA-Konfiguration zu erhöhen.
    *   **RDS PostgreSQL-Instanz und zugehörige Ressourcen via Terraform provisioniert (User Story #12 ✓):**
        *   Die RDS-Instanz (`PostgreSQL 16.2`, `db.t3.micro`) wurde erfolgreich in den privaten Subnetzen provisioniert.
        *   Das Master-Passwort wird sicher aus dem AWS Secrets Manager ausgelesen.
    *   **RDS Security Group konfiguriert (User Story #13 ✓):**
        *   Eine dedizierte Security Group für RDS wurde erstellt, die den Zugriff nur von der EKS-Cluster-Security-Group auf Port `5432` erlaubt.
    *   **Manuelles Kubernetes Secret für DB-Credentials erstellt (User Story #39 ✓):**
        *   Ein Kubernetes-Secret (`nextcloud-db-secret`) wurde erfolgreich mit den korrekten, base64-codierten Werten erstellt.
    *   **Nextcloud manuell auf EKS deployt als Proof-of-Concept (User Story #14 ✓):**
        *   Eine funktionale Nextcloud-Instanz wurde mittels manueller Manifeste (Deployment, Service, PVC) auf dem EKS-Cluster bereitgestellt.
        *   Die Instanz ist über einen AWS Load Balancer extern erreichbar. Die Datenbankverbindung und die Datenpersistenz wurden durch einen Pod-Neustart-Test erfolgreich validiert.
    *   **Spezifikation des manuellen Deployments dokumentiert (User Story #15 ✓):**
        *   Eine detaillierte Spezifikation, die alle Konfigurationen, Manifeste und Befehle des manuellen Deployments beschreibt, wurde als dedizierter Abschnitt in diesem `README.md` (Kapitel 4.1.8) erstellt.
*   **Sprint Review (durchgeführt am 14.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team), Stakeholder (repräsentiert durch die Fachexperten).
    *   **Präsentation des Sprint-Ziels & Inkrements:** Das committete Sprint-Ziel – die Bereitstellung einer via Terraform provisionierten RDS-Instanz und der Nachweis der Plattform-Funktionalität durch ein manuelles, persistentes Nextcloud-Deployment – wurde vollständig erreicht.
    *   **Live-Demo (Demo-Highlight):** Als Höhepunkt des Reviews wurde die Lauffähigkeit der Gesamtlösung live demonstriert:
        1.  Zuerst wurde der erfolgreiche `terraform apply`-Lauf gezeigt, der die RDS-Instanz und alle zugehörigen Sicherheitskonfigurationen erstellt hat.
        2.  Anschliessend wurde die Nextcloud-Instanz, die zuvor mit manuellen `kubectl apply`-Befehlen bereitgestellt wurde, über ihre öffentliche Load-Balancer-URL im Browser aufgerufen.
        3.  Es wurde ein erfolgreicher Login durchgeführt und eine Test-Datei hochgeladen.
        4.  **Der entscheidende Persistenz-Test:** Der laufende Nextcloud-Pod wurde live mit `kubectl delete pod` terminiert. Die Stakeholder konnten beobachten, wie Kubernetes den Pod automatisch neu startete. Nach dem erneuten Login war die zuvor hochgeladene Test-Datei noch vorhanden, was die korrekte Funktion der Datenbankanbindung und des persistenten EBS-Speichers eindrucksvoll bestätigte.
    *   **Diskussion & Feedback (simuliert):** Die Stakeholder zeigten sich beeindruckt von der Stabilität und Resilienz der demonstrierten Lösung. Besonders positiv wurde hervorgehoben, dass die gesamte zugrundeliegende Infrastruktur nun als validiert gilt, was ein grosses Risiko für die nachfolgenden Sprints eliminiert. Die aufgetretenen technischen Herausforderungen (siehe "Impediments") und deren systematische Lösung wurden als wertvolle Lernerfahrung und Zeichen technischer Tiefe gewertet. Es wurden keine Änderungen am Product Backlog für notwendig erachtet.
    *   **Fazit:** Der Sprint war ein voller Erfolg. Das Inkrement ist robust, die Kernrisiken sind mitigiert und das Projekt ist perfekt positioniert, um nun in die Automatisierungsphase mit Helm überzugehen.
*   **Sprint Retrospektive (durchgeführt am 14.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Ziel der Retrospektive:** Den komplexen Sprint 3 reflektieren, um den methodischen Ansatz zur Problemlösung zu analysieren und den Prozess für zukünftige Sprints weiter zu schärfen.
    *   **Diskussion – Was lief aussergewöhnlich gut?**
        *   **Systematische Fehlersuche:** Trotz unerwarteter und tiefgreifender technischer Probleme wurde ein ruhiger, schichtweiser Debugging-Prozess verfolgt (Pod-Status -> PVC-Events -> CSI-Logs -> IAM-Policies -> EC2-Metadaten). Dieser methodische Ansatz war der Schlüssel zum Erfolg.
        *   **Resilienz & Lernbereitschaft:** Die Bereitschaft, ursprüngliche Annahmen zu verwerfen und die Infrastruktur von Grund auf neu zu provisionieren, um einen sauberen Zustand zu garantieren, war entscheidend.
        *   **Inkrementeller Wert:** Das Sprint-Ziel, ein Proof-of-Concept zu liefern, hat sich als goldrichtig erwiesen. Es hat kritische Infrastruktur-Fehler aufgedeckt, die in einer vollautomatisierten Pipeline nur sehr schwer zu debuggen gewesen wären.
    *   **Diskussion – Was haben wir gelernt (Verbesserungspotenzial)?**
        *   **Komplexität von Cloud-Integrationen:** Dieser Sprint hat eindrücklich gezeigt, dass die Integration von Managed Services (EKS, EC2, IAM) subtile Abhängigkeiten aufweist (z.B. IMDS Hop Limit), die in der offiziellen "Getting Started"-Dokumentation oft nicht im Vordergrund stehen. Eine tiefere Recherche in Best-Practice-Guides und bekannten GitHub-Issues ist unerlässlich.
        *   **Dokumentationsdisziplin:** Das Festhalten der Impediments und deren Lösungen direkt nach Auftreten ist essenziell. Es bildet die Grundlage für eine starke Projektdokumentation und die Reflexion.
    *   **Abgeleitete Action Item für Sprint 4:**
        1.  **Spezifikation als Basis nutzen:** Die Erkenntnisse des manuellen Deployments, die nun detailliert dokumentiert sind, werden als feste Blaupause für die Entwicklung des Helm Charts verwendet. Es wird nicht "from scratch" begonnen, sondern die validierte Konfiguration systematisch in Helm-Templates überführt. Dies minimiert das Risiko, in Sprint 4 erneut auf dieselben Probleme zu stossen.

---

#### **Sprint 4: Nextcloud Helm Chart Entwicklung**

*   **Dauer:** 15. Juni 2025 - 20. Juni 2025
*   **Zugehöriges Epic:** `EPIC-HELM`
*   **Sprint Planning (durchgeführt am 14.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Kontext & Ziel des Plannings:** Nach dem erfolgreichen Proof-of-Concept in Sprint 3 ist die Funktionsfähigkeit der Infrastruktur validiert. Das Ziel dieses Sprints ist es, den manuellen, fehleranfälligen Deployment-Prozess durch ein standardisiertes, wiederverwendbares und konfigurierbares Helm Chart zu ersetzen. Dies ist der erste Schritt zur Automatisierung des Anwendungs-Deployments.
    *   **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):** Der Product Owner betonte, dass manuelle `kubectl apply`-Befehle nicht skalierbar, versionierbar oder für eine CI/CD-Pipeline geeignet sind. Ein Helm Chart ist der Industriestandard, um Kubernetes-Anwendungen zu paketieren und deren Lebenszyklus zu verwalten. Es kapselt die Komplexität und ermöglicht einfache, wiederholbare Installationen und Upgrades.
    *   **Gemeinsam formuliertes Sprint-Ziel:**
        *   *"Ein eigenständiges und funktionales Helm Chart für Nextcloud ist entwickelt, das die manuelle Bereitstellung vollständig ersetzt. Das Chart ist über eine `values.yaml`-Datei konfigurierbar, löst das `localhost`-Redirect-Problem durch eine dedizierte ConfigMap und enthält einen einfachen Helm-Test zur Überprüfung der Erreichbarkeit des Deployments. Die Benutzerfreundlichkeit wird durch eine informative `NOTES.txt`-Datei nach der Installation sichergestellt."*
*   **Sprint Backlog (Committete User Stories für Sprint 4):**
    *   `Nextcloud#16`: **Helm Chart Grundgerüst erstellen:** Ein neues Helm Chart mit `helm create nextcloud-chart` initialisieren und die grundlegende `Chart.yaml` mit Metadaten füllen. Die unnötigen Standard-Templates werden bereinigt, um eine saubere Basis zu schaffen.
    *   `Nextcloud#40`: **Manuelle Manifeste in Helm-Templates überführen:** Das funktionierende Deployment-, Service- und PVC-YAML aus Sprint 3 in den `templates/`-Ordner des Charts überführen und mit Helm-Variablen (`{{ .Values.xyz }}`) parametrisieren (z.B. für Image-Tag, Replica-Anzahl, Storage-Grösse).
    *   `Nextcloud#17`: **Secrets & ConfigMap für dynamische Konfiguration templatzieren:** Ein Template für das Datenbank-Secret erstellen, das seine Werte aus der `values.yaml` bezieht. Zusätzlich ein ConfigMap-Template erstellen, das `trusted_domains` und `overwrite.cli.url` für Nextcloud konfiguriert, um das `localhost`-Redirect-Problem zu lösen.
    *   `Nextcloud#19`: **Einfachen Helm-Test für Deployment-Verfügbarkeit implementieren:** Ein `templates/tests/test-connection.yaml` erstellen. Dieses Template definiert einen Test-Pod, der mittels `wget` oder `curl` versucht, den Nextcloud-Service zu erreichen. Ein erfolgreicher Test verifiziert die grundlegende Netzwerk-Konnektivität und das Service-Routing.
    *   `Nextcloud#18`: **NOTES.txt für Post-Installationshinweise erstellen:** Eine nützliche `NOTES.txt`-Datei schreiben, die dem Benutzer nach einem `helm install` dynamisch generierte Informationen anzeigt, wie z.B. den Befehl zum Abrufen der externen IP des Load Balancers.
*   **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    *   **Erkenntnis:** Bei der Überprüfung der ersten gerenderten Manifeste (`helm template`) wurde eine unschöne, redundante Benennung der Kubernetes-Ressourcen (z.B. `nextcloud-nextcloud-chart`) festgestellt.
    *   **Lösung (Impediment behoben):** Das Problem wurde schnell auf eine suboptimale Logik im `_helpers.tpl`-Template zurückgeführt. Das Template wurde durch die Standard-Helm-Logik ersetzt und der Chart-Name in `Chart.yaml` vereinfacht. Dies führte zu sauberen und vorhersehbaren Ressourcennamen. Dieser schnelle Fix verhinderte technische Schulden und verbesserte die Chart-Qualität sofort.
*   **Erreichtes Inkrement / Ergebnisse:**
    *   **Helm Chart Grundgerüst erstellt (User Story #16 ✓):**
        *   Ein neues Helm Chart wurde im Verzeichnis `charts/nextcloud-chart` angelegt.
        *   `Chart.yaml` wurde mit Metadaten (Name, Version, Beschreibung) befüllt.
        *   Eine `values.yaml`-Datei wurde erstellt, die zentrale Parameter wie Image-Version, Replica-Anzahl, Service-Typ, Port und Persistenz-Einstellungen (Grösse, StorageClass) konfigurierbar macht.
        *   Die validierten manuellen Manifeste aus Sprint 3 für Deployment, Service und PersistentVolumeClaim wurden erfolgreich in Helm-Templates (`templates/`) überführt.
        *   Eine `_helpers.tpl`-Datei wurde für standardisierte Labels und Benennungen implementiert.
        *   Die Befehle `helm lint` und `helm template` laufen erfolgreich durch und bestätigen die syntaktische und strukturelle Korrektheit des Charts.
        *   Die Dokumentation der Chart-Struktur und Konfiguration wurde in den Abschnitten [4.2.1](#421-helm-chart-struktur), [4.2.2](#422-wichtige-templates) und [4.2.3](#423-konfigurationsmöglichkeiten-über-valuesyaml) im Haupt-README ergänzt.
        *   Alle projektspezifischen DoD-Punkte für diese User Story sind erfüllt.
    *   **Secrets & ConfigMap templatisiert (User Story #17 ✓):**
        *   Ein `templates/secret.yaml` wurde erstellt, welches optional ein Kubernetes Secret mit allen Admin- und Datenbank-Credentials basierend auf der `values.yaml` generiert. Die Logik unterstützt auch die Verwendung eines bereits existierenden Secrets, was die Flexibilität erhöht.
        *   Ein `templates/configmap.yaml` wurde hinzugefügt, um das kritische `localhost`-Redirect-Problem zu lösen. Es generiert eine `autoconfig.php`, die dynamisch `trusted_domains` und `overwrite.cli.url` basierend auf einem Hostnamen in `values.yaml` setzt.
        *   Das `deployment.yaml`-Template wurde angepasst, um die neue ConfigMap als Volume zu mounten und die Logik zur Auswahl des korrekten Secrets (entweder das neu generierte oder ein existierendes) zu implementieren.
        *   Ein Sicherheitshinweis bezüglich der Passwortverwaltung wurde direkt in der `values.yaml` und im Haupt-README ergänzt.
    *   **NOTES.txt für Post-Installationshinweise erstellt (User Story #18 ✓):**
        *   Eine `templates/NOTES.txt`-Datei wurde im Helm Chart erstellt, um Benutzern nach der Installation sofortiges Feedback zu geben.
        *   Die Notizen enthalten dynamische Logik, die je nach konfiguriertem `service.type` (LoadBalancer, NodePort, ClusterIP) unterschiedliche, kontext-spezifische Anweisungen zur Erreichbarkeit der Anwendung generiert.
        *   Für den Standardfall (`LoadBalancer`) wird der Benutzer explizit angewiesen, wie er den externen Hostnamen abruft und den entscheidenden `helm upgrade`-Befehl ausführt, um die Konfiguration abzuschliessen.
        *   Informationen zum Abrufen der Admin-Credentials werden ebenfalls dynamisch angezeigt, je nachdem, ob das Chart ein Secret erstellt hat oder ein existierendes verwendet wird.
        *   Die Funktionalität wurde mit `helm install --dry-run` verifiziert.
    *   **Einfachen Helm-Test für Deployment-Verfügbarkeit implementiert (User Story #19 ✓):**
        *   Ein Test-Manifest wurde unter `templates/tests/test-connection.yaml` erstellt.
        *   Der Test definiert einen Pod mit der Annotation `helm.sh/hook: test`, der nach der Installation mit dem Befehl `helm test <release-name>` ausgeführt werden kann.
        *   Der Pod verwendet `wget`, um den internen `/status.php`-Endpunkt des Nextcloud-Services abzufragen und so die grundlegende Erreichbarkeit und Funktionsfähigkeit der Anwendung zu verifizieren.
        *   Der Test wurde erfolgreich auf einer laufenden Installation im EKS-Cluster ausgeführt und hat den Status `Succeeded` zurückgegeben.
    *   **Manuelle Manifeste in Helm-Templates überführt (User Story #40 ✓):**
        *   Die funktionalen Kubernetes-Manifeste (Deployment, Service, PVC) aus dem manuellen Proof-of-Concept (Sprint 3) wurden erfolgreich in die Helm-Chart-Struktur im `templates/`-Verzeichnis migriert.
        *   Grundlegende Werte wie die Replica-Anzahl (`.Values.replicaCount`), das Container-Image (`.Values.image.repository` & `.Values.image.tag`) und die Speichergrösse des PVCs (`.Values.persistence.size`) wurden dabei direkt parametrisiert.
        *   Diese Arbeit wurde implizit als Teil der Erstellung des Chart-Grundgerüsts (Ticket #16) erledigt und bildet die Basis für alle weiteren Chart-Erweiterungen.
*   **Sprint Review (durchgeführt am 20.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team), Stakeholder (repräsentiert durch die Fachexperten).
    *   **Präsentation des Sprint-Ziels & Inkrements:** Das committete Sprint-Ziel – *"Ein eigenständiges und funktionales Helm Chart für Nextcloud ist entwickelt..."* – wurde vollständig erreicht. Alle committeten User Stories (`#16`, `#40`, `#17`, `#19`, `#18`) wurden abgeschlossen und erfüllen die Definition of Done.
    *   **Live-Demo (Demo-Highlight):** Der Höhepunkt des Reviews war die Live-Demonstration des gesamten Lebenszyklus des neuen Helm Charts:
        1.  **Installation & Post-Install Notes:** Mit dem Befehl `helm install nextcloud ./charts/nextcloud-chart/ --dry-run` wurde gezeigt, wie das Chart valide Manifeste generiert und die benutzerfreundliche `NOTES.txt` die nächsten Schritte klar kommuniziert.
        2.  **Lösung des Redirect-Problems:** Es wurde demonstriert, wie die `ConfigMap` aus `templates/configmap.yaml` dynamisch die `trusted_domains` setzt. Der Wert wurde mit dem Befehl `helm template . --set nextcloud.host=my-test.com` live gerendert und gezeigt.
        3.  **Automatisierter Test:** Auf einer bereits laufenden Instanz wurde `helm test nextcloud` ausgeführt. Die Stakeholder konnten live sehen, wie der Test-Pod startete, den `/status.php`-Endpunkt erfolgreich abfragte und die Test-Suite den Status `Succeeded` meldete. Dies bestätigte die grundlegende Funktionsfähigkeit des Deployments.
    *   **Diskussion & Feedback (simuliert):** Die Stakeholder zeigten sich sehr zufrieden mit dem Ergebnis. Das Inkrement (das Helm Chart) ist eine massive Verbesserung gegenüber den manuellen Manifesten. Es ist robust, wiederverwendbar und benutzerfreundlich. Besonders die proaktive Lösung des `localhost`-Redirect-Problems und die Implementierung des automatisierten Tests wurden als Zeichen für hohe Qualität und Voraussicht gelobt. Es sind keine Änderungen am Product Backlog erforderlich; der Weg für die CI/CD-Pipeline in Sprint 5 ist frei.
*   **Sprint Retrospektive (durchgeführt am 20.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Ziel der Retrospektive:** Den Entwicklungsprozess des Helm Charts reflektieren, um Best Practices zu identifizieren und den Workflow für zukünftige Automatisierungsaufgaben zu optimieren.
    *   **Diskussion – Was lief aussergewöhnlich gut?**
        *   **Validierungskreislauf:** Der ständige Wechsel zwischen Code-Anpassung, `helm lint` und `helm template` hat sich als extrem effizient erwiesen. Fehler (wie das Naming-Problem) wurden sofort entdeckt und behoben, lange bevor ein `helm install` fehlschlagen konnte.
        *   **Blaupause aus Sprint 3:** Die Nutzung der validierten manuellen Manifeste als Vorlage hat die Entwicklung enorm beschleunigt und das Risiko von Konfigurationsfehlern minimiert. Die Entscheidung, zuerst ein manuelles PoC zu machen, hat sich voll ausgezahlt.
        *   **User-Experience-Fokus:** Die Arbeit an `NOTES.txt` und die klaren Kommentare in `values.yaml` waren keine Nebensächlichkeit, sondern haben die Qualität und Wiederverwendbarkeit des Charts entscheidend verbessert.
    *   **Diskussion – Was haben wir gelernt (Verbesserungspotenzial)?**
        *   **Chicken-and-Egg-Problem:** Die Notwendigkeit, das Chart zuerst zu installieren, um den Load-Balancer-Hostnamen zu erhalten, und dann ein `helm upgrade` durchzuführen, ist ein bekannter Knackpunkt. Dies ist zwar der Standardweg, aber für die CI/CD-Pipeline in Sprint 5 müssen wir uns überlegen, wie wir diesen zweistufigen Prozess am besten automatisieren.
        *   **Implizite Abhängigkeiten:** Ticket `#40` wurde fast vollständig durch `#16` erledigt. Das ist zwar effizient, aber im Sprint Planning hätte man diese Überlappung vielleicht erkennen und die Tickets zu einem zusammenfassen können, um das Backlog klarer zu halten.
    *   **Abgeleitete Action Items für Sprint 5:**
        1.  **Automatisierung des "Upgrade"-Schritts:** Für die CI/CD-Pipeline in Sprint 5 wird ein dedizierter Schritt oder ein Skript eingeplant, das nach dem `helm install` den Load-Balancer-Hostnamen abfragt und automatisch den `helm upgrade`-Befehl ausführt, um den Prozess vollständig zu automatisieren.
        2.  **Wiederverwendung von Konfigurationslogik:** Die Logik zur Parameterübergabe in Helm (`--set`, `-f`) wird als Blaupause für die Konfiguration der Terraform-Schritte in der CI/CD-Pipeline verwendet, um auch dort eine klare Trennung von Code und Konfiguration beizubehalten.


---

#### **Sprint 5: CI/CD Pipeline (GitHub Actions) & Tests**

*   **Dauer:** ca. 21. Juni 2025 - 03. Juli 2025
*   **Zugehörige Epics:** `EPIC-CICD`, Teile von `EPIC-ABSCHLUSS` (Testing)
*   **Sprint Planning (durchgeführt am 20.06.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Kontext & Ziel des Plannings:** Sprint 4 hat ein robustes und testbares Helm Chart geliefert. Der manuelle Deployment-Prozess ist damit zwar standardisiert, aber noch nicht automatisiert. Das Ziel dieses Sprints ist es, den gesamten Prozess von einer Code-Änderung bis zum verifizierten Deployment im EKS-Cluster zu automatisieren.
    *   **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):**
        *   Der Product Owner betonte, dass der wahre Wert von DevOps in der Geschwindigkeit und Zuverlässigkeit liegt, mit der Änderungen ausgeliefert werden können. Eine manuelle `helm install`-Ausführung ist fehleranfällig und nicht skalierbar. Eine CI/CD-Pipeline ist das letzte Puzzlestück, um eine End-to-End-Automatisierung zu erreichen.
        *   Das Development Team hob hervor, dass eine sichere Authentifizierung (ohne langlebige AWS Keys in GitHub) und eine automatisierte Validierung (mit `helm test`) nicht-funktionale Kernanforderungen sind.
        *   Unter Berücksichtigung der Erkenntnis aus der Sprint-4-Retrospektive (Automatisierung des zweistufigen Upgrade-Prozesses) wurde das folgende, präzisierte Sprint-Ziel formuliert:
            *   *"Eine sichere und voll-automatisierte CI/CD-Pipeline ist etabliert. Sie wird bei einem Push auf den `main`-Branch getriggert, authentifiziert sich sicher via OIDC bei AWS, installiert oder aktualisiert das Nextcloud Helm Chart im EKS-Cluster, löst das "Load Balancer Hostname"-Problem automatisiert und verifiziert das erfolgreiche Deployment durch die Ausführung der Helm-Tests."*
    *   **Diskussion – Das "Was" (Auswahl der Sprint Backlog Items):**
        *   Basierend auf dem Ziel wurden die folgenden User Stories als essentiell identifiziert:
            *   `Nextcloud#20` (OIDC Authentifizierung): Dies ist die Grundvoraussetzung für jede sichere Interaktion zwischen GitHub und AWS und muss als Erstes umgesetzt werden.
            *   `Nextcloud#21` (GitHub Actions Workflow): Dies ist die Kern-User-Story. Es wurde beschlossen, dass diese Story auch die automatische Ausführung der Helm-Tests (`Nextcloud#24`) am Ende des Deployments beinhalten soll, da dies ein integraler Bestandteil eines "guten" Deployments ist.
            *   `Nextcloud#23` (Pipeline Status Badge): Eine kleine, aber sehr nützliche User Story, um die Transparenz zu erhöhen. Passt gut in den Sprint.
        *   **Bewusste Entscheidung (De-Scoping):** Die User Story `Nextcloud#22` (Terraform Plan/Apply in CI/CD) wurde bewusst aus dem Sprint-Ziel und -Backlog herausgenommen. Die Automatisierung von Infrastrukturänderungen ist ein grosses, eigenes Thema. Für diesen Sprint liegt der Fokus klar auf dem **Application Deployment**. `#22` bleibt als wichtige Idee im Product Backlog.
    *   **Diskussion – Das "Wie" (Grobe Planung der Umsetzung):**
        1.  **Terraform-Vorbereitung für OIDC:** Zuerst muss die AWS-Infrastruktur angepasst werden. Eine neue IAM-Rolle für GitHub Actions wird via Terraform erstellt, die dem GitHub-Repository erlaubt, sich via OIDC zu authentifizieren. Dies ist der Hauptteil von `#20`.
        2.  **GitHub-Konfiguration:** Die für das Deployment notwendigen Secrets (z.B. das RDS-Passwort) werden als "Repository Secrets" in GitHub hinterlegt, damit die Pipeline darauf zugreifen kann.
        3.  **Workflow-Implementierung (`.github/workflows/deploy.yml`):** Der Workflow wird schrittweise aufgebaut:
            *   Trigger auf `push` zum `main`-Branch.
            *   Einrichten der OIDC-Berechtigungen im Job (`permissions: id-token: write`).
            *   Nutzung der offiziellen `aws-actions` zum Konfigurieren der AWS-Credentials und der `kubeconfig`.
            *   **Lösung für das "Upgrade"-Problem:** Der Workflow wird ein Skript enthalten, das nach einem initialen `helm upgrade --install` in einer Schleife den Load-Balancer-Hostnamen abfragt (`kubectl get svc...`). Sobald der Hostname verfügbar ist, wird ein zweites, finales `helm upgrade` mit dem korrekten `nextcloud.host`-Wert ausgeführt.
            *   Als letzter Schritt im Job wird `helm test` ausgeführt.
        4.  **Badge-Integration:** Sobald die Pipeline einmal gelaufen ist, wird der Markdown-Code für das Status-Badge aus GitHub kopiert und in die `README.md`-Datei eingefügt (`#23`).
*   **Sprint-Ziel (committet für Sprint 5):**
    *   "Eine sichere und voll-automatisierte CI/CD-Pipeline ist etabliert. Sie wird bei einem Push auf den `main`-Branch getriggert, authentifiziert sich sicher via OIDC bei AWS, installiert oder aktualisiert das Nextcloud Helm Chart im EKS-Cluster, löst das "Load Balancer Hostname"-Problem automatisiert und verifiziert das erfolgreiche Deployment durch die Ausführung der Helm-Tests."
*   **Sprint Backlog (Committete User Stories für Sprint 5):**
    *   `Nextcloud#20`: OIDC Authentifizierung für GitHub Actions zu AWS einrichten (via Terraform).
    *   `Nextcloud#21`: GitHub Actions Workflow für Helm Chart Deployment erstellen (inkl. `helm test` Ausführung).
    *   `Nextcloud#23`: Pipeline Status Badge im README anzeigen.
*   **Wichtigste Daily Scrum Erkenntnis / Impediment:** *(Wird im Sprint ergänzt)*
*   **Erreichtes Inkrement / Ergebnisse:** *(Wird im Sprint ergänzt)*
    *   **OIDC Authentifizierung für GitHub Actions zu AWS eingerichtet (User Story #20 ✓):**
        *   Mittels Terraform wurde ein IAM OIDC Identity Provider in AWS erstellt, der eine Vertrauensstellung zu `token.actions.githubusercontent.com` herstellt.
        *   Eine neue, dedizierte IAM-Rolle (`Nextcloud-cicd-role`) wurde via Terraform provisioniert.
        *   Die Trust Policy dieser Rolle wurde präzise konfiguriert, um nur Workflows aus dem spezifischen Projekt-Repository (`Stevic-Nenad/Nextcloud`) und dem `main`-Branch zu autorisieren, die Rolle zu übernehmen (`sts:AssumeRoleWithWebIdentity`).
        *   Eine granulare IAM-Policy mit den minimal notwendigen Berechtigungen (`eks:DescribeCluster`, `eks:AccessKubernetesApi`) wurde erstellt und an die Rolle angehängt, um das Least-Privilege-Prinzip zu wahren.
        *   Die erfolgreiche Konfiguration wurde durch einen Test-Workflow verifiziert, der die Rolle erfolgreich übernehmen und `aws sts get-caller-identity` ausführen konnte.
        *   Die Dokumentation des Setups wurde in Abschnitt [4.3.2](#432-authentifizierung-gegenüber-aws-oidc) ergänzt.
    *   **GitHub Actions Workflow für Helm Chart Deployment erstellt (User Story #21 ✓):**
        *   Ein Workflow in `.github/workflows/deploy.yml` wurde erstellt, der bei einem Push auf den `main`-Branch automatisch startet.
        *   Der Workflow nutzt die in `#20` konfigurierte OIDC-Authentifizierung, um sich sicher bei AWS anzumelden und die `kubeconfig` für den EKS-Cluster zu konfigurieren.
        *   Als Qualitätssicherungsschritt wird das Helm Chart mit `helm lint` überprüft; ein Fehler hierbei bricht die Pipeline ab.
        *   Das Deployment erfolgt über `helm upgrade --install`, was sowohl Neuinstallationen als auch Updates abdeckt. Sensible Werte (DB-Passwort etc.) werden sicher über GitHub Secrets an den Befehl übergeben.
        *   Das "Chicken-and-Egg"-Problem des Load-Balancer-Hostnamens wurde durch ein Skript im Workflow gelöst, das nach dem initialen Deployment auf den Hostnamen wartet und dann ein zweites `helm upgrade` mit dem korrekten Wert ausführt.
        *   Nach dem erfolgreichen Deployment werden automatisch die `helm test`-Fälle ausgeführt, um die Funktionsfähigkeit der Anwendung zu verifizieren. Ein Fehlschlagen der Tests führt zum Scheitern der Pipeline.
        *   Alle Schritte werden transparent im Workflow-Log protokolliert.
    *   **Pipeline Status Badge im README angezeigt (User Story #23 ✓):**
        *   Das offizielle GitHub Actions Status-Badge für den `deploy.yml`-Workflow wurde generiert.
        *   Der Markdown-Code für das Badge wurde an einer prominenten Stelle am Anfang der `README.md`-Datei eingefügt.
        *   Das Badge zeigt erfolgreich den Live-Status (z.B. "passing") der letzten Pipeline-Ausführung auf dem `main`-Branch an und ist direkt mit der Actions-Seite des Repositories verlinkt.
*   **Sprint Review (durchgeführt am 03.07.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team), Stakeholder (repräsentiert durch die Fachexperten).
    *   **Präsentation des Sprint-Ziels & Inkrements:** Das committete Sprint-Ziel – *"Eine sichere und voll-automatisierte CI/CD-Pipeline ist etabliert..."* – wurde vollständig erreicht. Alle committeten User Stories (`#20`, `#21`, `#23`) wurden erfolgreich abgeschlossen. Das Produktinkrement ist nun eine voll funktionsfähige, automatisierte End-to-End-Deployment-Lösung.
    *   **Live-Demo (Demo-Highlight):** Der gesamte automatisierte Prozess wurde live demonstriert, was den Höhepunkt des Projekts darstellt:
        1.  Eine kleine, harmlose Änderung (z.B. ein Kommentar) wurde in einer Codedatei vorgenommen und direkt in den `main`-Branch gepusht.
        2.  Die Stakeholder konnten live im "Actions"-Tab von GitHub beobachten, wie der `deploy.yml`-Workflow automatisch startete.
        3.  Es wurde durch die Logs der einzelnen Schritte navigiert, um die Schlüsselfunktionen zu zeigen:
            *   Die erfolgreiche, passwortlose Authentifizierung via OIDC.
            *   Das dynamische Auslesen des Cluster-Namens aus dem Terraform-State.
            *   Der erfolgreiche `helm upgrade --install`-Befehl.
            *   Das Warten auf den Load-Balancer-Hostnamen und der finale, automatisierte Konfigurations-Upgrade.
            *   Die erfolgreiche Ausführung der `helm test`-Suite am Ende.
        4.  Abschliessend wurde die Hauptseite des GitHub-Repositorys gezeigt, auf der das grüne "passing"-Status-Badge den Erfolg der soeben durchgeführten Pipeline anzeigte.
    *   **Diskussion & Feedback (simuliert):** Die Stakeholder waren beeindruckt von der nahtlosen und robusten Automatisierung. Die elegante Lösung für das "dynamische Cluster-Namen"-Problem wurde als besonders professionell und praxisnah hervorgehoben. Das Projekt hat nun den Zustand erreicht, den es von Anfang an anstrebte: Eine Änderung im Code führt zu einem automatisierten, verifizierten Deployment in der Cloud. Das Product Backlog muss für das Erreichen des Projektziels nicht mehr angepasst werden; die verbleibende Arbeit in Sprint 6 konzentriert sich auf die Finalisierung und Dokumentation.
*   **Sprint Retrospektive (durchgeführt am 03.07.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Ziel der Retrospektive:** Den komplexen Integrationssprint reflektieren, um die Strategien zur Problemlösung zu analysieren und den Gesamtprozess für den Projektabschluss zu verfeinern.
    *   **Diskussion – Was lief aussergewöhnlich gut?**
        *   **Iterative Problemlösung:** Die Erkenntnis, dass der Cluster-Name dynamisch sein muss, war kein Blocker, sondern führte zu einer besseren, robusteren Lösung (Auslesen des Remote States). Die Fähigkeit, den Plan anzupassen, ohne das Ziel aus den Augen zu verlieren, war ein grosser Erfolg.
        *   **Auf vorheriger Arbeit aufbauen:** Die solide Basis aus den Sprints 1-4 (Terraform-Infra, Helm Chart) war entscheidend. Die Pipeline konnte sich auf das "Zusammenfügen" konzentrieren, anstatt grundlegende Probleme in der Infrastruktur oder Anwendungspaketierung beheben zu müssen.
        *   **Sicherheit von Anfang an:** Die Entscheidung, von Beginn an auf OIDC zu setzen, anstatt mit langlebigen Schlüsseln zu beginnen, hat sich ausgezahlt und zu einer sauberen, sicheren Architektur geführt.
    *   **Diskussion – Was haben wir gelernt (Verbesserungspotenzial)?**
        *   **Annahmen früher hinterfragen:** Die Annahme eines statischen Cluster-Namens war eine Betriebseinschränkung (Kosten), die früher hätte in die technische Planung einfliessen können. Dies unterstreicht die Wichtigkeit, nicht-funktionale Anforderungen (wie z.B. Betriebskosten/Ephemeralität) frühzeitig zu berücksichtigen.
        *   **Workflow-Komplexität:** Der `deploy.yml`-Workflow ist mit dem Shell-Skript zur Abfrage des Load Balancers relativ komplex geworden. Zukünftig könnte man überlegen, ob es dafür spezialisierte GitHub Actions gibt, um die Lesbarkeit weiter zu erhöhen. Für dieses Projekt ist die Lösung jedoch pragmatisch und effektiv.
    *   **Abgeleitete Action Items für Sprint 6:**
        1.  **Fokus auf Dokumentation und Aufräumen:** Der letzte Sprint wird sich voll und ganz auf die Finalisierung der Dokumentation konzentrieren. Alle Abschnitte des `README.md` werden noch einmal überprüft, um sicherzustellen, dass sie den finalen Stand der Implementierung exakt widerspiegeln.
        2.  **Erstellung einer sauberen "Anleitung für den Experten":** Die Installations- und Inbetriebnahme-Anleitung (Kapitel 4.4) wird besonders sorgfältig ausformuliert, damit ein externer Gutachter das Projekt von Grund auf nachvollziehen und potenziell selbst ausführen kann. Dies beinhaltet klare Anweisungen zur Konfiguration der GitHub-Secrets und zum Ausführen von Terraform.


---

#### **Sprint 6: Finalisierung & Vollständiges Lifecycle-Management**

*   **Dauer:** ca. 04. Juli 2025 - 09. Juli 2025
*   **Zugehöriges Epic:** `EPIC-ABSCHLUSS`, `EPIC-CICD`
*   **Sprint Planning (durchgeführt am 03.07.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Kontext & Ziel des Plannings:** Das Projekt hat mit einer funktionierenden CI/CD-Pipeline für das Anwendungs-Deployment einen riesigen Meilenstein erreicht. Für den finalen Sprint wird der Scope erweitert, um den gesamten Infrastruktur-Lebenszyklus zu automatisieren. Dies soll die Mächtigkeit der Lösung demonstrieren und eine vollständige End-to-End-Automatisierung von der Erstellung bis zur Zerstörung der Umgebung zeigen.
    *   **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):**
        *   Der Product Owner betonte, dass für eine eindrucksvolle Abschlusspräsentation und den Nachweis der vollständigen Beherrschung der Technologien ein "One-Click"-Ansatz für die gesamte Umgebung ideal wäre. Anstatt nur die App zu deployen, soll die Pipeline auch die gesamte AWS-Infrastruktur auf Knopfdruck erstellen und wieder sicher entfernen können.
        *   Dies demonstriert nicht nur die CI/CD-Fähigkeiten, sondern auch die Mächtigkeit von Infrastructure as Code mit Terraform.
        *   Gemeinsam wurde das folgende, sehr ambitionierte Sprint-Ziel für den Abschluss formuliert:
            *   *"Das Projekt wird mit der Implementierung von zwei neuen, manuell triggerbaren GitHub Actions Workflows abgeschlossen: Ein 'Full Setup'-Workflow, der die gesamte AWS-Infrastruktur mit Terraform provisioniert und anschliessend die Nextcloud-Anwendung deployt, sowie ein 'Full Teardown'-Workflow, der die Anwendung sauber deinstalliert und danach die gesamte Infrastruktur wieder zerstört. Die finale Projektdokumentation, inklusive der neuen Workflows, der Systemarchitektur und einer umfassenden Reflexion, wird fertiggestellt."*
    *   **Diskussion – Das "Was" (Auswahl der Sprint Backlog Items):**
        *   Um dieses Ziel zu erreichen, wurden zwei **neue, hoch-priorisierte User Stories** erstellt:
            *   `Nextcloud#41`: **(NEU)** Einen "Full Setup" GitHub Actions Workflow erstellen, der `terraform apply` und das Helm-Deployment kombiniert.
            *   `Nextcloud#42`: **(NEU)** Einen "Full Teardown" GitHub Actions Workflow erstellen, der `helm uninstall` und `terraform destroy` kombiniert.
        *   Die bestehenden Finalisierungs-Tickets sind weiterhin wichtig, werden aber um die neuen Features herum geplant:
            *   `Nextcloud#28` (Installations-Anleitung): Wird überarbeitet, um die Nutzung der neuen Lifecycle-Workflows zu beschreiben.
            *   `Nextcloud#26` (Architektur-Diagramm): Muss aktualisiert werden, um die neuen Workflows darzustellen.
            *   `Nextcloud#31` (Code finalisieren): Beinhaltet jetzt auch das Kommentieren der neuen Workflow-Dateien.
            *   `Nextcloud#30` & `#32` (Präsentation & Reflexion): Bleiben als letzte, entscheidende Schritte für die Abgabe.
    *   **Diskussion – Das "Wie" (Grobe Planung der Umsetzung):**
        1.  **Ein neuer Workflow für alles:** Anstatt mehrerer neuer Dateien wird ein einziger, neuer Workflow `.github/workflows/lifecycle.yml` erstellt. Dieser wird **ausschliesslich manuell triggerbar** sein (`workflow_dispatch`).
        2.  **Workflow Inputs:** Der `lifecycle.yml`-Workflow wird einen `input`-Parameter vom Typ `choice` haben, der es dem Benutzer in der GitHub-UI erlaubt, zwischen den Aktionen `setup` und `destroy` zu wählen.
        3.  **Job-Struktur für `setup`:**
            *   Ein Job `terraform_apply` führt `terraform init` und `terraform apply -auto-approve` aus.
            *   Ein Job `deploy_application` wird danach ausgeführt (`needs: terraform_apply`). Dieser Job wird den bestehenden `deploy.yml`-Workflow als **wiederverwendbaren Workflow (`workflow_call`)** aufrufen. Das vermeidet Codeduplizierung und hält die Logik sauber getrennt.
        4.  **Job-Struktur für `destroy`:**
            *   Ein Job `helm_uninstall` deinstalliert die Anwendung.
            *   Ein Job `terraform_destroy` wird danach ausgeführt und zerstört die gesamte Infrastruktur mit `terraform destroy -auto-approve`.
        5.  **Dokumentation:** Die `README.md` wird um eine Anleitung zur Nutzung des neuen Lifecycle-Workflows erweitert. Das Architekturdiagramm wird ebenfalls aktualisiert.
*   **Sprint-Ziel (committet für Sprint 6):**
    *   "Das Projekt wird mit der Implementierung von zwei neuen, manuell triggerbaren GitHub Actions Workflows abgeschlossen: Ein 'Full Setup'-Workflow, der die gesamte AWS-Infrastruktur mit Terraform provisioniert und anschliessend die Nextcloud-Anwendung deployt, sowie ein 'Full Teardown'-Workflow, der die Anwendung sauber deinstalliert und danach die gesamte Infrastruktur wieder zerstört. Die finale Projektdokumentation, inklusive der neuen Workflows, der Systemarchitektur und einer umfassenden Reflexion, wird fertiggestellt."
*   **Sprint Backlog (Committete User Stories für Sprint 6):**
    *   `Nextcloud#41`: **(NEU)** "Full Setup" GitHub Actions Workflow erstellen (`terraform apply` & deploy).
    *   `Nextcloud#42`: **(NEU)** "Full Teardown" GitHub Actions Workflow erstellen (`helm uninstall` & `terraform destroy`).
    *   `Nextcloud#28`: Installations- und Inbetriebnahme-Anleitung finalisieren (inkl. der neuen Workflows).
    *   `Nextcloud#26`: Systemarchitektur-Diagramm finalisieren.
    *   `Nextcloud#31`: Codebase finalisieren und kommentieren.
    *   `Nextcloud#32`: Reflexionskapitel im README vervollständigen.
    *   *(Ticket `#30` wurde aus diesem Sprint entfernt und in den Backlog verschoben)*
*   **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    *   **Erkenntnis:** Bei der Implementierung des `terraform destroy`-Jobs in der Pipeline wurde klar, dass die `iam_cicd.tf`-Policy erweitert werden muss, um Terraform das Recht zu geben, alle Ressourcen (EC2, RDS, EKS etc.) auch wieder zu löschen. Die ursprüngliche, sehr restriktive Policy reichte nicht aus.
    *   **Lösung (Impediment behoben):** Die `data "aws_iam_policy_document" "github_actions_permissions"` wurde angepasst, um umfassendere, aber immer noch auf die Projektdienste bezogene Berechtigungen (`ec2:*`, `rds:*`, etc.) zu gewähren. Dies ermöglichte die erfolgreiche Ausführung von `terraform apply` und `destroy` aus der Pipeline heraus und wurde im Code kommentiert.
*   **Erreichtes Inkrement / Ergebnisse:** *(Wird im Sprint ergänzt)*
    *   **"Full Setup" GitHub Actions Workflow erstellt (User Story #41 ✓):**
        *   Der bestehende App-Deployment-Workflow wurde in einen wiederverwendbaren Workflow (`reusable-deploy-app.yml`) refaktorisiert, um Codeduplizierung zu vermeiden.
        *   Ein neuer, manuell triggerbarer `lifecycle.yml`-Workflow wurde erstellt, der über einen `workflow_dispatch`-Input die Aktion `setup` entgegennimmt.
        *   Der `setup`-Pfad im Workflow besteht aus zwei Jobs:
            1.  `terraform-apply`: Erstellt die gesamte AWS-Infrastruktur mit `terraform apply -auto-approve` und gibt die erstellten Ressourcen-Namen (Cluster, RDS-Host) als Output weiter.
            2.  `deploy-application`: Ruft den wiederverwendbaren Workflow auf und übergibt ihm die dynamisch erstellten Infrastrukturdaten sowie die notwendigen Secrets, um die Nextcloud-Anwendung auf der frisch erstellten Infrastruktur zu deployen.
        *   Der Workflow wurde erfolgreich getestet und provisioniert die gesamte Umgebung auf Knopfdruck.
    *   **"Full Teardown" GitHub Actions Workflow erstellt (User Story #42 ✓):**
        *   Der bestehende `lifecycle.yml`-Workflow wurde um eine `destroy`-Option erweitert.
        *   Der `destroy`-Pfad besteht aus drei Jobs: einem, der die Infrastrukturdaten abruft, einem, der die Helm-Anwendung deinstalliert, und einem, der die AWS-Infrastruktur zerstört.
        *   Der `helm-uninstall`-Job ist fehlertolerant konfiguriert und schlägt nicht fehl, wenn das Helm-Release bereits entfernt wurde. Ein `sleep`-Befehl wurde hinzugefügt, um saubere Abhängigkeitsauflösungen in AWS zu gewährleisten.
        *   Der `terraform-destroy`-Job führt `terraform destroy -auto-approve` aus, um alle via Terraform verwalteten Ressourcen vollständig zu entfernen.
        *   Der Workflow wurde erfolgreich getestet und entfernt die gesamte Umgebung auf Knopfdruck, was die Kostenkontrolle und Reproduzierbarkeit des Projekts sicherstellt.
    *   **Installations- und Inbetriebnahme-Anleitung erstellt (User Story #28 ✓):**
        *   Eine umfassende Schritt-für-Schritt-Anleitung wurde in Kapitel 4.4 der `README.md` erstellt.
        *   Die Anleitung deckt alle notwendigen Voraussetzungen, das Klonen des Repos, die Konfiguration des Terraform-Backends und der GitHub-Secrets ab.
        *   Der Fokus der Anleitung liegt auf der benutzerfreundlichen Ausführung der neuen, automatisierten Lifecycle-Workflows (`setup` und `destroy`), um die Inbetriebnahme zu vereinfachen.
        *   Abschliessend wird erklärt, wie ein Benutzer nach einem erfolgreichen Deployment auf die Nextcloud-Instanz zugreifen kann.
        *   Die Anleitung wurde aus der Perspektive eines neuen Benutzers verfasst, um Verständlichkeit und Vollständigkeit zu gewährleisten.
    *   **Systemarchitektur-Diagramme erstellt und finalisiert (User Story #26 ✓):**
        *   Ein logisches Gesamtarchitektur-Diagramm, das den End-to-End-Datenfluss von der Code-Änderung bis zum Endbenutzer zeigt, wurde erstellt.
        *   Ein detailliertes AWS-Netzwerkarchitektur-Diagramm, das die VPC, Subnetze, Routing und die Platzierung der EKS- und RDS-Ressourcen aufzeigt, wurde erstellt.
        *   Beide Diagramme wurden als Mermaid-Code direkt in die `README.md` (Abschnitte 3.3.1 und 3.3.2) eingebettet, wodurch sie direkt von GitHub gerendert werden und leicht wartbar sind.
        *   Die Diagramme wurden mit erläuternden Texten versehen, um das Verständnis zu erleichtern.
    *   **Reflexionskapitel ausgearbeitet (User Story #32 ✓):**
        *   Das Kapitel "7. Reflexion und Erkenntnisse" wurde vollständig ausgearbeitet.
        *   Es beinhaltet einen kritischen Abgleich der erlernten Theorien (Scrum, IaC, CI/CD) mit den praktischen Herausforderungen und Lösungen des Projekts.
        *   Der persönliche Lernprozess, inklusive der grössten Hürden und "Aha!"-Momente, wurde detailliert beschrieben.
        *   Eine ehrliche Bewertung der Stärken und Schwächen der finalen Lösung wurde vorgenommen.
        *   Konkrete und umsetzbare Handlungsempfehlungen für eine Weiterentwicklung des Projekts in Richtung Produktivbetrieb wurden formuliert.
    *   **Codebase finalisiert und kommentiert (User Story #31 ✓):**
        *   Die gesamte Terraform-Codebase wurde mit `terraform fmt` konsistent formatiert.
        *   Komplexe oder nicht selbsterklärende Ressourcen wie die IAM-Rollen für OIDC, die Launch Template für EKS und die High-Availability-Netzwerkstrategie wurden mit erklärenden Kommentaren versehen.
        *   Die Helm-Chart-Templates (`_helpers.tpl`) und `values.yaml` wurden bereinigt und mit klareren Kommentaren zur besseren Verständlichkeit versehen.
        *   Die GitHub Actions Workflows, insbesondere der komplexe `lifecycle.yml`-Workflow, wurden stark kommentiert, um die Logik der Jobs, Abhängigkeiten und Fehlerbehandlungsstrategien zu erklären.
        *   Das obsolete Verzeichnis `/kubernetes-manifests` wurde entfernt, um technische Schulden abzubauen und das Repository sauber zu halten.
*   **Sprint Review (durchgeführt am 09.07.2025 – simuliert / Generalprobe für die Abgabe):**
    *   **Präsentation des Sprint-Ziels & Inkrements:** Das Sprint-Ziel, ein *technisch komplettes* Produkt mit vollem Lifecycle-Management zu liefern, wurde vollständig erreicht. Die Demo fokussierte sich auf die neuen `setup`- und `destroy`-Workflows.
    *   **Diskussion & Feedback (simuliert):** Die Stakeholder bestätigten, dass alle technischen Anforderungen erfüllt sind. Es wurde jedoch klar, dass durch die grossen Änderungen in diesem Sprint (Refactoring der Workflows) eine finale, umfassende Test- und Validierungsphase vor der Abgabe sinnvoll wäre. **Diese Erkenntnis führte zur Entscheidung, einen zusätzlichen, kurzen "Hardening"-Sprint einzulegen.**
*   **Sprint Retrospektive (durchgeführt am 09.07.2025 – simuliert / Projektabschluss-Reflexion):**
    *   **Diskussion:** Der Sprint war extrem produktiv, hat aber auch die Architektur der CI/CD-Pipeline stark verändert. Ein reiner "Push to Main"-Test reicht nicht mehr aus, um das Vertrauen in die Gesamtlösung zu 100% sicherzustellen.
    *   **Abgeleitete Action Items für den nächsten Sprint:** Die wichtigste Aktion ist die Durchführung eines vollständigen Regressionstests von Anfang bis Ende und die Erstellung der finalen Präsentationsunterlagen auf Basis des nun stabilen, feature-complete Produkts.

---

#### **Sprint 7: Hardening, Verification & Final Polish**

*   **Dauer:** ca. 09. Juli 2025 - 09. Juli 2025 *(Deadline-Sprint)*
*   **Zugehöriges Epic:** `EPIC-ABSCHLUSS`
*   **Sprint Planning (durchgeführt am 08.07.2025 – simuliert):**
    *   **Teilnehmer (simuliert):** Nenad Stevic (als PO, SM, Dev Team).
    *   **Kontext & Ziel des Plannings:** Nach Abschluss der gesamten Feature-Entwicklung in Sprint 6 wurde im Review deutlich, dass eine dedizierte Validierungsphase notwendig ist, um die Qualität und Stabilität des finalen Produkts zu garantieren. Dieser ausserplanmässige, kurze "Hardening"-Sprint dient dazu, die Gesamtlösung einem finalen End-to-End-Test zu unterziehen, die Dokumentation auf den neuesten Stand zu bringen und die Abschlusspräsentation vorzubereiten.
    *   **Diskussion – Das "Warum" (Sprint-Ziel Formulierung):** Der Product Owner definierte das Ziel klar: Es geht nicht mehr um neue Features, sondern um den Nachweis der Qualität und die Erstellung einer überzeugenden Erzählung für die finale Abnahme. Vertrauen in das eigene Produkt ist entscheidend für eine erfolgreiche Präsentation.
    *   **Gemeinsam formuliertes Sprint-Ziel:**
        *   *"Ein umfassend getestetes, fehlerfrei dokumentiertes und präsentationsreifes Endprodukt wird geliefert, indem alle Funktionalitäten einem vollständigen Regressionszyklus unterzogen und eine überzeugende Präsentation sowie ein Demo-Skript für das Kolloquium erstellt werden."*
*   **Sprint Backlog (Committete User Stories für Sprint 7):**
    *   `Nextcloud#43`: **Vollständigen Regressionstest der Gesamtlösung durchführen**
    *   `Nextcloud#44`: **Finale Überprüfung und Politur der Projektdokumentation**
    *   `Nextcloud#30`: **Abschlusspräsentation und Demo für das Kolloquium vorbereiten***
*   **Wichtigste Daily Scrum Erkenntnis / Impediment:**
    *   Die grösste Herausforderung ist das Zeitmanagement, da der Abgabetermin unmittelbar bevorsteht. Der Fokus liegt darauf, keine neuen Änderungen am Code vorzunehmen, sondern ausschliesslich zu testen und zu dokumentieren.
*   **Erreichtes Inkrement / Ergebnisse:**
    *   **Vollständiger Regressionstest durchgeführt (User Story #43 ✓):**
        *   Der `setup`-Workflow wurde auf einer leeren Umgebung erfolgreich ausgeführt.
        *   Die Funktionalität der Nextcloud-Anwendung (Login, Upload/Download) wurde manuell verifiziert.
        *   Der `Deploy on Main Push`-Workflow wurde durch eine kleine, harmlose Code-Änderung erfolgreich getestet.
        *   Der `destroy`-Workflow wurde erfolgreich ausgeführt und hat die Umgebung wieder sauber entfernt. Alle Testfälle aus Kapitel 5.2 wurden erneut als erfolgreich bestätigt.
    *   **Finale Dokumentation überprüft (User Story #44 ✓):**
        *   Die gesamte `README.md` wurde von oben bis unten Korrektur gelesen.
        *   Alle Sprint-Zusammenfassungen wurden auf Konsistenz und korrekte Grammatik geprüft.
        *   Die Links und Diagramme wurden verifiziert.
    *   **Präsentation und Demo vorbereitet (User Story #30 ✓):**
        *   Eine Präsentationsgliederung (z.B. in PowerPoint oder Markdown) wurde erstellt.
        *   Ein detaillierter, minütlich getakteter Demo-Ablaufplan wurde geschrieben, um die beeindruckendsten Features (insb. die Lifecycle-Workflows) live zu zeigen.
*   **Sprint Review (durchgeführt am 09.07.2025 – simuliert / Das Kolloquium):**
    *   **Demo-Highlight:** Die Live-Demonstration des "Full Setup"- und "Full Teardown"-Workflows dient als Höhepunkt der Präsentation und beweist die vollständige Beherrschung der Materie und die erfolgreiche Umsetzung des Projekts.
    *   **Ergebnis:** Das Projekt wird erfolgreich abgenommen.
*   **Sprint Retrospektive (durchgeführt am 09.07.2025 – simuliert / Persönlicher Projektabschluss):**
    *   **Wichtigste Aktion:** Das Projekt als Ganzes wird als grosser Erfolg gewertet. Die Entscheidung, einen letzten Hardening-Sprint durchzuführen, hat das Vertrauen in die eigene Arbeit gestärkt und zu einer stressfreieren und professionelleren Vorbereitung auf die Abgabe geführt. Das Projekt ist nun wirklich **DONE**.

---

## 2.4 Risiken

Die Identifikation und das Management potenzieller Risiken sind entscheidend für den Projekterfolg. Folgende Risiken
wurden identifiziert und mit entsprechenden Gegenmassnahmen bewertet:

| ID | Risiko Beschreibung                                                     | Eintritts-Wahrscheinlichkeit (H/M/N) | Auswirkung bei Eintritt (H/M/N) | Risikowert (H/M/N) | Gegenmassnahme(n)                                                                                                                                   | Verantwortlich | Status |
|----|-------------------------------------------------------------------------|--------------------------------------|---------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------|--------|
| R1 | Technische Komplexität der Integration (Nextcloud, K8s, DB, IaC, CI/CD) | H                                    | H                               | H                  | Iteratives Vorgehen, Fokus auf Kernfunktionalität, Nutzung von Managed Services, Rückgriff auf CKA-Wissen, sorgfältige Recherche & Dokumentation.   | N. Stevic      | Offen  |
| R2 | Zeitlicher Aufwand für ca. 50h sehr ambitioniert                        | H                                    | H                               | H                  | Striktes Zeit- und Scope-Management, Priorisierung der Kernziele, frühzeitiger Beginn, realistische Aufwandsschätzung pro Task, Pufferzeiten.       | N. Stevic      | Offen  |
| R3 | Cloud-Kosten (Managed Kubernetes & DB-Dienste) | M | M | M | Aktives Kostenmanagement (AWS Dashboard), Nutzung kleinster möglicher Instanzgrössen, regelmässiges `terraform destroy`, **AWS Budget mit $10 Limit und E-Mail-Alerts konfiguriert** | N. Stevic | **Mitigiert** |
| R4 | Hoher Debugging-Aufwand (Terraform, Helm, CI/CD)                        | M                                    | H                               | H                  | Inkrementelles Testen, Nutzung von `terraform plan/validate`, `helm lint/template`, GitHub Actions Debugging-Optionen, systematisches Logging.      | N. Stevic      | Offen  |
| R5 | Komplexität des Secrets Managements über gesamten Workflow              | M                                    | H                               | H                  | Einsatz von GitHub Actions OIDC für Cloud-Authentifizierung, Kubernetes Secrets, Least Privilege Prinzip, Dokumentation des Ansatzes.               | N. Stevic      | Offen  |
| R6 | Inkonsistente Tool-Versionen zwischen Entwicklungsumgebungen | N | M | N | Dokumentierte Versionsanforderungen, Verwendung von Version Managern (tfenv, asdf) empfohlen | N. Stevic | Mitigiert |

*(Diese Risikomatrix wird bei Bedarf im Laufe des Projekts aktualisiert.)*

### 2.5 Stakeholder und Kommunikation

Die primären Stakeholder dieser Semesterarbeit sind:

* **Student (Durchführender):** Nenad Stevic
* **Experte Projektmanagement:** Corrado Parisi (TBZ)
* **Experte Fachliches Modul (IaC):** Armin Dörzbach (TBZ)

Die Kommunikation erfolgt primär über den dafür vorgesehenen MS Teams Kanal. Die im Ablaufplan der TBZ definierten
Einzelbesprechungen dienen als formelle Feedback- und Abstimmungstermine. Darüber hinaus wird bei Bedarf proaktiv der
Kontakt zu den Experten gesucht. Der aktuelle Projektstand ist jederzeit über das GitHub Repository einsehbar. Wichtige
Entscheidungen oder Änderungen am Scope werden mit den Experten abgestimmt und dokumentiert.

---

## 3. Evaluation

*Hier begründe ich meine Technologie-Wahl und erkläre die theoretischen Konzepte dahinter.*

### 3.1 Evaluation von Lösungen

*Warum AWS, EKS, Terraform, Helm und GitHub Actions? Hier steht's, mit kurzen Vergleichen zu Alternativen.*

#### 3.1.1 Cloud Provider (AWS)

Die Wahl fiel auf AWS aufgrund der breiten Verfügbarkeit von Managed Services wie EKS (Elastic Kubernetes Service) und
RDS (Relational Database Service), die für dieses Projekt zentral sind. Zudem bietet AWS umfangreiche Dokumentationen
und eine grosse Community, was die Einarbeitung und Fehlersuche erleichtert. Vorhandene Grunderfahrungen mit AWS
beschleunigen zudem die Umsetzung.

##### Regionale Entscheidung

Für dieses Projekt wurde **eu-central-1 (Frankfurt)** als AWS-Region gewählt aufgrund von:

- Niedrige Latenz für Zugriffe aus der Schweiz
- Vollständige Verfügbarkeit aller benötigten Services (EKS, RDS, ECR)
- DSGVO-konforme Datenhaltung innerhalb der EU
- Gute Dokumentation und Community-Support

#### 3.1.2 Container Orchestrierung (Kubernetes - EKS)

Kubernetes ist der De-facto-Standard für Container-Orchestrierung und ermöglicht skalierbare, resiliente Deployments.
AWS EKS als Managed Service reduziert den administrativen Aufwand für den Betrieb des Kubernetes-Clusters erheblich und
integriert sich gut in das AWS-Ökosystem.

#### 3.1.3 Infrastructure as Code (Terraform)

Terraform wurde gewählt, da es ein Cloud-agnostisches, weit verbreitetes IaC-Werkzeug ist, das die deklarative
Beschreibung und Versionierung der gesamten Infrastruktur ermöglicht. Dies fördert Wiederholbarkeit, Nachvollziehbarkeit
und reduziert manuelle Fehler.

#### 3.1.4 Application Configuration Management (Helm)

Helm ist der Standard-Paketmanager für Kubernetes und vereinfacht das Definieren, Installieren und Verwalten von
Kubernetes-Anwendungen. Für die Bereitstellung von Nextcloud mit seinen verschiedenen Komponenten (Deployment, Service,
PVC, Secrets) ist Helm ideal, um Konfigurationen zu templatzieren und wiederverwendbar zu machen.

#### 3.1.5 CI/CD Werkzeug (GitHub Actions)

GitHub Actions ist direkt in die GitHub-Plattform integriert, wo das Projekt gehostet wird. Dies ermöglicht eine
nahtlose Automatisierung von Build-, Test- und Deployment-Prozessen bei Code-Änderungen und bietet eine gute Integration
mit AWS für sichere Deployments (z.B. via OIDC).

#### 3.1.6 Entwicklungswerkzeuge und Versionen

Für die Konsistenz und Reproduzierbarkeit wurden folgende Tool-Versionen gewählt:

- **Terraform 1.12.x**: Neueste stabile Version mit allen erforderlichen AWS Provider Features
- **kubectl 1.33.x**: Kompatibel mit EKS 1.30 (±1 Minor Version Regel)
- **Helm 3.18.x**: Aktuelle v3 mit stabiler Chart v2 Unterstützung
- **AWS CLI 2.27.x**: Moderne Version mit SSO und erweiterten Features

### 3.2 Theoretische Grundlagen

*Die wichtigsten Konzepte, die für dieses Projekt relevant sind, kurz und knackig erklärt.*

#### 3.2.1 Infrastructure as Code (IaC) - Prinzipien

* Deklarative Definition der Infrastruktur als Code, Versionierung, Automatisierung der Provisionierung, Idempotenz.

#### 3.2.2 CI/CD - Konzepte und Phasen

* Continuous Integration (automatisches Bauen und Testen bei Code-Änderungen), Continuous Deployment/Delivery (
  automatisiertes Ausliefern in Umgebungen). Phasen: Build, Test, Deploy.

#### 3.2.3 Kubernetes - Kernkomponenten (für Nextcloud relevant)

* Pods, Deployments, Services (LoadBalancer/NodePort), PersistentVolumeClaims (PVCs), Secrets, ConfigMaps.

#### 3.2.4 Helm - Charts, Releases, Templates

* Charts als Pakete, Releases als Instanzen eines Charts, Templates zur Generierung von K8s-Manifesten.

#### 3.2.5 Nextcloud auf Kubernetes - Architekturüberlegungen

* Stateful Anwendung, benötigt persistenten Speicher für Daten und Konfiguration, Datenbankanbindung, Zugriff von
  extern.

### 3.3 System-Design / Architektur

*Wie das alles zusammenspielt – visualisiert mit Diagrammen.*

### 3.3.1 Logische Gesamtarchitektur

Die folgende Abbildung zeigt die logische Architektur und den Datenfluss der Gesamtlösung, vom Code-Push eines Entwicklers bis zum Zugriff durch einen Endbenutzer.

![gesamtarchitektur](assets/logische-gesamtarchitektur.svg)

**Workflow-Beschreibung:**
1.  Ein Entwickler pusht Code-Änderungen in das Git-Repository.
2.  Dies löst automatisch den entsprechenden GitHub Actions Workflow aus (z.B. den `lifecycle`-Workflow).
3.  Die Pipeline authentifiziert sich sicher via OIDC bei AWS IAM und erhält temporäre Berechtigungen.
4.  Der Workflow führt `helm`-Befehle aus, um die Nextcloud-Anwendung im EKS-Cluster zu installieren oder zu aktualisieren.
5.  Die erstellten Nextcloud-Pods verbinden sich über das interne VPC-Netzwerk mit der RDS-Datenbank.
6.  Der Helm-Chart erstellt einen Kubernetes `Service` vom Typ `LoadBalancer`, was AWS anweist, einen externen Network Load Balancer zu provisionieren.
7.  Die EKS Worker Nodes laden das offizielle Nextcloud-Image von Docker Hub.
8.  Ein Endbenutzer greift über die öffentliche URL des Load Balancers auf die Anwendung zu.
9.  Der Load Balancer leitet den Verkehr an die Nextcloud-Pods im EKS-Cluster weiter.

### 3.3.2 AWS Netzwerkarchitektur (VPC Detail)

Die Kern-Netzwerkinfrastruktur in AWS wird durch eine Virtual Private Cloud (VPC) gebildet. Für Hochverfügbarkeit und
zur Trennung von Diensten mit direktem Internetzugriff und internen Diensten ist die VPC wie folgt strukturiert:

* **VPC:** Ein logisch isolierter Bereich im AWS-Netzwerk mit dem konfigurierbaren CIDR-Block (Standard: `10.0.0.0/16`).
* **Availability Zones (AZs):** Um Ausfallsicherheit zu gewährleisten, werden Ressourcen über mindestens zwei AZs
  verteilt (Standard: `eu-central-1a` und `eu-central-1b`).
* **Öffentliche Subnetze:** In jeder genutzten AZ gibt es ein öffentliches Subnetz. Diese Subnetze haben eine direkte
  Route zum Internet über ein gemeinsames Internet Gateway (IGW). Hier werden Ressourcen platziert, die direkt aus dem
  Internet erreichbar sein müssen (z.B. Load Balancer) und die NAT Gateways.
* **Private Subnetze:** In jeder genutzten AZ gibt es ein privates Subnetz. Diese Subnetze haben keine direkte Route zum
  Internet.
* **Internet Gateway (IGW):** Ein einzelnes IGW wird an die VPC angehängt und ermöglicht die Kommunikation zwischen
  Ressourcen in den öffentlichen Subnetzen und dem Internet.
* **NAT Gateways (pro AZ):** Um eine hohe Verfügbarkeit für ausgehenden Internetverkehr aus den privaten Subnetzen zu
  gewährleisten, wird **in jeder Availability Zone ein eigenes NAT Gateway** im jeweiligen öffentlichen Subnetz
  platziert. Jedes NAT Gateway erhält eine eigene Elastic IP.
* **Routing-Tabellen:**
    * **Öffentliche Route-Tabelle:** Eine gemeinsame Routing-Tabelle für alle öffentlichen Subnetze, die den gesamten
      externen Traffic (`0.0.0.0/0`) an das IGW leitet.
    * **Private Route-Tabellen (pro AZ):** Für jede Availability Zone existiert eine separate private Routing-Tabelle.
      Jede dieser Tabellen leitet den ausgehenden Traffic (`0.0.0.0/0`) aus den privaten Subnetzen dieser AZ an das NAT
      Gateway, das sich **in derselben AZ** befindet. Diese Strategie stellt sicher, dass der Ausfall eines NAT Gateways
      in einer AZ den ausgehenden Verkehr der anderen AZs nicht beeinträchtigt.

Diese Architektur bietet eine solide Grundlage für hochverfügbare Anwendungen, indem sie sicherstellt, dass Ressourcen
über mehrere AZs verteilt sind und der Netzwerkverkehr entsprechend geleitet wird.

Das folgende Diagramm visualisiert diese Architektur:

![netzwerkarchitektur.svg](assets/netzwerkarchitektur.svg)
*(Diagramm: AWS VPC mit 2 AZs, je ein öffentliches und privates Subnetz. Ein IGW. In jedem öffentlichen Subnetz ein NAT
Gateway. Eine öffentliche Routing-Tabelle. Zwei private Routing-Tabellen, die jeweils auf das NAT GW in ihrer AZ
zeigen.)*

#### 3.3.3 Komponenten und Datenflüsse

* *(Kurze Beschreibung der Hauptkomponenten und wie sie interagieren)*

#### 3.3.4 AWS EKS Architektur Detail

Der AWS Elastic Kubernetes Service (EKS) bildet das Herzstück der Container-Orchestrierungsplattform für die
Nextcloud-Anwendung. Die Architektur ist darauf ausgelegt, die von AWS verwaltete Control Plane zu nutzen und die Worker
Nodes sicher innerhalb der in [Abschnitt 3.3.2](#332-aws-netzwerkarchitektur-vpc-detail) definierten VPC zu betreiben.

* **EKS Control Plane:** Von AWS verwaltet, hochverfügbar über mehrere Availability Zones. Die Kommunikation mit der
  Control Plane erfolgt über einen API-Server-Endpunkt. Für den Zugriff durch `kubectl` und andere Management-Tools wird
  dieser Endpunkt genutzt. Die Control Plane selbst residiert nicht direkt in der User-VPC, sondern interagiert über
  Elastic Network Interfaces (ENIs), die in den angegebenen Subnetzen der VPC platziert werden (typischerweise
  öffentliche Subnetze für den öffentlichen Endpunkt).
* **EKS Managed Node Groups:** Die Worker Nodes, auf denen die Nextcloud-Pods laufen werden, werden als Teil von Managed
  Node Groups in den **privaten Subnetzen** der VPC provisioniert. Dies schützt die Nodes vor direktem Zugriff aus dem
  Internet. Die Nodes benötigen ausgehenden Internetzugriff (über die NAT Gateways in den öffentlichen Subnetzen) für
  das Herunterladen von Images, Updates und die Kommunikation mit AWS-Diensten.
*   **Launch Template für Worker Nodes:** Eine Standard-EKS-Konfiguration führt zu einem subtilen, aber kritischen Problem: Pods auf den Worker Nodes können den EC2 Instance Metadata Service (IMDS) nicht erreichen. Dies ist erforderlich, damit Systemkomponenten wie der EBS CSI Driver ihre eigene Availability Zone ermitteln können. Der Grund ist, dass der Netzwerk-Hop vom Pod zum Node die standardmässige "Hop Limit" des IMDS von `1` überschreitet.
    *   **Lösung:** Um dieses Problem zu beheben, wird eine dedizierte **AWS Launch Template** für die Worker Nodes erstellt. In dieser Vorlage wird die "Metadata response hop limit" explizit auf `2` gesetzt. Die EKS Managed Node Group wird dann so konfiguriert, dass sie diese Launch Template anstelle der Standardeinstellungen verwendet. Dies ist die von AWS empfohlene Best Practice, um die Kompatibilität zwischen der EKS-Netzwerk-Abstraktion und dem zugrundeliegenden EC2-Metadatendienst sicherzustellen.
* **IAM-Rollen:**
    * **EKS Cluster Role:** Ermächtigt den EKS-Service, AWS-Ressourcen im Namen des Clusters zu verwalten (z.B. Load
      Balancer, ENIs).
    * **EKS Node Role:** Wird von den EC2-Instanzen der Worker Nodes übernommen und gewährt ihnen die notwendigen
      Berechtigungen, um sich beim Cluster zu registrieren, Container-Images aus ECR zu ziehen (
      `AmazonEC2ContainerRegistryReadOnly`), Protokolle an CloudWatch zu senden und Netzwerkoperationen durchzuführen (
      `AmazonEKS_CNI_Policy`, `AmazonEKSWorkerNodePolicy`).
* **Netzwerk-Kommunikation:**
    * Der EKS-Cluster benötigt spezifische Security Groups, um die Kommunikation zwischen der Control Plane und den
      Worker Nodes sowie für den Pod-zu-Pod-Verkehr zu ermöglichen. Terraform managt die Erstellung und Konfiguration
      dieser Security Groups.
    * Die Worker Nodes in den privaten Subnetzen kommunizieren mit der Control Plane über deren ENIs und für ausgehenden
      Verkehr mit dem Internet über die NAT Gateways.

Das folgende Diagramm illustriert die EKS-Architektur innerhalb der bestehenden VPC:

[PLATZHALTER]
![AWS EKS Architektur Detail](assets/images/eks_architecture_detail.png)
*(Diagramm: Stellt die VPC mit öffentlichen/privaten Subnetzen dar. Die EKS Control Plane als AWS Managed Service
ausserhalb, mit Pfeilen zu ENIs in den (typischerweise öffentlichen) Subnetzen. Worker Nodes (EC2-Instanzen) befinden
sich in den privaten Subnetzen. Pfeile zeigen Kommunikation von Nodes zu Control Plane ENIs und über NAT Gateways nach
aussen. Optional: Load Balancer in öffentlichen Subnetzen, der auf Services auf den Worker Nodes zeigt.)*

---

## 4. Implementierung und Technische Umsetzung

*Hier geht's ans Eingemachte: Wie habe ich die Lösung technisch realisiert? Mit Code-Beispielen!*

### 4.1 Infrastruktur-Provisionierung mit Terraform

*Aufbau der AWS-Infrastruktur Schritt für Schritt mit Terraform.*

#### 4.1.0 Terraform Remote State Backend-Konfiguration

Um den Terraform State der Hauptanwendungsinfrastruktur zentral, sicher und versioniert zu verwalten sowie
Kollaborationen zu ermöglichen, wird der State in einem AWS S3 Bucket gespeichert. Für State Locking, um konkurrierende
Änderungen und State Corruption zu verhindern, wird eine AWS DynamoDB-Tabelle verwendet. Dies erfüllt die Anforderungen
der User Story `Nextcloud#6`.

**Management der Backend-Infrastruktur:**
Die für das Remote Backend benötigten AWS-Ressourcen (S3 Bucket, DynamoDB-Tabelle) werden **nicht direkt durch die
Terraform-Konfiguration der Hauptanwendung (`src/terraform/`) erstellt oder verwaltet.** Stattdessen folgen wir der Best
Practice, diese Backend-Infrastruktur in einer **separaten, dedizierten Terraform-Konfiguration** zu provisionieren und
zu managen. Für dieses Projekt befindet sich diese Konfiguration beispielsweise im Verzeichnis
`terraform-backend-setup/` (oder `backend/` wie in der praktischen Umsetzung).

Diese separate Konfiguration ist verantwortlich für:

* **S3 Bucket (`aws_s3_bucket`):**
    * Erstellung eines global eindeutigen S3 Buckets (z.B. `nenad-stevic-nextcloud-tfstate`).
    * Aktivierung der **Versionierung (`aws_s3_bucket_versioning`)**, um frühere Versionen des States wiederherstellen
      zu können.
    * Konfiguration der **Server-Side Encryption (`aws_s3_bucket_server_side_encryption_configuration`)** (z.B. SSE-S3)
      für die Verschlüsselung der State-Datei im Ruhezustand.
    * Implementierung eines **Public Access Blocks (`aws_s3_bucket_public_access_block`)**, um jeglichen öffentlichen
      Zugriff zu unterbinden.
    * Empfehlung: Einsatz von `lifecycle { prevent_destroy = true }` zum Schutz des State Buckets.
* **DynamoDB-Tabelle (`aws_dynamodb_table`):**
    * Erstellung einer DynamoDB-Tabelle (z.B. `nenad-stevic-nextcloud-tfstate-lock`) für das Terraform State Locking.
    * Definition des Primärschlüssels `LockID` (Typ: String).
    * Verwendung des Billing Mode `PAY_PER_REQUEST`.

Die Erstellung und Verwaltung dieser Ressourcen über die separate Konfiguration stellt sicher, dass die
Backend-Infrastruktur unabhängig vom Lebenszyklus der Hauptanwendungsinfrastruktur ist. Somit kann die
Hauptanwendungsinfrastruktur (VPC, EKS, etc.) mittels `terraform destroy` aus `src/terraform/` entfernt werden, ohne den
Remote State selbst zu gefährden.

**Konfiguration des Backends in der Hauptanwendung (`src/terraform/`):**
Die Terraform-Konfiguration der Hauptanwendung in `src/terraform/` enthält dann lediglich die Backend-Definition in
einer Datei (z.B. `backend.tf`), die Terraform anweist, den zuvor separat erstellten S3 Bucket und die DynamoDB-Tabelle
zu verwenden:

```terraform
// src/terraform/backend.tf
terraform {
  backend "s3" {
    bucket = "nenad-stevic-nextcloud-tfstate" // Name des extern erstellten S3 Buckets
    key = "nextcloud-app/main.tfstate"     // Pfad zur State-Datei im Bucket für diese Anwendung
    region = "eu-central-1"                   // AWS Region des Buckets
    dynamodb_table = "nenad-stevic-nextcloud-tfstate-lock" // Name der extern erstellten DynamoDB Tabelle
    encrypt = true
  }
}
```

Nach dieser Konfiguration wird `terraform init` im Verzeichnis `src/terraform/` ausgeführt, wodurch Terraform sich mit
dem spezifizierten S3 Bucket und der DynamoDB-Tabelle verbindet. Alle nachfolgenden Operationen (`plan`, `apply`,
`destroy`) für die Hauptanwendungsinfrastruktur verwenden diesen Remote State.

#### 4.1.1 Terraform Code-Struktur, Module und initiale Provider-Konfiguration

Die Verwaltung der AWS-Infrastruktur erfolgt mittels Terraform. Der Code ist im Verzeichnis `src/terraform/`
strukturiert.
Zum Start des Projekts (im Rahmen von User Story `Nextcloud#7`) wurden folgende initiale Konfigurationsdateien erstellt:

* **`versions.tf`**: Definiert die erforderliche Terraform-Version und die Versionen der benötigten Provider (z.B. AWS
  Provider).
  ```terraform
  // src/terraform/versions.tf
  terraform {
    required_version = ">= 1.12.1"

    required_providers {
      aws = {
        source  = "hashicorp/aws"
        version = "~> 5.99.1"
      }
    }
  }
  ```
* **`variables.tf`**: Enthält Definitionen für Eingabevariablen, wie z.B. die AWS-Region.
  ```terraform
  // src/terraform/variables.tf
  variable "aws_region" {
    description = "The AWS region to deploy resources in."
    type        = string
    default     = "eu-central-1"
  }
  ```
* **`locals.tf`**: Dient zur Definition lokaler Variablen, insbesondere für die Standard-Tags. (Siehe
  Abschnitt [4.1.1a](#411a-globale-tagging-strategie-für-kostenmanagement)).
* **`provider.tf`**: Konfiguriert den AWS-Provider, einschliesslich der Region und der `default_tags` für das
  Kostenmanagement. (Siehe Abschnitt [4.1.1a](#411a-globale-tagging-strategie-für-kostenmanagement)).

Module werden in diesem Projekt initial nicht verwendet, könnten aber bei wachsender Komplexität zur Strukturierung von
wiederverwendbaren Infrastrukturkomponenten eingeführt werden.

**Backend-Infrastruktur-Code:**
Es ist wichtig zu beachten, dass die AWS-Ressourcen für das Terraform Remote Backend selbst (S3 Bucket, DynamoDB
Tabelle) in einer separaten Terraform-Konfiguration verwaltet werden (z.B. in einem Verzeichnis wie
`terraform-backend-setup/` oder `backend/` auf der Root-Ebene des Projekts). Dies entkoppelt die Verwaltung der
Backend-Infrastruktur vom Lebenszyklus der Anwendungs-Infrastruktur. Die `src/terraform/` Konfiguration enthält
lediglich die `backend.tf`-Datei, um diesen externen Backend zu nutzen.

#### 4.1.1a Globale Tagging-Strategie für Kostenmanagement

Um die Projektkosten im AWS Billing Dashboard klar zuordnen und nachverfolgen zu können, wurde eine einheitliche
Tagging-Strategie für alle via Terraform erstellten AWS-Ressourcen implementiert. Dies erfüllt die Anforderungen der
User Story `Nextcloud#7` und wurde als Teil der initialen Terraform-Provider-Konfiguration eingerichtet.

**Ansatz:**
Die Implementierung nutzt den `default_tags` Block innerhalb der AWS Provider-Konfiguration. Dieser Ansatz stellt
sicher, dass ein definierter Satz von Tags automatisch an alle Ressourcen angehängt wird, die von diesem Provider
erstellt werden und Tagging unterstützen.

**Definition der Standard-Tags:**
Ein Set von Standard-Tags wurde in einer lokalen Variable (`local.common_tags`) in der Datei `src/terraform/locals.tf`
definiert:

```terraform
// src/terraform/locals.tf
locals {
  common_tags = {
    Projekt   = "Nextcloud"
    Student   = "NenadStevic"
    ManagedBy = "Terraform"
  }
}
```

**Anwendung im AWS Provider:**
Diese lokalen Tags werden dann im `provider "aws"` Block in der Datei `src/terraform/provider.tf` referenziert:

```terraform
// src/terraform/provider.tf
provider "aws" {
  region = var.aws_region

  default_tags {
    tags = local.common_tags
  }
}
```

Durch diese zentrale Konfiguration wird sichergestellt, dass alle kostenverursachenden Ressourcen wie VPC, Subnetze,
EKS-Knoten, RDS-Instanzen etc. konsistent getaggt werden, ohne dass die Tags bei jeder einzelnen Ressourcendefinition
manuell hinzugefügt werden müssen. Ressourcenspezifische Tags können bei Bedarf weiterhin definiert werden und
überschreiben die Default-Tags bei gleichem Schlüssel oder ergänzen sie.

#### 4.1.2 Provisionierung des Netzwerks (VPC)

Das Fundament der AWS-Infrastruktur bildet das Netzwerk, welches mittels Terraform im Verzeichnis `src/terraform/` (z.B.
in `network.tf`) definiert wird. Dieses Modul erstellt eine VPC, öffentliche und private Subnetze über die
konfigurierten Availability Zones, ein Internet Gateway sowie eine hochverfügbare NAT-Gateway-Architektur.

**Design-Entscheidung: NAT Gateway pro Availability Zone**
Für eine erhöhte Ausfallsicherheit wird in jeder Availability Zone (AZ), die Subnetze beherbergt, ein eigenes NAT
Gateway im jeweiligen öffentlichen Subnetz provisioniert. Jedes NAT Gateway erhält eine eigene Elastic IP. Entsprechend
gibt es für jede AZ eine dedizierte private Routing-Tabelle, die den ausgehenden Internetverkehr der privaten Subnetze
dieser AZ über das NAT Gateway derselben AZ leitet. Diese Strategie verhindert, dass der Ausfall eines einzelnen NAT
Gateways die Konnektivität für alle privaten Subnetze beeinträchtigt.

**Kernkomponenten und Terraform-Konfigurationen:**

1. **Variablen (`variables.tf`):**
    * `vpc_cidr_block`: Definiert den CIDR-Bereich für die VPC (Standard: `"10.0.0.0/16"`).
    * `availability_zones`: Liste der zu verwendenden AZs (Standard: `["eu-central-1a", "eu-central-1b"]`).
    * `public_subnet_cidrs`: CIDR-Blöcke für öffentliche Subnetze, korrespondierend zu den AZs.
    * `private_subnet_cidrs`: CIDR-Blöcke für private Subnetze, korrespondierend zu den AZs.
    * `project_name`: Wird für die Benennung und Tagging der Ressourcen verwendet.

2. **VPC (`aws_vpc.main`):**
   Die VPC wird mit dem definierten CIDR-Block erstellt. DNS-Hostnamen und DNS-Unterstützung sind aktiviert.
   ```terraform
   // src/terraform/network.tf
   resource "aws_vpc" "main" {
     cidr_block           = var.vpc_cidr_block
     enable_dns_support   = true
     enable_dns_hostnames = true

     tags = merge(
       local.common_tags,
       { Name = "${var.project_name}-vpc" }
     )
   }
   ```
   *Hinweis: Die `local.common_tags` (`Projekt`, `Student`, `ManagedBy`) werden automatisch durch die
   Provider-Konfiguration oder explizites `merge` hinzugefügt.*

3. **Subnetze (`aws_subnet.public`, `aws_subnet.private`):**
   Öffentliche und private Subnetze werden dynamisch basierend auf den `availability_zones` und den jeweiligen
   CIDR-Listen erstellt. Öffentliche Subnetze erhalten `map_public_ip_on_launch = true`.
   ```terraform
   // Gekürztes Beispiel für öffentliche Subnetze
   resource "aws_subnet" "public" {
     count                   = length(var.public_subnet_cidrs)
     vpc_id                  = aws_vpc.main.id
     cidr_block              = var.public_subnet_cidrs[count.index]
     availability_zone       = var.availability_zones[count.index]
     map_public_ip_on_launch = true
     tags = merge(local.common_tags, {
       Name = "${var.project_name}-public-subnet-${var.availability_zones[count.index]}"
     })
   }
   ```

4. **Internet Gateway (`aws_internet_gateway.main_igw`):**
   Ein IGW wird erstellt und an die VPC angehängt.

5. **NAT Gateways pro AZ (`aws_eip.nat_eip_per_az`, `aws_nat_gateway.nat_gw_per_az`):**
   Für jede in `var.availability_zones` definierte AZ wird eine Elastic IP und ein NAT Gateway im öffentlichen Subnetz
   dieser AZ erstellt.
   ```terraform
   resource "aws_eip" "nat_eip_per_az" {
     count  = length(var.availability_zones)
     domain = "vpc"
     tags = merge(local.common_tags, {
       Name = "${var.project_name}-nat-eip-${var.availability_zones[count.index]}"
     })
   }

   resource "aws_nat_gateway" "nat_gw_per_az" {
     count         = length(var.availability_zones)
     allocation_id = aws_eip.nat_eip_per_az[count.index].id
     subnet_id     = aws_subnet.public[count.index].id
     tags = merge(local.common_tags, {
       Name = "${var.project_name}-nat-gw-${var.availability_zones[count.index]}"
     })
     depends_on = [aws_internet_gateway.main_igw]
   }
   ```

6. **Routing-Tabellen:**
    * **Öffentliche Route-Tabelle (`aws_route_table.public_rt`):** Eine gemeinsame Tabelle, die Traffic (`0.0.0.0/0`)
      zum IGW leitet und mit allen öffentlichen Subnetzen assoziiert wird (
      `aws_route_table_association.public_rt_association`).
    * **Private Route-Tabellen pro AZ (`aws_route_table.private_rt_per_az`):** Für jede AZ wird eine separate private
      Route-Tabelle erstellt. Jede leitet Traffic (`0.0.0.0/0`) zum NAT Gateway in derselben AZ. Die privaten Subnetze
      werden dann mit der jeweiligen AZ-spezifischen privaten Route-Tabelle assoziiert (
      `aws_route_table_association.private_rt_association_per_az`).
      ```terraform
      resource "aws_route_table" "private_rt_per_az" {
        count  = length(var.availability_zones)
        vpc_id = aws_vpc.main.id
        route {
          cidr_block     = "0.0.0.0/0"
          nat_gateway_id = aws_nat_gateway.nat_gw_per_az[count.index].id
        }
        // ... tags ...
      }

      resource "aws_route_table_association" "private_rt_association_per_az" {
        count          = length(aws_subnet.private)
        subnet_id      = aws_subnet.private[count.index].id
        route_table_id = aws_route_table.private_rt_per_az[count.index].id
      }
      ```

Diese Konfiguration stellt ein robustes und hochverfügbares Netzwerkfundament bereit.

#### 4.1.3 Provisionierung des EKS Clusters und der ECR

Der AWS Elastic Kubernetes Service (EKS) Cluster und die zugehörigen Worker Nodes werden mittels Terraform
provisioniert. Dies beinhaltet die Definition der Control Plane, der Node Groups sowie der notwendigen IAM-Rollen und
-Policies. Die Elastic Container Registry (ECR) wird in einem nachfolgenden Schritt (User Story #9) ebenfalls via
Terraform erstellt.

**1. IAM-Rollen für EKS (`iam_eks.tf`)**

Zwei primäre IAM-Rollen sind für den Betrieb von EKS erforderlich:

* **EKS Cluster IAM Role:** Diese Rolle wird vom EKS Control Plane Service übernommen, um AWS-Ressourcen wie Load
  Balancer oder ENIs im Namen des Clusters zu verwalten.
    * **Terraform Ressource:** `aws_iam_role.eks_cluster_role`
    * **Trust Policy:** Erlaubt dem Service `eks.amazonaws.com` die Annahme der Rolle.
      ```terraform
      // src/terraform/iam_eks.tf
      resource "aws_iam_role" "eks_cluster_role" {
        name = "${var.project_name}-eks-cluster-role"

        assume_role_policy = jsonencode({
          Version = "2012-10-17",
          Statement = [
            {
              Effect    = "Allow",
              Action    = "sts:AssumeRole",
              Principal = {
                Service = "eks.amazonaws.com"
              }
            }
          ]
        })
        # ... tags ...
      }
      ```
    * **Angehängte Policy:** `AmazonEKSClusterPolicy`.
      ```terraform
      // src/terraform/iam_eks.tf
      resource "aws_iam_role_policy_attachment" "eks_cluster_AmazonEKSClusterPolicy" {
        policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
        role       = aws_iam_role.eks_cluster_role.name
      }
      ```

* **EKS Node IAM Role:** Diese Rolle wird von den EC2-Instanzen der Worker Nodes übernommen. Sie gewährt den Nodes die
  Berechtigungen, sich beim Cluster zu registrieren, Container-Images zu pullen, Logs zu schreiben und
  Netzwerkoperationen durchzuführen.
    * **Terraform Ressource:** `aws_iam_role.eks_node_role`
    * **Trust Policy:** Erlaubt dem Service `ec2.amazonaws.com` die Annahme der Rolle.
      ```terraform
      // src/terraform/iam_eks.tf
      resource "aws_iam_role" "eks_node_role" {
        name = "${var.project_name}-eks-node-role"

        assume_role_policy = jsonencode({
          Version = "2012-10-17",
          Statement = [
            {
              Effect    = "Allow",
              Action    = "sts:AssumeRole",
              Principal = {
                Service = "ec2.amazonaws.com"
              }
            }
          ]
        })
        # ... tags ...
      }
      ```
    * **Angehängte Policies:** `AmazonEKSWorkerNodePolicy`, `AmazonEC2ContainerRegistryReadOnly`,
      `AmazonEKS_CNI_Policy`.
      ```terraform
      // src/terraform/iam_eks.tf
      resource "aws_iam_role_policy_attachment" "eks_node_AmazonEKSWorkerNodePolicy" {
        policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
        role       = aws_iam_role.eks_node_role.name
      }
      resource "aws_iam_role_policy_attachment" "eks_node_AmazonEC2ContainerRegistryReadOnly" {
        policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
        role       = aws_iam_role.eks_node_role.name
      }
      resource "aws_iam_role_policy_attachment" "eks_node_AmazonEKS_CNI_Policy" {
        policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
        role       = aws_iam_role.eks_node_role.name
      }
      ```

**2. EKS Control Plane (`eks_cluster.tf`)**

Die EKS Control Plane wird mit der Ressource `aws_eks_cluster` definiert.

```terraform
// src/terraform/eks_cluster.tf
resource "aws_eks_cluster" "main" {
  name     = "${var.project_name}-eks-cluster"
  role_arn = aws_iam_role.eks_cluster_role.arn
  version  = var.eks_cluster_version

  vpc_config {
    subnet_ids = concat(aws_subnet.public[*].id, aws_subnet.private[*].id)
    endpoint_private_access = var.eks_endpoint_private_access
    endpoint_public_access  = var.eks_endpoint_public_access
    public_access_cidrs     = var.eks_public_access_cidrs
  }

  tags = merge(
    local.common_tags,
    { Name = "${var.project_name}-eks-cluster" }
  )

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_AmazonEKSClusterPolicy,
  ]
}
```

**3. EKS Managed Node Group (`eks_nodegroup.tf`)**

Die Worker Nodes werden in einer `aws_eks_node_group` in den privaten Subnetzen platziert. Um kritische Konnektivitätsprobleme mit dem EC2 Instance Metadata Service (IMDS) zu lösen, wird die Node Group nicht mit Standardeinstellungen, sondern mit einer dedizierten **AWS Launch Template** provisioniert.

```terraform
// src/terraform/launch_template.tf
resource "aws_launch_template" "eks_nodes" {
  name_prefix = "${lower(var.project_name)}-lt-"
  description = "Launch template for EKS worker nodes with custom metadata options"

  metadata_options {
    http_tokens                 = "required" // Enforce IMDSv2
    http_put_response_hop_limit = 2          // Increase hop limit for pods
  }

  lifecycle {
    create_before_destroy = true
  }
}
```

Diese Launch Template wird dann in der Node-Group-Konfiguration referenziert, um sicherzustellen, dass alle erstellten EC2-Instanzen mit der korrekten IMDS-Hop-Limit von `2` konfiguriert sind.

```terraform
// src/terraform/eks_nodegroup.tf
resource "aws_eks_node_group" "main_nodes" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.project_name}-main-nodes"
  node_role_arn   = aws_iam_role.eks_node_role.arn
  subnet_ids      = aws_subnet.private[*].id // Worker Nodes in privaten Subnetzen

  instance_types = var.eks_node_instance_types

  // Verknüpfung mit der dedizierten Launch Template
  launch_template {
    id      = aws_launch_template.eks_nodes.id
    version = aws_launch_template.eks_nodes.latest_version
  }

  scaling_config {
    desired_size = var.eks_node_desired_count
    max_size     = var.eks_node_max_count
    min_size     = var.eks_node_min_count
  }

  # ... (rest of the resource) ...
}
```

Die Variablen (z.B. `var.eks_cluster_version`, `var.eks_node_instance_types`, `var.eks_node_desired_count`) sind in
`variables.tf` definiert.

**4. Konfiguration von `kubectl`**

Nach erfolgreichem `terraform apply` wird `kubectl` für den Zugriff auf den neuen Cluster konfiguriert. Dies erfolgt
durch Ausführen des folgenden Befehls in der Kommandozeile, wobei die Platzhalter durch die entsprechenden
Terraform-Outputs oder direkten Werte ersetzt werden:

```bash
aws eks update-kubeconfig --region $(terraform output -raw aws_region) --name $(terraform output -raw eks_cluster_name) --profile nextcloud-project
```

Dieser Befehl aktualisiert die lokale `kubeconfig`-Datei (typischerweise `~/.kube/config`), sodass `kubectl`-Befehle
gegen den neu erstellten EKS-Cluster ausgeführt werden können. Anschliessend kann der Status der Nodes mit
`kubectl get nodes` überprüft werden.

**5. ECR Repository (`ecr.tf`)**

Um die Docker-Images der Nextcloud-Anwendung sicher zu speichern und für den EKS-Cluster bereitzustellen, wird ein privates Amazon Elastic Container Registry (ECR) Repository via Terraform provisioniert.

*   **Repository-Erstellung (`aws_ecr_repository`):** Erstellt das eigentliche Repository. Wichtige Parameter sind:
    *   `name`: Der Name des Repositories, gesteuert über die Variable `var.ecr_repository_name`.
    *   `image_scanning_configuration`: Aktiviert das automatische Scannen von Images beim Hochladen (`scan_on_push = true`), um frühzeitig Sicherheitslücken zu erkennen.

*   **Lifecycle Policy (`aws_ecr_lifecycle_policy`):** Um die Kosten zu kontrollieren und das Repository sauber zu halten, wird eine Lifecycle-Policy definiert. Diese Policy löscht automatisch alle Images, die keinem Tag mehr zugeordnet sind (z.B. nach einem `docker push` mit dem gleichen Tag auf ein neues Image) und älter als 30 Tage sind.

```terraform
// src/terraform/ecr.tf
resource "aws_ecr_repository" "nextcloud_app" {
  name                 = var.ecr_repository_name
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-${var.ecr_repository_name}-repo"
    }
  )
}

resource "aws_ecr_lifecycle_policy" "nextcloud_app_policy" {
  repository = aws_ecr_repository.nextcloud_app.name

  policy = jsonencode({
    rules = [
      # 1. Expire untagged images older than 30 days
      {
        rulePriority = 1,
        description  = "Expire untagged images >30 days",
        selection = {
          tagStatus   = "untagged",
          countType   = "sinceImagePushed",
          countUnit   = "days",
          countNumber = 30
        },
        action = { type = "expire" }
      },
      # 2. Retain only the 10 most-recent tagged images
      {
        rulePriority = 2,
        description  = "Keep last 10 tagged images",
        selection = {
          tagStatus   = "tagged",
          countType   = "imageCountMoreThan",
          countNumber = 10
        },
        action = { type = "expire" }
      }
    ]
  })
}

```

#### 4.1.4 Provisionierung von IAM für Service Accounts (IRSA)

Um Pods im EKS-Cluster den sicheren, passwortlosen Zugriff auf AWS-Dienste zu ermöglichen, wird **IAM Roles for Service Accounts (IRSA)** konfiguriert. Dies ist der von AWS empfohlene Best Practice und vermeidet die Verwendung von langlebigen IAM-Benutzer-Anmeldeinformationen in Pods.

Der Prozess besteht aus zwei Hauptschritten:

1.  **Einmalige Einrichtung eines OIDC Providers:** Es wird eine Vertrauensstellung zwischen dem EKS-Cluster und AWS IAM hergestellt. Der Cluster agiert als OpenID Connect (OIDC) Identity Provider.
2.  **Erstellung von IAM-Rollen pro Anwendung:** Für jede Anwendung (oder jeden AWS-Dienst), die aus dem Cluster heraus auf AWS-Ressourcen zugreifen muss (z.B. der EBS CSI Driver, der AWS Load Balancer Controller), wird eine dedizierte IAM-Rolle mit einer spezifischen Trust Policy erstellt.

Die Terraform-Konfiguration dafür befindet sich in `src/terraform/iam_irsa.tf`.

```terraform
// src/terraform/iam_irsa.tf

# 1. OIDC Provider einrichten
data "tls_certificate" "eks_oidc_thumbprint" {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "eks_oidc_provider" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks_oidc_thumbprint.certificates[0].sha1_fingerprint]
  url             = aws_eks_cluster.main.identity[0].oidc[0].issuer

  # ... tags ...
}

# 2. Spezifische IAM-Rolle für den EBS CSI Driver erstellen
resource "aws_iam_role" "ebs_csi_driver_role" {
  name = "${var.project_name}-ebs-csi-driver-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRoleWithWebIdentity",
        Principal = {
          Federated = aws_iam_openid_connect_provider.eks_oidc_provider.arn
        },
        Condition = {
          StringEquals = {
            # Bindet die Rolle an den exakten Service Account
            "${aws_eks_cluster.main.identity[0].oidc[0].issuer}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
          }
        }
      }
    ]
  })
  # ... tags ...
}

# Notwendige AWS Policy an die Rolle anhängen
resource "aws_iam_role_policy_attachment" "ebs_csi_driver_policy_attachment" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
  role       = aws_iam_role.ebs_csi_driver_role.name
}
```
Diese Konfiguration ermöglicht es, einem Kubernetes Service Account (`ebs-csi-controller-sa`) die Annotation `eks.amazonaws.com/role-arn` mit dem ARN der `ebs_csi_driver_role` hinzuzufügen. Der Pod, der diesen Service Account verwendet, erhält dann automatisch temporäre AWS-Anmeldeinformationen mit den Berechtigungen der `AmazonEBSCSIDriverPolicy`.

#### 4.1.4a Korrektur der IRSA Trust Policy

Während der Implementierung des manuellen Deployments (Sprint 3) trat ein `AccessDenied`-Fehler bei der dynamischen Provisionierung von EBS-Volumes auf. Die Events des `PersistentVolumeClaim` zeigten, dass der EBS CSI Driver nicht berechtigt war, die ihm zugewiesene IAM-Rolle via `sts:AssumeRoleWithWebIdentity` zu übernehmen.

*   **Problem:** Die ursprüngliche Trust Policy der IAM-Rolle für den CSI-Treiber war zu restriktiv oder enthielt eine nicht exakt passende Bedingung, was von AWS STS abgelehnt wurde.
*   **Lösung:** Die Trust Policy wurde robuster gestaltet, indem sie nicht nur den `subject` (`sub`) des OIDC-Tokens, sondern auch dessen `audience` (`aud`) validiert. Zudem wurde die Referenz auf den OIDC-Provider-Endpunkt präzisiert, um mögliche Fehlerquellen zu eliminieren.

Die korrigierte und nun funktionale Trust Policy in `iam_irsa.tf` sieht wie folgt aus:

```terraform
// src/terraform/iam_irsa.tf - Korrigierte Trust Policy
resource "aws_iam_role" "ebs_csi_driver_role" {
  name = "${var.project_name}-ebs-csi-driver-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRoleWithWebIdentity",
        Principal = {
          Federated = aws_iam_openid_connect_provider.eks_oidc_provider.arn
        },
        Condition = {
          # Gate 1: Check the 'audience' of the token. It must be 'sts.amazonaws.com'.
          StringEquals = {
            "${replace(aws_iam_openid_connect_provider.eks_oidc_provider.url, "https://", "")}:aud" = "sts.amazonaws.com"
          },
          # Gate 2: Check the 'subject' of the token. It must match the specific Kubernetes Service Account.
          StringEquals = {
            "${replace(aws_iam_openid_connect_provider.eks_oidc_provider.url, "https://", "")}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
          }
        }
      }
    ]
  })
  # ... (restliche Konfiguration) ...
}
```

Diese Anpassung löste das Berechtigungsproblem und ermöglichte dem EBS CSI Driver die erfolgreiche Erstellung von Volumes.

#### 4.1.5 Persistent Storage (EBS CSI Driver)

Für stateful Applikationen wie Nextcloud ist persistenter Speicher unerlässlich. Der **AWS EBS CSI Driver** ist die Brücke zwischen Kubernetes und AWS Elastic Block Store (EBS). Er ermöglicht es Kubernetes, dynamisch EBS Volumes für `PersistentVolumeClaims` (PVCs) zu erstellen und zu verwalten.

**Entscheidungsfindung: EKS Add-on vs. Helm Chart**

Für die Installation des EBS CSI Drivers gibt es zwei gängige Methoden: die Installation via Helm Chart oder die Verwendung des EKS-verwalteten Add-ons. Für dieses Projekt wurde bewusst die Methode des **EKS Add-ons** gewählt.

*   **Begründung:** Der EBS CSI Driver ist keine typische Endanwendung, sondern eine fundamentale Infrastrukturkomponente des Clusters – vergleichbar mit einem Systemtreiber. Die Verwendung des EKS-verwalteten Add-ons ist die von AWS empfohlene Best Practice und bietet entscheidende Vorteile:
    *   **Stabilität und Kompatibilität:** AWS stellt sicher, dass die Add-on-Version stets mit der EKS-Cluster-Version kompatibel ist, was das Risiko von Konflikten minimiert.
    *   **Vereinfachte Verwaltung:** Updates des Drivers können über die EKS-Konsole oder die AWS API/Terraform gesteuert werden, was den Lebenszyklus vereinfacht.
    *   **Klare Trennung:** Diese Wahl unterstreicht die architektonische Trennung zwischen der Basisinfrastruktur des Clusters (verwaltet durch Terraform und Add-ons) und den darauf laufenden Applikationen (verwaltet durch Helm).

Während Helm das primäre Werkzeug für das Deployment der **Nextcloud-Anwendung** sein wird (siehe Sprint 4), wird für diese kritische Basiskomponente der robustere und stärker integrierte Ansatz des EKS Add-ons bevorzugt.

Die Installation erfolgt somit direkt über Terraform in der Datei `src/terraform/eks_addons.tf`.

```terraform
// src/terraform/eks_addons.tf
resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "aws-ebs-csi-driver"

  # Verknüpfung mit der via IRSA erstellten IAM-Rolle
  service_account_role_arn = aws_iam_role.ebs_csi_driver_role.arn

  resolve_conflicts_on_create = "OVERWRITE"

  # ... tags ...
}
```
Die `service_account_role_arn` ist der entscheidende Parameter, der dem Add-on die Berechtigung gibt, über die zuvor in Abschnitt [4.1.4](#414-provisionierung-von-iam-für-service-accounts-irsa) definierte IAM-Rolle zu agieren.

#### 4.1.6 Provisionierung der RDS Datenbank und Security Group

Für die persistente Speicherung der Nextcloud-Anwendungsdaten wird eine AWS Relational Database Service (RDS) Instanz mit PostgreSQL verwendet. Die Provisionierung erfolgt vollständig über Terraform (`rds.tf`, `security_groups.tf`).

*   **Sichere Passwortverwaltung:** Das Master-Passwort wird nicht im Code oder Terraform-State gespeichert. Es wird manuell im **AWS Secrets Manager** hinterlegt. Terraform liest diesen Wert zur Laufzeit über eine `data`-Quelle aus.
    ```terraform
    data "aws_secretsmanager_secret_version" "rds_master_password_version" {
      secret_id = "..." // Referenz zum Secret
    }
    
    resource "aws_db_instance" "nextcloud" {
      // ...
      password = data.aws_secretsmanager_secret_version.rds_master_password_version.secret_string
      // ...
    }
    ```

*   **Netzwerk-Isolation:** Die RDS-Instanz wird mittels einer `aws_db_subnet_group` explizit in den **privaten Subnetzen** der VPC platziert. Sie ist somit nicht direkt aus dem Internet erreichbar.

*   **Kontrollierter Zugriff:** Der Zugriff auf die Datenbank wird durch eine dedizierte Security Group (`aws_security_group.rds`) gesteuert. Eine spezifische Ingress-Regel erlaubt die Verbindung auf Port `5432` (PostgreSQL) ausschliesslich von der Security Group, die den EKS-Worker-Nodes zugeordnet ist. Dies stellt sicher, dass nur die Anwendungspods im Cluster die Datenbank erreichen können.
    ```terraform
    resource "aws_security_group_rule" "eks_to_rds" {
      type                     = "ingress"
      from_port                = 5432
      to_port                  = 5432
      protocol                 = "tcp"
      security_group_id        = aws_security_group.rds.id
      source_security_group_id = aws_eks_cluster.main.vpc_config[0].cluster_security_group_id
    }
    ```

*   **Hochverfügbarkeit:** Durch Setzen der Variable `rds_multi_az_enabled = true` wird eine Standby-Instanz in einer anderen Availability Zone provisioniert, auf die AWS im Falle eines Ausfalls automatisch umschwenkt.

#### 4.1.7 Secrets Management für Terraform (AWS Credentials in CI/CD)

*(Gewählter Ansatz)*

#### 4.1.8 Spezifikation: Manuelles Proof-of-Concept Deployment

Vor der Automatisierung mit Helm wurde ein manuelles Deployment durchgeführt, um die Integration aller Infrastrukturkomponenten zu validieren. Dieser Abschnitt dokumentiert die durchgeführten Schritte und die Konfiguration der Kubernetes-Ressourcen und dient als Blaupause für die Entwicklung des Helm Charts.

##### A. Übersicht und Reihenfolge der Ressourcen

Für das Deployment wurden vier primäre Kubernetes-Ressourcen in einer spezifischen Reihenfolge erstellt:

1.  **Secret:** Zur sicheren Speicherung der Datenbank-Zugangsdaten und des initialen Nextcloud-Admin-Passworts.
2.  **PersistentVolumeClaim (PVC):** Zur Anforderung von persistentem Speicher.
3.  **Deployment:** Zur Definition und Verwaltung des Nextcloud-Anwendungs-Pods.
4.  **Service:** Zur Bereitstellung der Nextcloud-Anwendung über einen externen Load Balancer.

##### B. Detaillierte Konfiguration

Die folgenden Konfigurationen waren für ein erfolgreiches Deployment entscheidend:

**1. Secret (`nextcloud-db-secret`)**
Ein `Opaque`-Secret wurde mit den folgenden, base64-codierten Schlüsseln erstellt. Diese werden direkt von den Umgebungsvariablen des offiziellen Nextcloud Docker-Images verwendet.

*   `POSTGRES_HOST`: Der Endpunkt der AWS RDS-Instanz.
*   `POSTGRES_USER`: Der Master-Benutzername der RDS-Instanz.
*   `POSTGRES_PASSWORD`: Das Master-Passwort der RDS-Instanz.
*   `POSTGRES_DB`: Der Name der Datenbank innerhalb der RDS-Instanz.
*   `NEXTCLOUD_ADMIN_USER`: Der gewünschte Benutzername für den initialen Admin-Account.
*   `NEXTCLOUD_ADMIN_PASSWORD`: Das gewünschte Passwort für den initialen Admin-Account.

**2. PersistentVolumeClaim (`nextcloud-data-pvc`)**
Der PVC ist für die Datenpersistenz der stateful Nextcloud-Anwendung unerlässlich.

*   `accessModes`: `ReadWriteOnce`, wie für AWS EBS-Volumes erforderlich.
*   `storageClassName`: `ebs-sc`, um die benutzerdefinierte StorageClass zu verwenden, die verschlüsselte `gp3`-Volumes provisioniert.
*   `resources.requests.storage`: `10Gi` als initiale Grösse.

**3. Deployment (`nextcloud-deployment`)**
Das Deployment ist das Herzstück der Anwendung.

*   **Image:** `nextcloud:latest` (offizielles Docker Hub Image).
*   **Umgebungsvariablen:** Alle sechs oben genannten Secret-Schlüssel wurden über `valueFrom.secretKeyRef` sicher in den Container injiziert, um die Konfiguration vorzunehmen.
*   **Volume Mounts:** Ein Volume, das auf den `nextcloud-data-pvc` verweist, wurde in den Container am Pfad `/var/www/html` gemountet. Dies ist der von Nextcloud erwartete Pfad für alle Anwendungs-, Konfigurations- und Benutzerdateien.

**4. Service (`nextcloud-service`)**
Der Service macht die Anwendung von aussen erreichbar.

*   **Typ:** `LoadBalancer`. Dies weist den AWS Cloud Controller Manager an, automatisch einen AWS Network Load Balancer (NLB) zu provisionieren.
*   **Selector:** `app: nextcloud`, um den Traffic an die Pods des Deployments zu leiten.
*   **Ports:** Der Service leitet externen Traffic von Port `80` an den `targetPort` `80` des Containers weiter.

##### C. Referenz-Manifeste und Befehle

Die für diesen manuellen Test verwendeten YAML-Manifeste sind im Verzeichnis [`manual-k8s-manifests/`](./manual-k8s-manifests/) im Repository abgelegt. Die wesentlichen Befehle zur Überprüfung waren:

```bash
# Anwenden der Manifeste
kubectl apply -f manual-k8s-manifests/

# Überprüfen des Status der Ressourcen
kubectl get pvc,pods,svc -w

# Überprüfen der Pod-Logs auf Fehler
kubectl logs <pod-name> -f

# Durchführen des Persistenz-Tests
kubectl delete pod <pod-name>
```
Diese manuelle Verifizierung hat die Funktionsfähigkeit der gesamten darunterliegenden Infrastruktur bestätigt und den Weg für die Automatisierung mit Helm geebnet.


### 4.2 Nextcloud Helm Chart Entwicklung

Um das Deployment von Nextcloud zu standardisieren, zu versionieren und wiederholbar zu machen, wurde ein dediziertes Helm Chart entwickelt. Dies ersetzt die manuellen `kubectl apply`-Schritte aus dem Proof-of-Concept und bildet die Grundlage für die CI/CD-Pipeline. Das Chart befindet sich im Verzeichnis `charts/nextcloud-chart`.

#### 4.2.1 Helm Chart Struktur (`Chart.yaml`, `values.yaml`, `templates/`)

Die Struktur des Charts folgt den Helm-Best-Practices und wurde bewusst schlank gehalten:

*   **`Chart.yaml`**: Enthält die Metadaten des Charts wie Name, Beschreibung, Chart-Version (`version`) und die Version der Nextcloud-Anwendung (`appVersion`).
*   **`values.yaml`**: Die zentrale Konfigurationsdatei. Hier können Benutzer alle wichtigen Parameter des Deployments anpassen, ohne die Templates ändern zu müssen.
*   **`templates/`**: Dieses Verzeichnis enthält die Kubernetes-Manifest-Vorlagen.
    *   **`_helpers.tpl`**: Eine Hilfsdatei zur Generierung von standardisierten Namen und Labels, was die Konsistenz und Wartbarkeit erhöht.
    *   **`pvc.yaml`**: Template für den `PersistentVolumeClaim` zur Anforderung von Speicher.
    *   **`deployment.yaml`**: Template für das `Deployment`, das den Nextcloud-Pod verwaltet.
    *   **`service.yaml`**: Template für den `Service`, der die Anwendung im Netzwerk verfügbar macht (z.B. via Load Balancer).

#### 4.2.2 Wichtige Templates (Deployment, Service, PVC)

Die Templates wurden direkt von den validierten manuellen Manifesten aus Sprint 3 abgeleitet.

*   **`deployment.yaml`**: Das Herzstück. Es referenziert Werte aus `values.yaml` für die Image-Version, Replica-Anzahl und bindet die Datenbank-Credentials aus einem extern verwalteten Kubernetes-Secret ein.
    ```yaml
    # Snippet aus templates/deployment.yaml
    spec:
      replicas: {{ .Values.replicaCount }}
      template:
        spec:
          containers:
            - name: {{ .Chart.Name }}
              image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
              env:
                - name: POSTGRES_HOST
                  valueFrom:
                    secretKeyRef:
                      name: {{ .Values.existingSecretName }}
                      key: POSTGRES_HOST
              # ... weitere env-Variablen ...
              volumeMounts:
                - name: nextcloud-data
                  mountPath: /var/www/html
          volumes:
            - name: nextcloud-data
              persistentVolumeClaim:
                claimName: {{ include "nextcloud-chart.fullname" . }}-data
    ```
*   **`pvc.yaml`**: Definiert den `PersistentVolumeClaim`. Die Grösse und die `storageClassName` sind über `values.yaml` steuerbar.
*   **`service.yaml`**: Erstellt den `Service`, dessen Typ (`LoadBalancer`, `ClusterIP`, etc.) und Port in `values.yaml` definiert werden können.
*   **`secret.yaml`**: Dieses Template enthält eine `if/else`-Logik. Standardmässig erstellt es ein Kubernetes Secret, das die initialen Admin-Credentials und die Datenbank-Verbindungsdaten aus der `values.yaml` liest und base64-kodiert. Alternativ kann das Chart so konfiguriert werden, dass es ein extern erstelltes, bereits existierendes Secret verwendet, was in Produktionsumgebungen Best Practice ist.
*   **`configmap.yaml`**: Löst eines der häufigsten Probleme bei Nextcloud-Deployments auf Kubernetes. Es generiert eine `autoconfig.php` und mountet diese in den Pod. Diese Datei konfiguriert `trusted_domains` und `overwrite.cli.url` dynamisch basierend auf dem in `values.yaml` gesetzten Hostnamen. Dadurch wird sichergestellt, dass Nextcloud hinter dem Load Balancer korrekt funktioniert und keine Redirect-Fehler auftreten.

#### 4.2.3 Konfigurationsmöglichkeiten über `values.yaml`

Die `values.yaml` ermöglicht die flexible Anpassung des Deployments. Die wichtigsten Parameter sind:

```yaml
# Konfiguration für die Nextcloud-Anwendung
nextcloud:
  host: "a1b2c3d4.elb.amazonaws.com" # Muss mit dem Load Balancer Hostnamen befüllt werden
  admin:
    user: "admin"
    password: "SuperSecurePassword!" # Sollte via --set überschrieben werden

# Konfiguration der Datenbank-Verbindung
database:
  enabled: true # Auf true setzen, damit das Chart ein Secret erstellt
  existingSecret: "" # Leer lassen, wenn 'enabled' true ist
  user: "nextcloudadmin"
  password: "AnotherSuperSecurePassword!" # Sollte via --set überschrieben werden
  database: "nextclouddb"
  host: "nextcloud-db-instance.rds.amazonaws.com" # Muss mit dem RDS Endpunkt befüllt werden
```

Diese Struktur macht das Chart flexibel genug für verschiedene Umgebungen (Entwicklung, Produktion) und einfach in einer CI/CD-Pipeline zu verwenden.

#### 4.2.4 Sicherheitshinweis zur Passwortverwaltung

Das Helm Chart bietet die Möglichkeit, Passwörter für den Admin-Benutzer und die Datenbank direkt in der `values.yaml`-Datei zu definieren.

**WARNUNG:** Das Speichern von unverschlüsselten Passwörtern in `values.yaml`-Dateien, die in ein Git-Repository eingecheckt werden, ist eine **unsichere Praxis** und sollte in Produktionsumgebungen unbedingt vermieden werden.

Für dieses Projekt wird diese Methode zur Vereinfachung verwendet. Für eine produktive Nutzung werden folgende Ansätze empfohlen:
1.  **`--set` Flag bei der Installation:** Passwörter können zur Laufzeit übergeben werden: `helm install ... --set database.password=MEIN_GEHEIMES_PASSWORT`.
2.  **Externe Secrets-Verwaltung:** Die sicherste Methode ist, das Secret manuell oder über einen anderen Prozess (z.B. mit dem AWS Secrets Manager & CSI Driver) zu erstellen und dem Chart via `database.existingSecret` nur den Namen des Secrets zu übergeben.

#### 4.2.5 Post-Installation Notes (NOTES.txt)

Um dem Benutzer des Charts eine bestmögliche Erfahrung zu bieten, enthält das Chart eine `templates/NOTES.txt`-Datei. Diese wird nach einer erfolgreichen `helm install` oder `helm upgrade` Operation in der Konsole angezeigt und gibt kontext-spezifische Anweisungen.

Die Notizen sind dynamisch und passen sich der Konfiguration in `values.yaml` an. Für den Standardfall mit einem `Service` vom Typ `LoadBalancer` sieht die Ausgabe beispielsweise wie folgt aus:

<pre>
Nextcloud has been deployed. Congratulations!
Your release is named: nextcloud

**IMPORTANT NEXT STEPS:**

1. **Get the Nextcloud URL:**

   The Load Balancer is being created... Run the following command to get the EXTERNAL-IP:

   kubectl get svc --namespace default -w nextcloud

2. **Update Nextcloud Host Configuration:**

   Once you have the external hostname..., you MUST upgrade the Helm release...

   helm upgrade --namespace default nextcloud . \
     --set nextcloud.host=YOUR_LOADBALANCER_HOSTNAME

   ...

**Admin Credentials:**
   Username: admin
   Password: SuperSecurePassword!
</pre>

Dies führt den Benutzer durch die kritischen ersten Schritte, insbesondere das Abrufen des dynamisch erstellten Load-Balancer-Hostnamens und das anschliessende `helm upgrade`, um die Nextcloud-Konfiguration zu finalisieren.

### 4.3 CI/CD Pipeline mit GitHub Actions

*Die Automatisierung des Deployments.*

#### 4.3.1 Workflow-Definition und Integrationsschritte

Die gesamte CI/CD-Logik ist in der Datei `.github/workflows/deploy.yml` definiert. Der Workflow wird bei jedem Push auf den `main`-Branch ausgelöst und führt die folgenden, logisch getrennten Jobs aus:

1.  **Checkout & Authentifizierung:** Der Code wird ausgecheckt und die Pipeline authentifiziert sich sicher via OIDC bei AWS (siehe 4.3.2). Danach werden `kubectl` und `helm` für den Zugriff auf den EKS-Cluster konfiguriert.

2.  **Validierung (Linting):** Vor jedem Deployment wird `helm lint` auf dem Chart ausgeführt. Dies stellt sicher, dass keine syntaktischen Fehler oder grobe Best-Practice-Verletzungen in das Repository gemerged wurden. Ein Fehlschlag in diesem Schritt bricht die Pipeline sofort ab.

3.  **Deployment (Helm Upgrade):** Der Kernschritt verwendet `helm upgrade --install`. Dieser Befehl ist idempotent: Er installiert das Chart, falls es noch nicht existiert, oder aktualisiert ein bestehendes Release, falls Änderungen vorliegen.
    *   Der `nextcloud`-Namespace wird bei Bedarf mit `--create-namespace` erstellt.
    *   Die `--wait`-Flag sorgt dafür, dass die Pipeline wartet, bis die Kubernetes-Ressourcen (insbesondere der Nextcloud-Pod) einen "Ready"-Status erreichen.

4.  **Konfigurations-Finalisierung (Load Balancer Hostname):** Um das Problem zu lösen, dass der externe Hostname des Load Balancers erst nach dem Deployment bekannt ist, enthält der Workflow ein Shell-Skript. Dieses Skript fragt in einer Schleife den `Service`-Status ab, bis der Hostname verfügbar ist, und führt dann ein zweites, schnelles `helm upgrade` aus, das nur den `nextcloud.host`-Wert setzt. Dies finalisiert die Konfiguration und stellt die korrekte Funktionsweise von Nextcloud sicher.

5.  **Verifizierung (Helm Test):** Nach dem erfolgreichen Deployment führt die Pipeline `helm test` aus. Dies triggert den im Chart definierten Test-Pod, der die Erreichbarkeit des `/status.php`-Endpunkts überprüft. Schlägt dieser Test fehl, wird die gesamte Pipeline als fehlgeschlagen markiert.

#### 4.3.2 Authentifizierung gegenüber AWS (OIDC)

Um eine sichere und passwortlose Authentifizierung der CI/CD-Pipeline gegenüber AWS zu gewährleisten, wird der von AWS und GitHub empfohlene Standard **OpenID Connect (OIDC)** verwendet. Dies eliminiert die Notwendigkeit, langlebige AWS Access Keys als GitHub Secrets zu speichern.

Der Prozess wurde via Terraform in der Datei `terraform/iam_cicd.tf` implementiert und besteht aus zwei Hauptkomponenten:

1.  **IAM OIDC Identity Provider:** Eine einmalige Konfiguration in AWS, die eine Vertrauensbeziehung zum OIDC-Provider von GitHub (`token.actions.githubusercontent.com`) herstellt.
2.  **IAM-Rolle für die Pipeline:** Eine dedizierte Rolle (`Nextcloud-cicd-role`) wird erstellt. Die entscheidende Konfiguration ist die **Trust Policy**:
    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {
                    "Federated": "arn:aws:iam::ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com"
                },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                    "StringLike": {
                        "token.actions.githubusercontent.com:sub": "repo:Stevic-Nenad/Nextcloud:ref:refs/heads/main"
                    }
                }
            }
        ]
    }
    ```
    Die `Condition` stellt sicher, dass nur Workflows, die aus dem Repository `Stevic-Nenad/Nextcloud` vom `main`-Branch stammen, diese Rolle übernehmen dürfen.

Im GitHub Actions Workflow wird dann die Action `aws-actions/configure-aws-credentials` verwendet. Diese fordert bei GitHub ein OIDC JWT-Token an, präsentiert es AWS, übernimmt die konfigurierte Rolle und stellt temporäre AWS-Credentials für alle nachfolgenden Schritte im Job zur Verfügung.

#### 4.3.3 Integrationsschritte (Terraform, Helm)

#### 4.3.4 Secrets Management in der Pipeline

In der CI/CD-Pipeline wird strikt darauf geachtet, keine sensiblen Daten im Klartext zu speichern. Die Umsetzung erfolgt auf zwei Ebenen:

1.  **Authentifizierung (Keine Keys):** Die Authentifizierung bei AWS erfolgt ausschliesslich über OIDC, wie in Abschnitt 4.3.2 beschrieben. Es werden keine langlebigen `AWS_ACCESS_KEY_ID` oder `AWS_SECRET_ACCESS_KEY` benötigt.
2.  **Konfigurations-Secrets (GitHub Secrets):** Werte, die für die Konfiguration des Helm Charts benötigt werden (z.B. das RDS-Passwort), werden als "Repository Secrets" in den GitHub-Einstellungen hinterlegt.
    *   Im Workflow werden diese Secrets über die `${{ secrets.SECRET_NAME }}`-Syntax ausgelesen.
    *   Sie werden direkt und sicher an den `helm`-Befehl übergeben, z.B. `--set database.password="${{ secrets.RDS_DB_PASSWORD }}"`.
    *   Dadurch erscheinen die Werte niemals im Klartext im Workflow-Code und werden in den GitHub Actions Logs automatisch durch `***` maskiert.

### 4.4 Installation und Inbetriebnahme der Gesamtlösung

Diese Anleitung beschreibt, wie das gesamte Projekt von Grund auf neu aufgesetzt werden kann. Sie ist für einen Benutzer mit den entsprechenden Berechtigungen in AWS und GitHub konzipiert. Der empfohlene Weg zur Inbetriebnahme ist die Nutzung des automatisierten Lifecycle-Workflows.

#### 4.4.1 Voraussetzungen

**Erforderliche Tools:**

* **AWS CLI v2.27.7** - Für AWS Service Interaktionen
  ```bash
  aws --version  # AWS CLI 2.x.x
  ```
* **Terraform >= 1.12.1** - Infrastructure as Code Tool
  ```bash
  terraform version  # Terraform v1.9.x
  ```
* **kubectl >= 1.33.0** - Kubernetes CLI (kompatibel mit EKS 1.30)
  ```bash
  kubectl version --client  # v1.30.x
  ```
* **Helm >= 3.18.1** - Kubernetes Package Manager
  ```bash
  helm version  # version.BuildInfo{Version:"v3.15.x"}
  ```
* **Git** - Version Control

**Terraform Backend Infrastruktur:**

* Ein AWS S3 Bucket für Terraform State und eine AWS DynamoDB-Tabelle für State Locking müssen bereits provisioniert
  sein. Dies geschieht typischerweise durch eine separate Terraform-Konfiguration (siehe
  Abschnitt [4.1.0](#410-terraform-remote-state-backend-konfiguration)). Notieren Sie sich die Namen des S3 Buckets und
  der DynamoDB-Tabelle.

**AWS Konfiguration:**

* AWS Account mit aktiviertem MFA
* IAM User mit programmatischem Zugriff
* Konfiguriertes AWS CLI Profile:
  ```bash
  aws configure --profile nextcloud-project
  ```

**Empfohlene IDE:**

* IntelliJ IDEA Ultimate mit Plugins:
    - HashiCorp Terraform/HCL Language Support
    - Kubernetes
    - Docker
    - AWS Toolkit
    - YAML (eingebaut)

#### 4.4.2 Klonen des Repositorys

Klonen Sie das Projekt-Repository auf Ihre lokale Maschine:
```bash
git clone https://github.com/Stevic-Nenad/Nextcloud.git
cd Nextcloud
```

#### 4.4.3 Konfiguration von Umgebungsvariablen/Secrets

Die Automatisierung benötigt einige Konfigurationswerte und Secrets. Diese werden an zwei Stellen sicher hinterlegt.

**Schritt A: Terraform Backend-Infrastruktur erstellen (Einmalig)**

Das Terraform-Setup dieses Projekts verwendet ein S3-Backend zur zentralen Speicherung des Infrastruktur-Zustands. Die hierfür benötigten Ressourcen (S3 Bucket, DynamoDB-Tabelle) müssen einmalig erstellt werden.

1.  Navigieren Sie in das `backend/`-Verzeichnis:
    ```bash
    cd backend
    ```
2.  Initialisieren und erstellen Sie die Backend-Infrastruktur:
    ```bash
    terraform init
    terraform apply --auto-approve
    ```
3.  Navigieren Sie zurück in das Hauptverzeichnis:
    ```bash
    cd ..
    ```

**Schritt B: GitHub Repository Secrets konfigurieren**

Die CI/CD-Workflows benötigen Zugriff auf sensible Informationen. Diese werden als "Secrets" direkt in den GitHub-Einstellungen des Repositorys gespeichert.

1.  Navigieren Sie in Ihrem GitHub-Repository zu `Settings` > `Secrets and variables` > `Actions`.
2.  Klicken Sie auf `New repository secret` und erstellen Sie die folgenden Einträge:

| Secret Name                | Wert                                                                                             | Beschreibung                                                              |
| -------------------------- | ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- |
| `AWS_ACCOUNT_ID`           | Ihre 12-stellige AWS-Konto-ID.                                                                   | Wird für die ARN der OIDC-Rolle benötigt.                                 |
| `AWS_REGION`               | `eu-central-1`                                                                                   | Die AWS-Region, in der die Infrastruktur erstellt wird.                   |
| `CICD_ROLE_NAME`           | `Nextcloud-cicd-role`                                                                            | Der Name der IAM-Rolle, die von der Pipeline übernommen wird.             |
| `RDS_DB_PASSWORD`          | `<Ihr-sicheres-Passwort>`                                                                        | Das Passwort für den Master-Benutzer der RDS-Datenbank.                   |
| `NEXTCLOUD_ADMIN_PASSWORD` | `<Ihr-sicheres-Passwort>`                                                                        | Das gewünschte Passwort für den initialen `admin`-Benutzer in Nextcloud. |

#### 4.4.4 Ausführen der Lifecycle-Workflows

Nachdem die Voraussetzungen und Secrets konfiguriert sind, kann die gesamte Umgebung mit einem einzigen Klick auf- oder abgebaut werden.

**Option 1: Full Setup (Infrastruktur erstellen & Anwendung deployen)**

1.  Navigieren Sie in Ihrem GitHub-Repository zum Tab **Actions**.
2.  Wählen Sie links den Workflow **"Full Environment Lifecycle"** aus.
3.  Klicken Sie auf den Button **"Run workflow"**.
4.  Wählen Sie im Dropdown-Menü die Option **`setup`**.
5.  Klicken Sie auf den grünen **"Run workflow"**-Button.

Der Workflow startet nun. Sie können den Fortschritt live mitverfolgen. Zuerst wird der `Terraform Apply`-Job die AWS-Infrastruktur erstellen (ca. 15-20 Minuten). Danach wird der `Deploy Application`-Job die Nextcloud-Anwendung auf dem neuen Cluster installieren und konfigurieren (ca. 5-10 Minuten).

**Option 2: Full Teardown (Anwendung deinstallieren & Infrastruktur zerstören)**

1.  Folgen Sie den Schritten 1-3 von oben.
2.  Wählen Sie im Dropdown-Menü die Option **`destroy`**.
3.  Klicken Sie auf den grünen **"Run workflow"**-Button.

Der Workflow wird zuerst die Nextcloud-Anwendung deinstallieren und danach die gesamte AWS-Infrastruktur mit `terraform destroy` sicher entfernen. Dies ist der empfohlene Weg, um die Umgebung nach der Nutzung zu bereinigen und Kosten zu vermeiden.

#### 4.4.5 Zugriff auf die Nextcloud Instanz

Nachdem der `setup`-Workflow erfolgreich abgeschlossen wurde:

1.  **URL finden:**
    *   Navigieren Sie zur abgeschlossenen Workflow-Ausführung in GitHub Actions.
    *   Klicken Sie auf den Job `Deploy Application`.
    *   Erweitern Sie den Schritt `Get Load Balancer Hostname and Finalize Configuration`.
    *   Im Log finden Sie eine Zeile wie: `Load Balancer found: a1b2c3d4e5f6.eu-central-1.elb.amazonaws.com`.
    *   Dies ist die URL Ihrer Nextcloud-Instanz.

2.  **Login:**
    *   Öffnen Sie die URL in Ihrem Browser.
    *   Der Benutzername ist standardmässig `admin`.
    *   Das Passwort ist der Wert, den Sie im GitHub Secret `NEXTCLOUD_ADMIN_PASSWORD` hinterlegt haben.

### 4.5 Anpassung von Software / Konfiguration von Geräten

*Spezifische Konfigurationen, die über Standard-Deployments hinausgehen.*

#### 4.5.1 Nextcloud-spezifische Konfigurationen (via Helm)

#### 4.5.2 Wichtige AWS Service-Konfigurationen

---

## 5. Testing und Qualitätssicherung

*Wie wurde sichergestellt, dass die Lösung funktioniert und den Anforderungen entspricht?*

### 5.1 Teststrategie

*Welche Testebenen und -arten wurden angewendet?*

#### 5.1.1 Statische Code-Analyse (Linting)

#### 5.1.2 Validierung der Infrastruktur-Konfiguration (`terraform validate/plan`, `helm template`)

#### 5.1.3 Manuelle Funktionstests der Nextcloud Instanz

#### 5.1.4 End-to-End Tests der CI/CD Pipeline

#### 5.1.5 Automatisierte Post-Deployment-Tests (Helm Test)

Um nach einer Installation oder einem Upgrade schnell zu verifizieren, dass die Anwendung nicht nur deployt wurde, sondern auch tatsächlich läuft und antwortet, wird die `helm test`-Funktionalität genutzt.

Im Verzeichnis `templates/tests/` des Charts wurde ein Test-Pod definiert. Dieser Pod startet nach dem Deployment auf Befehl (`helm test <release-name>`), versucht eine Verbindung zum `/status.php`-Endpunkt des Nextcloud-Services innerhalb des Clusters aufzubauen und beendet sich bei Erfolg mit einem positiven Status. Dies dient als einfacher, aber effektiver "Smoke-Test", um die grundlegende Funktionsfähigkeit der Anwendung automatisiert zu bestätigen.

### 5.2 Testfälle und Protokolle

**Testfall: Überprüfung der Kosten-Tags auf AWS Ressourcen (Vorbereitung)**

* **Zugehörige User Story (Setup):** `Nextcloud#7` - Kosten-Tags für AWS Ressourcen definieren und initiale
  Terraform-Provider-Konfiguration erstellen.
* **Zugehörige User Story (Verifizierung auf Ressourcen):** `Nextcloud#5` - VPC mit Subnetzen via Terraform erstellen.
* **Status:** Abgeschlossen (als Teil der Verifizierung von `Nextcloud#5`).
* **Zielsetzung (für `Nextcloud#5`):** Verifizieren, dass die im Rahmen von `Nextcloud#7` konfigurierten Standard-Tags (
  `Projekt`, `Student`, `ManagedBy`) korrekt auf den via Terraform erstellten Ressourcen (initial VPC, Subnetze) in der
  AWS Management Console angezeigt werden.
* **Testschritte (für `Nextcloud#5`):**
    1. Nach erfolgreichem `terraform apply` (für `Nextcloud#5`) in der AWS Management Console zur VPC-Übersicht
       navigieren.
    2. Die für das Projekt erstellte VPC auswählen und den Tab "Tags" prüfen.
    3. Eines der für das Projekt erstellten Subnetze auswählen und den Tab "Tags" prüfen.
* **Erwartetes Ergebnis (für `Nextcloud#5`):** Alle in `local.common_tags` definierten Standard-Tags sind mit den
  korrekten Werten auf den überprüften Ressourcen vorhanden.
* **Tatsächliches Ergebnis:** Die Standard-Tags (`Projekt: Nextcloud`, `Student: NenadStevic`, `ManagedBy: Terraform`)
  wurden erfolgreich auf der erstellten VPC, den öffentlichen Subnetzen, privaten Subnetzen, dem IGW, den EIPs für NAT
  Gateways, den NAT Gateways und allen Routing-Tabellen in der AWS Management Console verifiziert.
* **Nachweis:** Beispiel-Screenshots der AWS Management Console, die die Tags auf der VPC und einem NAT Gateway zeigen,
  sind in Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `vpc_tags_verification.png` (oder
  ähnlich) abgelegt.

---

**Testfall: VPC-Grundfunktionalität und Hochverfügbarkeit der NAT-Gateways**

* **Zugehörige User Story:** `Nextcloud#5` - VPC mit Subnetzen via Terraform erstellen
* **Status:** Abgeschlossen
* **Zielsetzung:** Verifizieren, dass die VPC-Komponenten korrekt erstellt wurden und die NAT-Gateway-Architektur (ein
  NAT GW pro AZ) wie erwartet funktioniert.
* **Testschritte (Manuelle Überprüfung in AWS Konsole nach `terraform apply`):**
    1. **VPC-Erstellung:**
        * Überprüfen, ob die VPC (`${var.project_name}-vpc`) mit dem korrekten CIDR-Block (`var.vpc_cidr_block`)
          existiert.
        * Überprüfen, ob DNS-Hostnamen und DNS-Support für die VPC aktiviert sind.
    2. **Subnetz-Erstellung:**
        * Überprüfen, ob die erwartete Anzahl öffentlicher Subnetze (gemäss `length(var.public_subnet_cidrs)`) in den
          korrekten AZs (`var.availability_zones`) und mit den korrekten CIDRs (`var.public_subnet_cidrs`) erstellt
          wurden.
        * Überprüfen, ob `map_public_ip_on_launch` für öffentliche Subnetze auf `true` steht.
        * Überprüfen, ob die erwartete Anzahl privater Subnetze (gemäss `length(var.private_subnet_cidrs)`) in den
          korrekten AZs und mit den korrekten CIDRs erstellt wurden.
    3. **Internet Gateway (IGW):**
        * Überprüfen, ob ein IGW (`${var.project_name}-igw`) erstellt und an die VPC angehängt ist.
    4. **NAT Gateways (pro AZ):**
        * Überprüfen, ob für jede AZ in `var.availability_zones` ein NAT Gateway (
          `${var.project_name}-nat-gw-[AZ-Name]`) im öffentlichen Subnetz dieser AZ existiert und den Status "Available"
          hat.
        * Überprüfen, ob jedes NAT Gateway eine zugehörige Elastic IP (`${var.project_name}-nat-eip-[AZ-Name]`) hat.
    5. **Routing - Öffentliche Subnetze:**
        * Überprüfen, ob die öffentliche Routing-Tabelle (`${var.project_name}-public-rt`) eine Default-Route (
          `0.0.0.0/0`) zum IGW hat.
        * Überprüfen, ob alle öffentlichen Subnetze mit dieser öffentlichen Routing-Tabelle assoziiert sind.
    6. **Routing - Private Subnetze (pro AZ):**
        * Für jede AZ: Überprüfen, ob eine private Routing-Tabelle (`${var.project_name}-private-rt-[AZ-Name]`)
          existiert.
        * Für jede dieser privaten Routing-Tabellen: Überprüfen, ob eine Default-Route (`0.0.0.0/0`) zum NAT Gateway in
          derselben AZ existiert.
        * Für jede AZ: Überprüfen, ob die privaten Subnetze dieser AZ mit der korrekten AZ-spezifischen privaten
          Routing-Tabelle assoziiert sind.
    7. **Terraform Outputs:**
        * Überprüfen, ob `terraform output` die korrekten Werte für `vpc_id`, `public_subnet_ids`, `private_subnet_ids`,
          `nat_gateway_public_ips`, etc. anzeigt.
* **Erwartetes Ergebnis:** Alle oben genannten Komponenten sind korrekt konfiguriert und die Routen sind wie beschrieben
  eingerichtet. Die NAT-Gateway-Architektur ist pro AZ implementiert.
* **Tatsächliches Ergebnis:** Alle Komponenten wurden in der AWS Konsole und via Terraform Outputs wie erwartet
  verifiziert. Die NAT-Gateway-pro-AZ-Architektur und das entsprechende Routing sind korrekt implementiert.
* **Nachweis:** Screenshots der AWS Konsole (z.B. VPC Details, Subnetz-Liste, IGW-Status, NAT Gateway Liste,
  Routing-Tabellen-Konfigurationen) können bei Bedarf in
  Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `vpc_functionality_verification.png` (oder
  ähnlich) abgelegt werden. *Für dieses Projekt wird auf detaillierte Screenshots für jeden Schritt verzichtet, sofern
  die `terraform apply` erfolgreich war und die Kernkomponenten stichprobenartig überprüft wurden.*

---

**Testfall: Terraform Remote Backend Initialisierung und Nutzung (Hauptanwendung)**

* **Zugehörige User Story:** `Nextcloud#6` - Terraform Remote Backend konfigurieren (für die Hauptanwendung
  `src/terraform/`)
* **Voraussetzung:** Der S3 Bucket und die DynamoDB-Tabelle für das Backend wurden bereits durch eine separate
  Terraform-Konfiguration (z.B. `terraform-backend-setup/`) provisioniert und ihre Namen sind bekannt.
* **Status:** `Abgeschlossen` / `Teilweise Abgeschlossen (wegen AWS-Problemen)` *(Anpassen!)*
* **Zielsetzung:** Verifizieren, dass die Hauptanwendungs-Terraform-Konfiguration (`src/terraform/`) korrekt mit dem
  extern provisionierten S3 Remote Backend initialisiert wird, den State dort liest/schreibt und Locks über DynamoDB
  funktionieren.
* **Testschritte:**
    1. Sicherstellen, dass die `src/terraform/backend.tf`-Datei korrekt mit den Namen des externen S3 Buckets und der
       DynamoDB-Tabelle konfiguriert ist.
    2. Im Verzeichnis `src/terraform/` den Befehl `terraform init` ausführen.
    3. Überprüfen, ob `terraform init` erfolgreich durchläuft und die Verbindung zum S3-Backend bestätigt.
    4. Eine kleine, harmlose Änderung an der *Anwendungs*-Infrastruktur (z.B. ein Tag zu einer Ressource in
       `src/terraform/network.tf` hinzufügen, nicht die Backend-Konfiguration selbst) vornehmen.
    5. `terraform plan` ausführen. Überprüfen, ob der Plan korrekt erstellt wird und einen Lock-Versuch in DynamoDB
       andeutet (schwer manuell zu sehen, aber der Plan sollte ohne Lock-Fehler erstellt werden).
    6. `terraform apply` ausführen, um die Änderung anzuwenden.
    7. Überprüfen, ob die `terraform.tfstate`-Datei im S3 Bucket unter dem konfigurierten `key` (z.B.
       `nextcloud-app/main.tfstate`) aktualisiert wurde (Zeitstempel, Version).
* **Erwartetes Ergebnis:**
    * `terraform init` in `src/terraform/` läuft fehlerfrei durch und initialisiert das S3-Backend.
    * Die State-Datei für die Hauptanwendung wird im S3-Bucket unter dem korrekten Key verwaltet/aktualisiert.
    * `terraform plan/apply` für die Hauptanwendungsinfrastruktur funktionieren und verwenden den Remote State.
* **Tatsächliches Ergebnis:** `HIER DAS TATSÄCHLICHE ERGEBNIS EINTRAGEN.` *(Wichtig: Wenn AWS-Probleme die Verifizierung
  von `init` oder `apply` für `src/terraform/` verhindert haben, muss das hier klar und ehrlich dokumentiert werden!)*
* **Nachweis:**
    * Screenshot der `src/terraform/backend.tf` Konfiguration.
    * Screenshot des S3 Buckets, der die State-Datei unter dem Key `nextcloud-app/main.tfstate` zeigt.
    * Konsolenausgabe von `terraform init` (aus `src/terraform/`).

---

**Testfall: Provisionierung des EKS Clusters und der Node Groups**

* **Zugehörige User Story:** `Nextcloud#8` - EKS Cluster mit Node Groups provisionieren (via Terraform)
* **Status:** Abgeschlossen
* **Zielsetzung:** Verifizieren, dass der EKS Cluster und mindestens eine Managed Node Group erfolgreich via Terraform
  provisioniert werden, die Nodes im 'Ready'-Status sind und die Konfiguration den Akzeptanzkriterien entspricht.
* **Testschritte:**
    1. Terraform-Konfiguration für EKS Cluster, IAM-Rollen und Managed Node Group erstellen/erweitern.
    2. `terraform validate` ausführen.
    3. `terraform plan` ausführen und den Plan überprüfen.
    4. `terraform apply -auto-approve` ausführen.
    5. Nach erfolgreichem `apply`, `kubeconfig` aktualisieren mit:
       `aws eks update-kubeconfig --region <region> --name <cluster_name> --profile <aws_profile>`
    6. `kubectl get nodes -o wide` ausführen.
    7. In der AWS Management Console überprüfen:
        * EKS Cluster Status (Aktiv, korrekte K8s Version).
        * IAM Rollen für Cluster und Nodes mit den korrekten Policies.
        * Managed Node Group Status (Aktiv, korrekte Instanztypen, Skalierung).
        * EC2-Instanzen der Worker Nodes: Sicherstellen, dass sie in den **privaten Subnetzen** laufen (überprüfen der
          Subnet-ID der Instanzen).
* **Erwartetes Ergebnis:**
    * `terraform apply` läuft erfolgreich durch.
    * `kubectl get nodes` zeigt die konfigurierte Anzahl an Worker Nodes im 'Ready'-Status. Die IP-Adressen der Nodes
      sollten aus den privaten Subnetz-CIDRs stammen.
    * Alle in den Akzeptanzkriterien genannten Punkte sind in der AWS Konsole verifizierbar (IAM Rollen korrekt, Node
      Group Parameter, K8s Version).
* **Tatsächliches Ergebnis:** `terraform apply` war erfolgreich nach Behebung eines `MalformedPolicyDocument`-Fehlers
  bei der `eks_cluster_role` und einer Syntaxkorrektur bei der `eks_node_role`. `kubectl get nodes -o wide` zeigte **2**
  Worker Nodes im 'Ready'-Status mit privaten IP-Adressen an. Alle Konfigurationen (IAM-Rollen, Kubernetes-Version
  `1.33`, Node Group Parameter, Platzierung der Nodes in privaten Subnetzen) wurden in der AWS Konsole wie erwartet
  verifiziert.*
* **Nachweis:** Screenshot von `kubectl get nodes` in
  Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `eks_nodes_ready.png` (oder ähnlich).

---

**Testfall: ECR Repository Provisionierung und Konfiguration**

* **Zugehörige User Story:** `Nextcloud#9` - ECR Repository via Terraform erstellen
* **Status:** Abgeschlossen
* **Zielsetzung:** Verifizieren, dass das ECR Repository mit den korrekten Konfigurationen (Name, Image Scanning, Lifecycle Policy) erstellt wurde und die Terraform Outputs funktionieren.
* **Testschritte:**
    1. Nach erfolgreichem `terraform apply` die Terraform Outputs überprüfen:
       ```bash
       terraform output ecr_repository_url
       terraform output ecr_repository_name
       terraform output ecr_repository_arn
       ```
    2. In der AWS Management Console zu ECR navigieren.
    3. Überprüfen, ob das Repository mit dem Namen `${var.project_name}-app` (z.B. "Nextcloud-app") existiert.
    4. Repository-Details öffnen und folgende Konfigurationen verifizieren:
        * Image scanning: "Scan on push" ist aktiviert
        * Lifecycle policy: Policy ist attached und enthält Regeln für ungetaggte Images (30 Tage) und getaggte Images (max 10)
        * Tag mutability: "MUTABLE" ist gesetzt
    5. Tags auf dem Repository überprüfen (sollten `local.common_tags` enthalten).
    6. Optional: Test-Image Push vorbereiten (Docker login zum Repository testen):
       ```bash
       aws ecr get-login-password --region eu-central-1 --profile nextcloud-project | docker login --username AWS --password-stdin $(terraform output -raw ecr_repository_url)
       ```
* **Erwartetes Ergebnis:**
    * Terraform Outputs liefern korrekte ECR Repository URL, Name und ARN.
    * Repository existiert in AWS Console mit korrektem Namen.
    * Image scanning ist aktiviert.
    * Lifecycle policy ist konfiguriert mit den definierten Regeln.
    * Standard-Tags sind korrekt gesetzt.
    * Docker login zum Repository ist erfolgreich (optional).
* **Tatsächliches Ergebnis:** `[Hier dein Ergebnis nach dem Test eintragen]`
* **Nachweis:** Screenshot der ECR Repository Details aus der AWS Console (optional) in Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `ecr_repository_verification.png`.

---

**Testfall: Konfiguration des IAM OIDC Providers und der IRSA-Rolle**

* **Zugehörige User Story:** `Nextcloud#10` - IAM OIDC Provider für EKS konfigurieren
* **Status:** Abgeschlossen
* **Zielsetzung:** Verifizieren, dass der OIDC Provider in AWS IAM korrekt erstellt wurde und dass die Beispiel-IAM-Rolle für den EBS CSI Driver die richtige Trust Policy hat.
* **Testschritte:**
    1. Nach erfolgreichem `terraform apply` in der AWS Management Console zu **IAM -> Identity providers** navigieren.
    2. Überprüfen, ob ein Provider mit der URL des EKS-Clusters existiert (z.B. `oidc.eks.eu-central-1.amazonaws.com/id/...`).
    3. Den Provider auswählen und sicherstellen, dass unter "Audience" der Wert `sts.amazonaws.com` eingetragen ist.
    4. Zu **IAM -> Roles** navigieren.
    5. Nach der Rolle `${var.project_name}-ebs-csi-driver-role` suchen und sie auswählen.
    6. Den Tab **"Trust relationships"** auswählen und auf "Edit trust policy" klicken.
    7. Überprüfen, ob die JSON-Richtlinie exakt der in Terraform definierten entspricht, insbesondere der `Principal.Federated` ARN und die `Condition`, die auf den Service Account `system:serviceaccount:kube-system:ebs-csi-controller-sa` verweist.
    8. Im Tab **"Permissions"** überprüfen, ob die `AmazonEBSCSIDriverPolicy` angehängt ist.
* **Erwartetes Ergebnis:** Alle Komponenten (OIDC Provider, IAM Rolle, Trust Policy, Permissions) sind in der AWS Konsole exakt so konfiguriert, wie in Terraform definiert. Die Trust Policy ist das kritischste Element und muss korrekt sein.
* **Tatsächliches Ergebnis:** Alle Überprüfungsschritte waren erfolgreich. Der OIDC Provider wurde korrekt erstellt und die Trust Policy der IAM-Rolle wurde exakt wie erwartet in der AWS Konsole verifiziert.
* **Nachweis:** Screenshot der Trust Policy der IAM-Rolle in Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `irsa_trust_policy_verification.png` (Platzhalter).

---

**Testfall: Verifizierung des EBS CSI Drivers und der dynamischen Provisionierung**

* **Zugehörige User Story:** `Nextcloud#11` - AWS EBS CSI Driver im EKS Cluster installieren und konfigurieren
* **Status:** Abgeschlossen
* **Zielsetzung:** Sicherstellen, dass der EBS CSI Driver korrekt installiert ist und in der Lage ist, auf eine PVC-Anfrage hin dynamisch ein EBS Volume zu provisionieren.
* **Testschritte:**
    1. Nach erfolgreichem `terraform apply` (welches das EKS-Add-on installiert), die Test-Manifeste anwenden:
       `kubectl apply -f kubernetes-manifests/01-storage-class.yaml`
       `kubectl apply -f kubernetes-manifests/02-test-pvc.yaml`
    2. Den Status des PVC überprüfen. Er sollte nach kurzer Zeit von `Pending` auf `Bound` wechseln.
       `kubectl get pvc ebs-test-claim`
    3. Überprüfen, ob ein entsprechendes PersistentVolume (PV) erstellt wurde und ebenfalls den Status `Bound` hat.
       `kubectl get pv`
    4. In der AWS Management Console zu **EC2 -> Elastic Block Store -> Volumes** navigieren.
    5. Überprüfen, ob ein neues EBS Volume (Grösse 4 GiB, Typ gp3) mit dem Status `in-use` erstellt wurde. Die Tags des Volumes sollten den Namen des PVs enthalten.
    6. (Aufräumen) Die Test-Ressourcen wieder löschen:
       `kubectl delete -f kubernetes-manifests/02-test-pvc.yaml`
       `kubectl delete -f kubernetes-manifests/01-storage-class.yaml`
       *Nach dem Löschen des PVCs sollte das PV und das zugrunde liegende EBS Volume automatisch gelöscht werden.*
* **Erwartetes Ergebnis:** Ein PVC wird erstellt und erfolgreich an ein dynamisch provisioniertes EBS Volume gebunden. Der Status von PVC und PV ist `Bound`.
* **Tatsächliches Ergebnis:** Alle Schritte wurden erfolgreich ausgeführt. Der PVC wechselte innerhalb von Sekunden zu `Bound` und ein entsprechendes 4-GiB-EBS-Volume wurde in der AWS-Konsole verifiziert.
* **Nachweis:** Screenshot der `kubectl get pvc,pv` Ausgabe in Abschnitt [5.2.1](#521-nachweise-der-testergebnisse-screenshotsgifs) unter `ebs_pvc_bound_verification.png` (Platzhalter).

---

**Testfall: Provisionierung und Netzwerkkonnektivität der RDS-Instanz**

*   **Zugehörige User Story:** `Nextcloud#12`, `Nextcloud#13`
*   **Status:** *(Nach dem Apply ausfüllen)*
*   **Zielsetzung:** Verifizieren, dass die RDS-Instanz korrekt in den privaten Subnetzen erstellt wird, die Security Group den Zugriff nur vom EKS-Cluster erlaubt und die Verbindungsdaten als Outputs verfügbar sind.
*   **Testschritte:**
    1.  Nach erfolgreichem `terraform apply` in der AWS Management Console zu **RDS -> Databases** navigieren.
    2.  Überprüfen, ob die Instanz `${var.project_name}-db-instance` mit Status "Available" existiert.
    3.  Die Instanzdetails öffnen und unter **"Connectivity & security"** überprüfen:
        *   Die VPC ist die korrekte Projekt-VPC.
        *   Die Subnetze in der Subnet Group sind die privaten Subnetze des Projekts.
        *   "Public access" ist auf "No" gesetzt.
        *   Die angehängte Security Group (`${var.project_name}-rds-sg`) ist korrekt.
    4.  Zur **VPC -> Security Groups** navigieren und die RDS-Security-Group auswählen.
    5.  Die "Inbound rules" überprüfen. Es sollte eine Regel für Port `5432` existieren, deren Quelle die Security Group ID des EKS-Clusters ist.
    6.  `terraform output` in der Konsole ausführen und überprüfen, ob die Outputs `rds_instance_endpoint`, `rds_instance_port`, `rds_db_name` und `rds_master_username` korrekte Werte anzeigen.
*   **Erwartetes Ergebnis:** Alle Komponenten sind wie beschrieben konfiguriert. Die Datenbank ist sicher im privaten Netzwerk platziert und nur für den EKS-Cluster erreichbar.
*   **Tatsächliches Ergebnis:** *(Nach dem Apply ausfüllen)*
*   **Nachweis:** *(Optional) Screenshot der Inbound-Regeln der RDS-Security-Group.*

---

**Testfall: Verifizierung der EKS-Node-Konfiguration (IMDS & Topologie)**

*   **Zugehörige User Story:** Indirekt mit `#11` und `#14` verbunden. Entstanden aus der Fehleranalyse in Sprint 3.
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass die EKS-Worker-Nodes korrekt konfiguriert sind, um Konnektivitätsprobleme mit dem EC2 Metadata Service (IMDS) zu vermeiden und die dynamische Volume-Provisionierung zu ermöglichen.
*   **Testschritte:**
    1.  Nach erfolgreichem `terraform apply` und dem Start der Worker Nodes den Namen eines Nodes ermitteln: `kubectl get nodes`.
    2.  Überprüfen, ob die IMDS Hop-Limit auf dem EC2-Level korrekt gesetzt ist (erfordert AWS CLI):
        `aws ec2 describe-instances --instance-ids <instance-id> --query "Reservations[*].Instances[*].MetadataOptions"`
        *Erwartetes Ergebnis: `http_put_response_hop_limit` sollte `2` sein.*
    3.  Überprüfen, ob der EBS CSI-Treiber die Topologie-Informationen auf dem Kubernetes-Node-Objekt korrekt setzen konnte:
        `kubectl describe node <node-name> | findstr "topology.ebs.csi.aws.com/zone"`
        *Erwartetes Ergebnis: Der Befehl sollte eine Zeile ausgeben, z.B. `topology.ebs.csi.aws.com/zone=eu-central-1a`.*
    4.  Die Logs des `ebs-csi-node`-Pods auf dem jeweiligen Node überprüfen:
        `kubectl logs <ebs-csi-node-pod-name> -n kube-system -c ebs-plugin`
        *Erwartetes Ergebnis: Die Logs dürfen **keine** Timeout-Fehler (`context deadline exceeded`) bei der IMDS-Abfrage enthalten.*
*   **Tatsächliches Ergebnis:** Alle Überprüfungsschritte waren nach der Implementierung der `aws_launch_template` erfolgreich. Die IMDS-Konnektivität wurde hergestellt und die Topologie-Labels wurden korrekt gesetzt, was die `Pending`-PVC-Probleme löste.
*   **Nachweis:** Die erfolgreiche Bereitstellung des Nextcloud-PVC dient als finaler, funktionierender Nachweis für diesen Testfall.

---

**Testfall: Validierung der OIDC-Authentifizierung von GitHub Actions zu AWS**

*   **Zugehörige User Story:** `Nextcloud#20` - OIDC Authentifizierung für GitHub Actions zu AWS einrichten
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Verifizieren, dass die via Terraform erstellte OIDC-Konfiguration es einem GitHub Actions Workflow erlaubt, die dedizierte IAM-Rolle sicher zu übernehmen und AWS-API-Aufrufe auszuführen.
*   **Testschritte:**
    1.  **Infrastruktur-Provisionierung:** Ausführen von `terraform apply` im `terraform/`-Verzeichnis, um den `aws_iam_openid_connect_provider` und die `aws_iam_role` (`Nextcloud-cicd-role`) zu erstellen.
    2.  **Manuelle Überprüfung in AWS IAM:**
        *   Navigieren zu **IAM -> Identity providers**. Verifizieren, dass der Provider für `token.actions.githubusercontent.com` mit der korrekten `Audience` (`sts.amazonaws.com`) existiert.
        *   Navigieren zu **IAM -> Roles** und die Rolle `Nextcloud-cicd-role` auswählen.
        *   Den Tab **"Trust relationships"** prüfen. Die Policy muss die `Federated`-Principal-ARN des OIDC-Providers und die `Condition` enthalten, die den Zugriff auf das korrekte GitHub-Repository (`repo:Stevic-Nenad/Nextcloud:ref:refs/heads/main`) beschränkt.
    3.  **Funktionale Verifizierung via GitHub Actions:**
        *   Erstellen eines temporären Workflows (`.github/workflows/test-auth.yml`), der manuell getriggert werden kann.
        *   Der Workflow-Job muss die `permissions: id-token: write` besitzen.
        *   Der Workflow verwendet die Action `aws-actions/configure-aws-credentials@v4`, um die Übernahme der `Nextcloud-cicd-role` zu versuchen.
        *   Als nachfolgenden Schritt führt der Workflow `aws sts get-caller-identity` aus.
        *   Den Workflow manuell über die GitHub-UI starten.
*   **Erwartetes Ergebnis:**
    *   Die Terraform-Ressourcen werden ohne Fehler erstellt.
    *   Die manuelle Überprüfung in der AWS-Konsole bestätigt die korrekte Konfiguration der Trust Policy.
    *   Der Test-Workflow in GitHub Actions läuft erfolgreich durch.
    *   Die Ausgabe des `aws sts get-caller-identity`-Befehls im Workflow-Log zeigt die `Arn` der übernommenen Rolle (`...:assumed-role/Nextcloud-cicd-role/...`), nicht die eines IAM-Users.
*   **Tatsächliches Ergebnis:** Alle Schritte waren erfolgreich. Die manuelle Überprüfung der IAM-Konfiguration war korrekt. Der `test-auth.yml`-Workflow wurde erfolgreich ausgeführt und die Ausgabe von `get-caller-identity` bestätigte, dass die Pipeline-Rolle wie erwartet übernommen wurde.
*   **Nachweis:** Ein Screenshot der erfolgreichen `test-auth.yml`-Workflow-Ausgabe in GitHub, der die `aws sts get-caller-identity`-Antwort zeigt, dient als primärer Nachweis.

---

**Testfall: End-to-End-Validierung der CI/CD-Pipeline**

*   **Zugehörige User Story:** `Nextcloud#21` - GitHub Actions Workflow für Helm Chart Deployment erstellen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Verifizieren, dass der gesamte CI/CD-Workflow von Anfang bis Ende erfolgreich durchläuft, dynamisch Infrastrukturdaten aus dem Terraform-State liest, das Helm Chart korrekt bereitstellt, das Load-Balancer-Problem löst und die Installation mit Helm-Tests verifiziert.
*   **Testschritte:**
    1.  **Voraussetzung:** Sicherstellen, dass die AWS-Infrastruktur (inkl. EKS-Cluster und RDS) via `terraform apply` provisioniert und der Terraform-State im S3-Backend aktuell ist.
    2.  **Secret-Konfiguration:** Verifizieren, dass alle notwendigen Secrets (`AWS_ACCOUNT_ID`, `AWS_REGION`, etc.) in den GitHub-Repository-Einstellungen konfiguriert sind.
    3.  **Trigger:** Eine Code-Änderung (z.B. ein Kommentar in der `README.md`) in den `main`-Branch pushen, um den Workflow auszulösen.
    4.  **Beobachtung des Workflows:** Im "Actions"-Tab von GitHub den laufenden Workflow beobachten und die Logs der einzelnen Jobs und Schritte prüfen:
        *   **Job `get-infra-data`:**
            *   Muss `terraform init` erfolgreich ausführen.
            *   Muss `terraform output` erfolgreich ausführen und die korrekten Werte für `eks_cluster_name` und `rds_host` als Job-Outputs setzen.
        *   **Job `deploy-application`:**
            *   Muss die Outputs des `get-infra-data`-Jobs korrekt empfangen.
            *   Der `Configure Kubectl`-Schritt muss den dynamisch bezogenen Clusternamen im Log anzeigen.
            *   `helm lint` muss erfolgreich sein.
            *   Der erste `helm upgrade`-Schritt muss erfolgreich sein und den dynamisch bezogenen RDS-Host verwenden.
            *   Das `Get Load Balancer Hostname`-Skript muss den externen Hostnamen finden und loggen.
            *   Der zweite `helm upgrade`-Schritt muss erfolgreich sein.
            *   Der `Run Helm Tests`-Schritt muss die Tests erfolgreich ausführen und als "succeeded" abschliessen.
*   **Erwartetes Ergebnis:** Der gesamte Workflow wird mit einem grünen Haken (Status: Success) abgeschlossen. Jeder einzelne Schritt verläuft wie erwartet, und die Logs bestätigen die dynamische Übergabe der Infrastrukturdaten und den erfolgreichen Abschluss aller Deployment- und Testphasen.
*   **Tatsächliches Ergebnis:** Der Workflow wurde nach dem Push auf `main` erfolgreich ausgelöst. Der `get-infra-data`-Job hat die korrekten Cluster- und RDS-Namen aus dem S3-Remote-State gelesen. Der `deploy-application`-Job hat diese Werte erfolgreich übernommen, die Nextcloud-Anwendung installiert, den Load-Balancer-Hostnamen korrekt konfiguriert und die Helm-Tests bestanden. Der gesamte Workflow wurde erfolgreich abgeschlossen.
*   **Nachweis:** Ein Screenshot der erfolgreichen Workflow-Übersichtsseite in GitHub Actions, der beide Jobs mit grünen Haken zeigt.

---

**Testfall: Validierung des Pipeline Status-Badges**
*   **Zugehörige User Story:** `Nextcloud#23` - Pipeline Status Badge im README anzeigen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass das Status-Badge korrekt im `README.md` angezeigt wird, den Live-Status der Pipeline widerspiegelt und korrekt verlinkt ist.
*   **Testschritte:**
    1.  Den Markdown-Code für das Status-Badge aus der GitHub Actions UI für den `deploy.yml`-Workflow kopieren.
    2.  Den Code in die `README.md`-Datei einfügen und die Änderung in den `main`-Branch pushen.
    3.  Warten, bis der dadurch ausgelöste Workflow abgeschlossen ist.
    4.  Die Hauptseite des GitHub-Repositorys im Browser neu laden.
    5.  Visuell überprüfen, ob das Badge angezeigt wird und den korrekten Status (z.B. "passing") anzeigt.
    6.  Auf das Badge klicken.
*   **Erwartetes Ergebnis:** Das Badge ist sichtbar und zeigt den korrekten Status. Ein Klick auf das Badge leitet den Benutzer auf die Übersichtsseite der Workflow-Ausführungen für `deploy.yml` weiter.
*   **Tatsächliches Ergebnis:** Alle Schritte waren erfolgreich. Das Badge wurde korrekt angezeigt und der Link funktionierte wie erwartet.
*   **Nachweis:** Die `README.md`-Datei selbst im Repository dient als permanenter, live-aktualisierter Nachweis.

---

**Testfall: Validierung des "Full Setup"-Lifecycle-Workflows**
*   **Zugehörige User Story:** `Nextcloud#41` - "Full Setup" GitHub Actions Workflow erstellen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass der manuelle `setup`-Workflow die gesamte AWS-Infrastruktur und die Nextcloud-Anwendung von Grund auf erfolgreich und automatisiert provisionieren kann.
*   **Testschritte:**
    1.  **Voraussetzung:** Es existiert keine Projekt-Infrastruktur in AWS (ggf. vorherigen Teardown durchführen).
    2.  In der GitHub Actions UI den "Full Environment Lifecycle"-Workflow auswählen.
    3.  Den Workflow manuell mit der ausgewählten Option `setup` starten.
    4.  Den Fortschritt der beiden Jobs (`terraform-apply`, `deploy-application`) beobachten.
    5.  Die Logs des `terraform-apply`-Jobs überprüfen, um die erfolgreiche Ressourcenerstellung zu bestätigen.
    6.  Die Logs des `deploy-application`-Jobs überprüfen, der den wiederverwendbaren Workflow aufruft, um das erfolgreiche Helm-Deployment und die Helm-Tests zu bestätigen.
*   **Erwartetes Ergebnis:** Beide Jobs laufen erfolgreich durch. Der gesamte Workflow wird mit einem grünen Haken abgeschlossen. Eine Überprüfung in der AWS-Konsole zeigt die neu erstellte Infrastruktur, und die Nextcloud-Instanz ist über ihren Load-Balancer-Endpunkt erreichbar.
*   **Tatsächliches Ergebnis:** Der Workflow wurde erfolgreich ausgeführt. Der `terraform-apply`-Job erstellte alle AWS-Ressourcen fehlerfrei. Der `deploy-application`-Job übernahm die Outputs korrekt und installierte und verifizierte die Nextcloud-Anwendung.
*   **Nachweis:** Ein Screenshot der erfolgreichen `lifecycle.yml`-Workflow-Ausführung (für `setup`) in der GitHub Actions UI.

---

**Testfall: Validierung des "Full Teardown"-Lifecycle-Workflows**
*   **Zugehörige User Story:** `Nextcloud#42` - "Full Teardown" GitHub Actions Workflow erstellen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass der manuelle `destroy`-Workflow die gesamte Anwendung und AWS-Infrastruktur sauber, vollständig und fehlertolerant entfernen kann.
*   **Testschritte:**
    1.  **Voraussetzung:** Eine vollständige Projekt-Infrastruktur und Anwendungs-Deployment existieren (erstellt durch den `setup`-Workflow).
    2.  In der GitHub Actions UI den "Full Environment Lifecycle"-Workflow auswählen.
    3.  Den Workflow manuell mit der ausgewählten Option `destroy` starten.
    4.  Den Fortschritt der Jobs (`get-destroy-data`, `helm-uninstall`, `terraform-destroy`) beobachten.
    5.  Die Logs des `helm-uninstall`-Jobs überprüfen, um die erfolgreiche Deinstallation des Helm-Releases zu bestätigen.
    6.  Die Logs des `terraform-destroy`-Jobs überprüfen, um die erfolgreiche Zerstörung der AWS-Ressourcen zu bestätigen.
    7.  Nach Abschluss des Workflows in der AWS Management Console überprüfen, ob die Kernressourcen (VPC, EKS-Cluster, EC2-Nodes, RDS-Instanz) nicht mehr existieren.
    8.  **(Optionaler Fehlertoleranz-Test):** Den `destroy`-Workflow erneut ausführen.
*   **Erwartetes Ergebnis:** Alle Jobs laufen erfolgreich durch. Die Überprüfung in der AWS-Konsole bestätigt, dass alle Ressourcen entfernt wurden. Der optionale zweite Lauf schlägt nicht fehl, sondern überspringt die Schritte, da keine Ressourcen mehr vorhanden sind.
*   **Tatsächliches Ergebnis:** Der `destroy`-Workflow lief wie erwartet durch, entfernte zuerst die Helm-Anwendung und zerstörte danach die gesamte AWS-Infrastruktur. Die AWS-Konsole war danach wieder leer (bezogen auf die Projekt-Ressourcen).
*   **Nachweis:** Ein Screenshot der erfolgreichen `lifecycle.yml`-Workflow-Ausführung (für `destroy`) in der GitHub Actions UI.

---

**Testfall: Validierung der Installations- und Inbetriebnahme-Anleitung**
*   **Zugehörige User Story:** `Nextcloud#28` - Installations- und Inbetriebnahme-Anleitung erstellen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass die Anleitung in Kapitel 4.4 vollständig, korrekt und verständlich ist und es einem neuen Benutzer ermöglicht, das Projekt erfolgreich von Grund auf in Betrieb zu nehmen.
*   **Testschritte:**
    1.  Führen Sie eine "mentale Generalprobe" durch: Lesen Sie Kapitel 4.4 von Anfang bis Ende aus der Perspektive einer Person, die das Projekt zum ersten Mal sieht.
    2.  Überprüfen Sie jeden Schritt auf Klarheit und Eindeutigkeit. Gibt es potenziell missverständliche Formulierungen?
    3.  Vergleichen Sie die in der Anleitung geforderten Konfigurationsschritte (insbesondere die Liste der GitHub Secrets) mit der tatsächlichen Implementierung in den Workflow-Dateien.
    4.  Folgen Sie den Anweisungen zum Auslösen des `setup`-Workflows.
    5.  Folgen Sie den Anweisungen zum Zugriff auf die Nextcloud-Instanz.
    6.  Folgen Sie den Anweisungen zum Auslösen des `destroy`-Workflows.
*   **Erwartetes Ergebnis:** Die Anleitung ist logisch, lückenlos und führt einen Benutzer ohne zusätzliches Wissen erfolgreich durch den gesamten Prozess von der Konfiguration bis zum funktionierenden (und wieder entfernten) System.
*   **Tatsächliches Ergebnis:** Die Anleitung wurde sorgfältig überprüft. Die Schritte sind korrekt und führen zu einem erfolgreichen Ergebnis. Die Anweisungen sind klar und fokussieren sich auf den automatisierten "Happy Path" über die GitHub Actions, was die Komplexität für den Endbenutzer minimiert.
*   **Nachweis:** Die finale Version von Kapitel 4.4 in der `README.md` dient als Nachweis für diesen Testfall.

---

**Testfall: Überprüfung der Architektur-Diagramme**
*   **Zugehörige User Story:** `Nextcloud#26` - Systemarchitektur-Diagramm erstellen und pflegen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass die Architektur-Diagramme den finalen Stand des Projekts korrekt, verständlich und vollständig repräsentieren.
*   **Testschritte:**
    1.  Betrachten Sie das logische Gesamtarchitektur-Diagramm in Abschnitt 3.3.1.
    2.  Vergleichen Sie die dargestellten Komponenten und Flüsse mit der tatsächlichen Implementierung (GitHub Actions Workflow, Helm Chart, AWS-Dienste). Sind alle Hauptkomponenten vorhanden? Sind die Interaktionen korrekt dargestellt?
    3.  Betrachten Sie das AWS-Netzwerkarchitektur-Diagramm in Abschnitt 3.3.2.
    4.  Vergleichen Sie die Darstellung (VPC, Subnetze, NAT-Gateway-Strategie, Platzierung von EKS/RDS in privaten Subnetzen) mit der implementierten Terraform-Konfiguration (`terraform/network.tf`, `eks_nodegroup.tf`, `rds.tf`).
*   **Erwartetes Ergebnis:** Beide Diagramme stellen die Realität der Implementierung korrekt dar. Sie sind eine genaue visuelle Abstraktion des gebauten Systems.
*   **Tatsächliches Ergebnis:** Die Diagramme wurden sorgfältig erstellt, um die finale, funktionierende Architektur, inklusive der OIDC-Authentifizierung und der High-Availability-Netzwerkstruktur, widerzuspiegeln. Sie sind korrekt und aktuell.
*   **Nachweis:** Die gerenderten Diagramme in den Abschnitten 3.3.1 und 3.3.2 der `README.md` dienen als Nachweis.

---

**Testfall: Validierung des Reflexionskapitels**
*   **Zugehörige User Story:** `Nextcloud#32` - Reflexionskapitel im README vervollständigen
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass das Reflexionskapitel vollständig, ehrlich und aussagekräftig ist und die Anforderungen der Semesterarbeit erfüllt.
*   **Testschritte:**
    1.  Lesen Sie das gesamte Kapitel 7.
    2.  Überprüfen Sie, ob jeder Unterabschnitt (7.1 bis 7.4) inhaltlich gefüllt ist.
    3.  Prüfen Sie, ob die Reflexion die tatsächlichen Ereignisse und Herausforderungen aus den Sprints (z.B. PVC-Problem, Naming-Konvention, dynamischer Cluster-Name) widerspiegelt.
    4.  Bewerten Sie, ob die formulierten Verbesserungspotenziale und Handlungsempfehlungen konkret und nachvollziehbar sind.
*   **Erwartetes Ergebnis:** Das Kapitel ist eine kohärente, kritische und ehrliche Auseinandersetzung mit dem Projektverlauf und den Lernergebnissen.
*   **Tatsächliches Ergebnis:** Das Kapitel wurde sorgfältig ausgearbeitet und reflektiert den Projektverlauf authentisch. Die Analyse der Theorie-Praxis-Lücke, der persönlichen Entwicklung und der Lösung selbst ist kritisch und fundiert.
*   **Nachweis:** Das finale Kapitel 7 in der `README.md` dient als Nachweis.

---

**Testfall: Codebase-Qualität und -Verständlichkeit**
*   **Zugehörige User Story:** `Nextcloud#31` - Codebase finalisieren und kommentieren
*   **Status:** Abgeschlossen
*   **Zielsetzung:** Sicherstellen, dass die Codebase den Qualitätsanforderungen entspricht, gut dokumentiert ist und frei von unnötigen Artefakten ist.
*   **Testschritte:**
    1.  **Formatierung prüfen:** Führen Sie `terraform fmt --check -recursive` im `/terraform`-Verzeichnis aus.
    2.  **Code-Review (Kommentare):** Führen Sie ein manuelles Review der folgenden Dateien durch und prüfen Sie, ob die Kommentare das "Warum" hinter dem Code erklären:
        *   `terraform/iam_cicd.tf`
        *   `terraform/launch_template.tf`
        *   `.github/workflows/lifecycle.yml`
        *   `charts/nextcloud-chart/templates/deployment.yaml`
    3.  **Code-Review (Dead Code):** Stellen Sie sicher, dass keine auskommentierten, nicht-funktionalen Codeblöcke mehr vorhanden sind.
    4.  **Struktur-Review:** Überprüfen Sie die Projekt-Verzeichnisstruktur. Das Verzeichnis `/kubernetes-manifests` darf nicht mehr existieren.
*   **Erwartetes Ergebnis:** Die Formatierung ist korrekt. Die Kommentare sind hilfreich und erklären komplexe Logik. Das Repository ist sauber und enthält keine obsoleten Dateien mehr.
*   **Tatsächliches Ergebnis:** Alle Formatierungs-, Kommentierungs- und Bereinigungsschritte wurden erfolgreich durchgeführt. Die Codebase ist in einem sauberen, verständlichen und gut dokumentierten Zustand.
*   **Nachweis:** Der finale Zustand des `main`-Branches im Git-Repository dient als Nachweis.

---

#### 5.2.1 Nachweise der Testergebnisse (Screenshots/GIFs)

[PLATZHALTER]

* **Überprüfung der Kosten-Tags auf VPC-Ressourcen (User Story #7 & #5):**
    * ![Nachweis Kosten-Tags auf VPC](assets/images/tests/vpc_tags_verification.png)
      *(Beschreibung: Beispiel-Screenshot aus der AWS Management Console, der die Tags "Projekt: Nextcloud", "Student:
      NenadStevic", "ManagedBy: Terraform" auf der erstellten VPC und/oder einem NAT Gateway zeigt.)*

[PLATZHALTER]

* **Überprüfung der VPC-Grundfunktionalität (User Story #5):**
    * Die erfolgreiche Ausführung von `terraform apply` und die stichprobenartige Überprüfung der Kernkomponenten (VPC,
      Subnetze, IGW, NAT Gateways, Routen) in der AWS Konsole dienen als Nachweis für die korrekte Provisionierung.
      Detaillierte Screenshots für jeden einzelnen Verifizierungsschritt werden nicht beigefügt, um die Dokumentation
      schlank zu halten, können aber bei Bedarf nachgereicht werden. Die Terraform Outputs bestätigen ebenfalls die
      Erstellung der Ressourcen-IDs.

[PLATZHALTER]

* **Überprüfung der EKS Worker Nodes (User Story #8):**
    * ![Nachweis EKS Nodes Ready](assets/images/tests/eks_nodes_ready.png)
      *(Beschreibung: Screenshot der Ausgabe von `kubectl get nodes -o wide`, der die Worker Nodes im Status "Ready" mit
      ihren privaten IP-Adressen zeigt.)*

---

## 6. Projektdokumentation (Zusammenfassung)

*Diese `README.md` dient als zentrale Projektdokumentation. Alle relevanten Informationen, Entscheidungen und Ergebnisse
sind hier festgehalten oder direkt verlinkt.*

### 6.1 Verzeichnisse und Zusammenfassungen

*Das Inhaltsverzeichnis am Anfang dieser Datei bietet eine schnelle Navigation. Die wesentlichen Zusammenfassungen
finden sich in den jeweiligen Kapiteln und Sprint-Abschnitten.*

### 6.2 Quellenangaben und verwendete Werkzeuge

*Auflistung externer Quellen, wichtiger Tutorials oder Dokumentationen, die herangezogen wurden (sofern nicht direkt im
Text erwähnt). Sowie eine Liste der Kernwerkzeuge.*

### 6.3 Lizenz

Der **gesamte Quellcode** dieses Projekts (Terraform, Helm-Charts, GitHub-Actions-Workflows usw.) steht unter
der [MIT-Lizenz](LICENSE).  
Die **Dokumentation** dieses Repositories ist, sofern nicht anders gekennzeichnet, ebenfalls unter MIT veröffentlicht.  
Das verwendete Nextcloud-Docker-Image unterliegt der AGPL-3.0 (→
siehe [Nextcloud-Projekt](https://github.com/nextcloud/docker)).
---

## 7. Reflexion und Erkenntnisse

*Ein kritischer Rückblick auf das Projekt, den Prozess und die persönlichen Lernerfahrungen.*

### 7.1 Abgleich von Theorie und Praxis

Dieses Projekt war eine intensive Übung im Abgleich von theoretischen Konzepten aus dem Studium mit der oft unvorhersehbaren Realität der Cloud-nativen Entwicklung.

*   **Scrum in der Einzelarbeit:** In der Theorie ist Scrum ein Framework für Teams. In der Praxis der Einzelarbeit wurde es zu einem mächtigen Werkzeug für Selbstmanagement und Disziplin. Die Zeremonien zwangen mich zu regelmässiger Planung (Sprint Planning), täglicher Reflexion (Daily Scrum), Ergebniskontrolle (Sprint Review) und Prozessverbesserung (Retrospektive). Besonders wertvoll war die strikte Einhaltung des committeten Sprint-Ziels. Die bewusste Entscheidung in Sprint 5, die Terraform-Automatisierung (`#22`) *nicht* ins Backlog aufzunehmen, war eine praktische Anwendung des Prinzips, Scope Creep zu vermeiden und den Fokus auf das versprochene Inkrement zu legen.

*   **Infrastructure as Code (Terraform):** Die Theorie verspricht eine deklarative, wiederholbare Infrastruktur. Die Praxis konfrontierte mich mit subtilen Abhängigkeiten und Cloud-spezifischen Eigenheiten. Das "IMDS Hop Limit"-Problem in Sprint 3 war ein perfektes Beispiel: ein tiefgreifendes technisches Problem, das in keinem "Getting Started"-Guide steht, aber in der Realität ein kompletter Blocker sein kann. Die Lösung erforderte eine Recherche, die weit über die reine Terraform-Syntax hinausging und ein Verständnis der darunterliegenden EC2-Architektur verlangte. Ebenso zeigte die Notwendigkeit von `depends_on` und die `sleep`-Verzögerung im Teardown-Workflow, dass die theoretische Idempotenz in der Praxis oft durch Timing-Probleme der Cloud-Provider auf die Probe gestellt wird.

*   **CI/CD (GitHub Actions):** Das theoretische Konzept einer nahtlosen Pipeline vom Code-Push zum Deployment traf auf das klassische "Chicken-and-Egg"-Problem des Load-Balancer-Hostnamens. Die Lösung war kein einzelner, vordefinierter Action-Block, sondern ein pragmatisches Shell-Skript innerhalb des Workflows. Dies war eine wichtige Erkenntnis: CI/CD in der Praxis ist oft eine Kombination aus etablierten Best Practices (wie OIDC) und kreativen, kontext-spezifischen Skripten, um die Lücken zu füllen.

### 7.2 Eigene Erfahrungen und persönlicher Lernprozess

Mein persönlicher Lernprozess in diesem Projekt war steil und vielschichtig. Die grössten Erkenntnisse waren:

*   **Die Macht des schnellen Feedbacks:** Der konsequente Einsatz von `helm template` und `helm lint` während der Chart-Entwicklung (Sprint 4) war ein "Aha!"-Moment. Fehler, wie die anfänglich redundante Benennung der Ressourcen, wurden sofort sichtbar und konnten behoben werden, bevor sie überhaupt zu einem fehlgeschlagenen Deployment führen konnten. Dieser schnelle, lokale Feedback-Zyklus ist unbezahlbar.

*   **Systematisches Debugging ist alles:** Die grösste Hürde war zweifellos die Fehlersuche beim `Pending`-PVC-Status in Sprint 3. Das Problem führte mich von der Kubernetes-Ebene (`kubectl describe pvc`) über die Treiber-Logs (`kubectl logs -n kube-system ...`) bis hin zu den tiefsten Einstellungen der EC2-Infrastruktur. Diese Erfahrung hat mir die Wichtigkeit einer schichtweisen, systematischen Fehlersuche in verteilten Systemen eindrücklich vor Augen geführt.

*   **Architektur > Code:** Die Lösung des Problems mit dem dynamischen Cluster-Namen in Sprint 5 war ein weiterer Wendepunkt. Anstatt eine "Quick-and-Dirty"-Lösung zu suchen, führte die Überlegung zu einer besseren Architektur: die Entkopplung der Jobs und das Auslesen des Terraform-Remote-States. Dies führte zu einer robusteren, professionelleren und wartbareren Pipeline. Es hat mir gezeigt, dass ein Schritt zurück, um die Architektur zu überdenken, oft schneller zum Ziel führt als stures Weiterprogrammieren.

### 7.3 Bewertung der eigenen Lösung und Verbesserungspotenzial

Die entwickelte Lösung erfüllt alle gesetzten Projektziele und ist eine robuste, vollautomatisierte End-to-End-Plattform.

**Stärken:**
*   **Vollständige Automatisierung:** Der gesamte Lebenszyklus von der Erstellung der Infrastruktur bis zur Zerstörung ist auf Knopfdruck automatisiert.
*   **Sicherheit:** Durch den konsequenten Einsatz von OIDC werden keine langlebigen Credentials in GitHub gespeichert.
*   **Wiederverwendbarkeit:** Das Helm Chart ist sauber strukturiert, gut dokumentiert und kann auch für andere Deployments verwendet werden.
*   **Reproduzierbarkeit:** Dank Infrastructure as Code (Terraform) und dem zentralen State ist die Umgebung jederzeit exakt reproduzierbar.

**Schwächen & Verbesserungspotenzial:**
Obwohl die Lösung im Rahmen der Semesterarbeit vollständig ist, gibt es für einen echten Produktiveinsatz bewusst in Kauf genommene Vereinfachungen und fehlende Aspekte:

*   **Terraform-Automatisierung:** Die grösste Vereinfachung war das De-Scoping der Terraform-Ausführung in der CI/CD-Pipeline für den `main`-Branch. Ein `terraform plan` auf Pull Requests wäre der nächste logische Schritt für eine echte GitOps-Strategie für die Infrastruktur.
*   **Sicherheits-Härtung:** Die IAM-Rolle für die Pipeline ist zwar nicht `AdministratorAccess`, könnte aber noch granularer sein. Innerhalb des Clusters fehlen `NetworkPolicies`, um den Traffic zwischen den Pods weiter einzuschränken.
*   **Monitoring und Logging:** Es gibt keine dedizierte Lösung für das Monitoring (z.B. Prometheus/Grafana) oder das zentrale Logging (z.B. Loki/Fluentd). Im Fehlerfall ist man auf `kubectl logs` angewiesen.
*   **Backup und Restore:** Eine Backup-Strategie für die Nextcloud-Daten (EBS Volume) und die RDS-Datenbank wurde nicht implementiert. Werkzeuge wie Velero oder die Nutzung von RDS Snapshots wären hier anzusetzen.
*   **Pragmatischer `sleep`:** Der `sleep 60`-Befehl im Teardown-Workflow ist ein pragmatischer Workaround für ein bekanntes Timing-Problem bei der Löschung von Cloud-Ressourcen. Eine robustere Lösung würde in einer Schleife den Status der Abhängigkeit abfragen.

### 7.4 Handlungsempfehlungen für das weitere Vorgehen

Basierend auf der Bewertung ergeben sich folgende konkrete Handlungsempfehlungen, um die Lösung in Richtung eines produktiven Systems weiterzuentwickeln:

1.  **Terraform-Plan auf Pull-Requests implementieren:** Den `lifecycle.yml`-Workflow so erweitern, dass bei Pull Requests, die das `terraform/`-Verzeichnis ändern, automatisch ein `terraform plan` ausgeführt und das Ergebnis als Kommentar im PR gepostet wird. Dies erhöht die Transparenz und Kontrolle über Infrastruktur-Änderungen.

2.  **GitOps für Anwendungs-Deployments einführen:** Einen GitOps-Operator wie ArgoCD oder FluxCD im EKS-Cluster installieren. Der CI-Workflow würde dann nicht mehr `helm upgrade` ausführen, sondern nur noch ein neues Image-Tag in einer Git-Repository-Konfigurationsdatei aktualisieren. Der GitOps-Operator würde diese Änderung erkennen und das Deployment im Cluster automatisch synchronisieren.

3.  **Secrets-Management verbessern:** Den AWS Secrets Store CSI Driver im EKS-Cluster installieren. Dies würde es ermöglichen, die RDS-Credentials direkt als Volume in den Nextcloud-Pod zu mounten, anstatt sie über GitHub Secrets und den `helm --set`-Befehl zu injizieren.

4.  **Monitoring-Stack implementieren:** Ein Helm-Chart für einen Monitoring-Stack (z.B. `kube-prometheus-stack`) deployen, um Metriken von EKS, den Nodes und der Nextcloud-Anwendung zu sammeln und in Grafana zu visualisieren.


---

## 8. Anhänge

*Zusätzliche Materialien, die das Verständnis unterstützen oder für die Nachvollziehbarkeit relevant sind.*

### 8.1 Verwendete Scrum-Vorlagen (Templates)

*Die hier aufgeführten Markdown-Vorlagen dienten als Grundlage und Inspiration für die Dokumentation der
Scrum-Zeremonien und Artefakte.*

* [Sprint Planning Vorlage](docs/templates/sprint_planning.md)
* [Daily Scrum Log Vorlage](docs/templates/daily_scrum.md)
* [Sprint Review Vorlage](docs/templates/sprint_review.md)
* [Sprint Retrospektive Vorlage](docs/templates/sprint_retro.md)
* [User Story Vorlage](docs/templates/user_story.md)

### 8.2 Weitere Referenzen (Optional)

*Platz für zusätzliche Links, interessante Artikel oder Dokumente, die im Projektkontext relevant waren.*

### 8.3 Link zum GitHub Repository

*Der vollständige Quellcode und diese Dokumentation sind öffentlich zugänglich
unter: https://github.com/Stevic-Nenad/Nextcloud*

### 8.4 Link zum GitHub Project Board (Kanban/Scrum Board)

*Der aktuelle Stand der Aufgaben und das Product/Sprint Backlog sind einsehbar
unter: https://github.com/users/Stevic-Nenad/projects/1/views/1*

---

**Selbstständigkeitserklärung**

Ich erkläre hiermit, dass ich die vorliegende Semesterarbeit selbstständig und ohne fremde Hilfe verfasst und keine
anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Stellen, die dem Wortlaut oder dem Sinn nach
anderen Werken entnommen sind, wurden unter Angabe der Quelle kenntlich gemacht.

Ort, Datum Nenad Stevic

---

==================================================
File: .\terraform\backend.tf
==================================================
terraform {
  backend "s3" {
    bucket         = "nenad-stevic-nextcloud-tfstate"
    key            = "nextcloud-project/terraform.tfstate"
    region         = "eu-central-1"
    dynamodb_table = "nenad-stevic-nextcloud-tfstate-lock"
    encrypt        = true
    profile        = "nextcloud-project"
  }
}

==================================================
File: .\terraform\ecr.tf
==================================================
resource "aws_ecr_repository" "nextcloud_app" {
  name                 = var.ecr_repository_name
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-${var.ecr_repository_name}-repo"
    }
  )
}

resource "aws_ecr_lifecycle_policy" "nextcloud_app_policy" {
  repository = aws_ecr_repository.nextcloud_app.name

  policy = jsonencode({
    rules = [
      # 1. Expire untagged images older than 30 days
      {
        rulePriority = 1,
        description  = "Expire untagged images >30 days",
        selection = {
          tagStatus   = "untagged",
          countType   = "sinceImagePushed",
          countUnit   = "days",
          countNumber = 30
        },
        action = {
          type = "expire"
        }
      },
      # 2. Keep last 10 images for common tag patterns
      {
        rulePriority = 2,
        description  = "Keep last 10 tagged images",
        selection = {
          tagStatus     = "tagged",
          tagPrefixList = ["v", "latest", "main", "dev", "prod", "release"],
          countType     = "imageCountMoreThan",
          countNumber   = 10
        },
        action = {
          type = "expire"
        }
      }
    ]
  })
}

==================================================
File: .\terraform\eks_addons.tf
==================================================
# --- EKS Managed Add-on: AWS EBS CSI Driver ---
# This installs the AWS EBS CSI (Container Storage Interface) Driver, which is the
# modern way for Kubernetes to provision and manage AWS EBS volumes for persistent storage.
#
# Why use the EKS Add-on method?
# - It's managed by AWS, simplifying updates and ensuring compatibility.
# - It integrates seamlessly with Terraform.
# - It's the recommended approach for EKS clusters.
#
# The 'service_account_role_arn' is the most critical part. It links this add-on
# to the specific IAM role we created for it via IRSA, granting it the necessary
# permissions to create/delete/attach EBS volumes on your behalf, without
# ever needing static credentials.

resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "aws-ebs-csi-driver"

  # The ARN of the role created in iam_irsa.tf. This connects the driver to its permissions.
  service_account_role_arn = aws_iam_role.ebs_csi_driver_role.arn

  # This setting prevents conflicts if the add-on already exists, forcing it
  # to adopt the configuration we've defined here in Terraform.
  resolve_conflicts_on_create = "OVERWRITE"

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-ebs-csi-driver-addon"
    }
  )

  # Explicitly depend on the OIDC provider to ensure it exists before the add-on is configured.
  depends_on = [
    aws_iam_openid_connect_provider.eks_oidc_provider
  ]
}

==================================================
File: .\terraform\eks_cluster.tf
==================================================
resource "aws_eks_cluster" "main" {
  name     = "${var.project_name}-eks-cluster"
  role_arn = aws_iam_role.eks_cluster_role.arn
  version  = var.eks_cluster_version

  vpc_config {
    subnet_ids              = concat(aws_subnet.public[*].id, aws_subnet.private[*].id)
    endpoint_private_access = false
    endpoint_public_access  = true
    public_access_cidrs     = var.eks_public_access_cidrs
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-eks-cluster"
    }
  )

  # Ensure IAM role and policies are created before the cluster
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_AmazonEKSClusterPolicy,
  ]
}

==================================================
File: .\terraform\eks_nodegroup.tf
==================================================
resource "aws_eks_node_group" "main_nodes" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.project_name}-main-nodes"
  node_role_arn   = aws_iam_role.eks_node_role.arn

  subnet_ids = aws_subnet.private[*].id

  instance_types = var.eks_node_instance_types
  capacity_type  = "ON_DEMAND"

  scaling_config {
    desired_size = 2 //Enough for basic HA
    max_size     = 2
    min_size     = 2
  }

  update_config {
    max_unavailable_percentage = 50
  }

  launch_template {
    id      = aws_launch_template.eks_nodes.id
    version = aws_launch_template.eks_nodes.latest_version
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-main-node-group"
    }
  )

  # Ensure the cluster and node IAM role policies are created before the node group
  depends_on = [
    aws_eks_cluster.main,
    aws_iam_role_policy_attachment.eks_node_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.eks_node_AmazonEC2ContainerRegistryReadOnly,
    aws_iam_role_policy_attachment.eks_node_AmazonEKS_CNI_Policy,
  ]
}

==================================================
File: .\terraform\iam_cicd.tf
==================================================
# This file defines the IAM resources required for the GitHub Actions CI/CD pipeline
# to securely authenticate with AWS using OIDC.

# --- IAM OIDC Provider for GitHub Actions ---
# This resource establishes the trust relationship between your AWS account and GitHub's OIDC provider.
# It only needs to be created once per AWS account. It tells AWS to trust tokens from GitHub.
resource "aws_iam_openid_connect_provider" "github_actions_oidc_provider" {
  url = "https://token.actions.githubusercontent.com"

  client_id_list = ["sts.amazonaws.com"]

  # These thumbprints are for the root certificates of the GitHub Actions OIDC provider.
  # This is a security measure to ensure AWS is talking to the real GitHub.
  # These values are provided by GitHub's documentation and may need updating in the future.
  thumbprint_list = ["6938fd4d98bab03faadb97b34396831e3780aea1", "1c58a3a8518e8759bf075b76b750d4f2df264fcd"]

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-github-actions-oidc-provider"
    }
  )
}

# --- Trust Policy Data Source for the CI/CD Role ---
# This data source constructs the JSON trust policy that defines who can assume our CI/CD role.
data "aws_iam_policy_document" "github_actions_trust_policy" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"

    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.github_actions_oidc_provider.arn]
    }

    # CRITICAL: This condition scopes the trust down to ONLY workflows from your specific GitHub repository
    # and (optionally) a specific branch, preventing any other repository from assuming this role.
    condition {
      test     = "StringLike"
      variable = "token.actions.githubusercontent.com:sub"
      values   = ["repo:Stevic-Nenad/Nextcloud:ref:refs/heads/main"]
    }
  }
}

# --- IAM Role for the CI/CD Pipeline ---
# This is the specific role that our GitHub Actions workflow will assume.
resource "aws_iam_role" "github_actions_role" {
  name               = "${var.project_name}-cicd-role"
  assume_role_policy = data.aws_iam_policy_document.github_actions_trust_policy.json
  description        = "IAM Role to be assumed by GitHub Actions for deploying the Nextcloud app and managing infrastructure."
  max_session_duration = 3600 # 1 hour

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-github-actions-cicd-role"
    }
  )
}

# --- Permissions Policy for the CI/CD Role ---
# This data source constructs the JSON permissions policy, adhering to the principle of least privilege.
data "aws_iam_policy_document" "github_actions_permissions" {
  # Permissions to allow Terraform to manage all project resources
  statement {
    sid    = "AllowTerraformToManageProjectResources"
    effect = "Allow"
    actions = [
      "ec2:*",
      "eks:*",
      "rds:*",
      "iam:*", # Needed for creating roles, policies, etc.
      "s3:*",  # Needed for backend state
      "dynamodb:*",
      "ecr:*"
    ]
    # In a real production scenario, this would be scoped down further.
    # For this project, allowing full control over these services is pragmatic.
    resources = ["*"]
  }
}

# --- IAM Policy and Attachment ---
resource "aws_iam_policy" "github_actions_policy" {
  name        = "${var.project_name}-cicd-policy"
  description = "Permissions for the GitHub Actions CI/CD pipeline."
  policy      = data.aws_iam_policy_document.github_actions_permissions.json
}

resource "aws_iam_role_policy_attachment" "github_actions_attach_policy" {
  role       = aws_iam_role.github_actions_role.name
  policy_arn = aws_iam_policy.github_actions_policy.arn
}

==================================================
File: .\terraform\iam_eks.tf
==================================================
# --- IAM Role for EKS Cluster ---
# This IAM role is assumed by the EKS control plane.
# It grants the EKS service permissions to make AWS API calls on your behalf to manage resources
# required for the cluster's operation. For example, creating and managing Elastic Network Interfaces (ENIs)
# in your VPC subnets for control plane communication, or managing Load Balancers for Kubernetes services.
#
# Assume Role Policy (Trust Policy):
# This JSON policy defines which principals (entities) are allowed to assume this role.
# For the EKS Cluster Role, the principal is the EKS service itself ('eks.amazonaws.com').
# AWS Documentation on Trust Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/roles-trust-policies.html
resource "aws_iam_role" "eks_cluster_role" {
  name = "${var.project_name}-eks-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRole",
        Principal = {
          Service = "eks.amazonaws.com"
        }
      }
    ]
  })

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-eks-cluster-role"
    }
  )
}

resource "aws_iam_role_policy_attachment" "eks_cluster_AmazonEKSClusterPolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.eks_cluster_role.name
}

# --- IAM Role for EKS Node Group ---
# This IAM role is assumed by the EC2 instances that act as worker nodes in your EKS cluster.
# It grants these instances permissions to:
#   1. Communicate with the EKS control plane (e.g., register themselves, receive workloads).
#   2. Pull container images from Amazon ECR.
#   3. Manage networking resources via the AWS VPC CNI plugin.
#   4. Other operations necessary for a Kubernetes node to function (e.g., write logs to CloudWatch).
#
# Assume Role Policy (Trust Policy):
# For the EKS Node Role, the principal is the EC2 service ('ec2.amazonaws.com'), allowing EC2 instances
# to assume this role when they are launched as part of the EKS node group.
resource "aws_iam_role" "eks_node_role" {
  name = "${var.project_name}-eks-node-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRole",
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-eks-node-role"
    }
  )
}

resource "aws_iam_role_policy_attachment" "eks_node_AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "eks_node_AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "eks_node_AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.eks_node_role.name
}

==================================================
File: .\terraform\iam_irsa.tf
==================================================
# --- IAM OIDC Provider for EKS ---
# This establishes the trust relationship between your EKS cluster and AWS IAM.
# It allows IAM to verify tokens issued by your cluster's OIDC provider, which is the
# foundation for IAM Roles for Service Accounts (IRSA).

data "tls_certificate" "eks_oidc_thumbprint" {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "eks_oidc_provider" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks_oidc_thumbprint.certificates[0].sha1_fingerprint]
  url             = aws_eks_cluster.main.identity[0].oidc[0].issuer

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-eks-oidc-provider"
    }
  )
}

# --- Example IAM Role for EBS CSI Driver Service Account ---
# This is a specific IAM role designed to be assumed by the EBS CSI Driver's Kubernetes Service Account.
# The magic happens in the 'assume_role_policy' (Trust Policy).
#
# Trust Policy Breakdown:
# 1. "Action": "sts:AssumeRoleWithWebIdentity" - This is the specific action for OIDC federation.
# 2. "Principal": It trusts the OIDC provider we created above.
# 3. "Condition": This is the critical part. It ensures that this role can ONLY be assumed by a token
#    whose 'sub' (subject) claim matches the specified Kubernetes Service Account.
#    The format is: system:serviceaccount:<namespace>:<service-account-name>

resource "aws_iam_role" "ebs_csi_driver_role" {
  name = "${var.project_name}-ebs-csi-driver-role"

  # This is the corrected and more robust Trust Policy.
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRoleWithWebIdentity",
        Principal = {
          # The entity that is allowed to assume the role. In this case, our EKS cluster's OIDC provider.
          Federated = aws_iam_openid_connect_provider.eks_oidc_provider.arn
        },
        # These conditions are the security gates. The request must pass ALL of them.
        Condition = {
          # Gate 1: Check the 'audience' of the token. It must be 'sts.amazonaws.com'.
          StringEquals = {
            "${replace(aws_iam_openid_connect_provider.eks_oidc_provider.url, "https://", "")}:aud" = "sts.amazonaws.com"
          },
          # Gate 2: Check the 'subject' of the token. It must match the specific Kubernetes Service Account.
          StringEquals = {
            "${replace(aws_iam_openid_connect_provider.eks_oidc_provider.url, "https://", "")}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
          }
        }
      }
    ]
  })

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-ebs-csi-driver-role"
    }
  )
}

# Attach the AWS-managed policy required for the EBS CSI driver to function.
resource "aws_iam_role_policy_attachment" "ebs_csi_driver_policy_attachment" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
  role       = aws_iam_role.ebs_csi_driver_role.name
}

==================================================
File: .\terraform\launch_template.tf
==================================================
resource "aws_launch_template" "eks_nodes" {
  name_prefix = "${lower(var.project_name)}-lt-"
  description = "Launch template for EKS worker nodes with custom metadata options"

  metadata_options {
    http_tokens                 = "required" # Enforce IMDSv2 for security
    http_put_response_hop_limit = 2          # Increase hop limit for pods
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-eks-launch-template"
    }
  )

  lifecycle {
    create_before_destroy = true
  }
}

==================================================
File: .\terraform\locals.tf
==================================================
locals {
  common_tags = {
    Project   = "Nextcloud"
    Student   = "StevicNenad"
    ManagedBy = "Terraform"
  }
}

==================================================
File: .\terraform\network.tf
==================================================
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr_block
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-vpc"
    }
  )
}

resource "aws_subnet" "public" {
  count                   = length(var.public_subnet_cidrs)
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.public_subnet_cidrs[count.index]
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-public-subnet-${var.availability_zones[count.index]}"
    }
  )
}

resource "aws_subnet" "private" {
  count             = length(var.private_subnet_cidrs)
  vpc_id            = aws_vpc.main.id
  cidr_block        = var.private_subnet_cidrs[count.index]
  availability_zone = var.availability_zones[count.index]

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-private-subnet-${var.availability_zones[count.index]}"
    }
  )
}

resource "aws_internet_gateway" "main_igw" {
  vpc_id = aws_vpc.main.id

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-igw"
    }
  )
}

resource "aws_eip" "nat_eip_per_az" {
  count  = length(var.availability_zones)
  domain = "vpc"

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-nat-eip-${var.availability_zones[count.index]}"
    }
  )
}

resource "aws_nat_gateway" "nat_gw_per_az" {
  count         = length(var.availability_zones)
  allocation_id = aws_eip.nat_eip_per_az[count.index].id
  subnet_id     = aws_subnet.public[count.index].id

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-nat-gw-${var.availability_zones[count.index]}"
    }
  )

  depends_on = [aws_internet_gateway.main_igw]
}

resource "aws_route_table" "public_rt" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main_igw.id
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-public-rt"
    }
  )
}

resource "aws_route_table_association" "public_rt_association" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public_rt.id
}

resource "aws_route_table" "private_rt_per_az" {
  count  = length(var.availability_zones)
  vpc_id = aws_vpc.main.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.nat_gw_per_az[count.index].id
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-private-rt-${var.availability_zones[count.index]}"
    }
  )
}

resource "aws_route_table_association" "private_rt_association_per_az" {
  count          = length(aws_subnet.private)
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private_rt_per_az[count.index].id
}

==================================================
File: .\terraform\outputs.tf
==================================================
output "vpc_id" {
  description = "The ID of the VPC."
  value       = aws_vpc.main.id
}

output "vpc_cidr_block" {
  description = "The CIDR block of the VPC."
  value       = aws_vpc.main.cidr_block
}

output "public_subnet_ids" {
  description = "List of IDs of public subnets."
  value       = aws_subnet.public[*].id
}

output "private_subnet_ids" {
  description = "List of IDs of private subnets."
  value       = aws_subnet.private[*].id
}

output "availability_zones_used" {
  description = "List of Availability Zones where subnets were created."
  value       = var.availability_zones
}

output "nat_gateway_public_ips" {
  description = "List of Public IP addresses of the NAT Gateways (one per AZ)."
  value       = aws_eip.nat_eip_per_az[*].public_ip
}

output "nat_gateway_ids" {
  description = "List of IDs of the NAT Gateways (one per AZ)."
  value       = aws_nat_gateway.nat_gw_per_az[*].id
}

output "eks_cluster_name" {
  description = "The name of the EKS cluster."
  value       = aws_eks_cluster.main.name
}

output "eks_cluster_endpoint" {
  description = "Endpoint for your EKS Kubernetes API server."
  value       = aws_eks_cluster.main.endpoint
}

output "eks_cluster_certificate_authority_data" {
  description = "Base64 encoded certificate data required to communicate with your cluster."
  value       = aws_eks_cluster.main.certificate_authority[0].data
  sensitive   = true // Mark as sensitive as it's credential-like
}

output "eks_node_group_role_arn" {
  description = "ARN of the IAM role for the EKS Node Group."
  value       = aws_iam_role.eks_node_role.arn
}

output "eks_cluster_version_output" {
  description = "The Kubernetes version of the EKS cluster."
  value       = aws_eks_cluster.main.version
}

output "eks_node_group_name" {
  description = "The name of the EKS managed node group."
  value       = aws_eks_node_group.main_nodes.node_group_name
}

output "eks_node_group_status" {
  description = "The status of the EKS managed node group."
  value       = aws_eks_node_group.main_nodes.status
}

output "ecr_repository_url" {
  description = "The URL of the ECR repository."
  value       = aws_ecr_repository.nextcloud_app.repository_url
}

output "ebs_csi_driver_role_arn" {
  description = "The ARN of the IAM role for the EBS CSI Driver Service Account."
  value       = aws_iam_role.ebs_csi_driver_role.arn
}

output "rds_instance_endpoint" {
  description = "The connection endpoint for the RDS instance."
  value       = aws_db_instance.nextcloud.endpoint
}

output "rds_instance_port" {
  description = "The port for the RDS instance."
  value       = aws_db_instance.nextcloud.port
}

output "rds_db_name" {
  description = "The database name."
  value       = aws_db_instance.nextcloud.db_name
}

output "rds_master_username" {
  description = "The master username for the database."
  value       = aws_db_instance.nextcloud.username
}

output "rds_security_group_id" {
  description = "The ID of the RDS security group."
  value       = aws_security_group.rds.id
}

==================================================
File: .\terraform\provider.tf
==================================================
provider "aws" {
  region  = var.aws_region
  profile = "nextcloud-project"

  default_tags {
    tags = local.common_tags
  }
}

==================================================
File: .\terraform\rds.tf
==================================================
resource "aws_db_subnet_group" "nextcloud_rds" {
  name       = "${lower(var.project_name)}-rds-subnet-group"
  subnet_ids = aws_subnet.private[*].id

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-rds-subnet-group"
    }
  )
}

resource "aws_db_parameter_group" "nextcloud_postgres" {
  name = "${lower(var.project_name)}-postgres-${replace(var.rds_pg_version, ".", "")}"
  # The family parameter requires the major version of postgres, e.g., "postgres16".
  # This splits the version string "16.2" at the dot and takes the first part.
  family = "postgres${split(".", var.rds_pg_version)[0]}"

  parameter {
    name  = "client_encoding"
    value = "UTF8"
  }

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-pg-params"
    }
  )
}

data "aws_secretsmanager_secret" "rds_master_password_secret" {
  name = "nextcloud/rds/master-password"
}

data "aws_secretsmanager_secret_version" "rds_master_password_version" {
  secret_id = data.aws_secretsmanager_secret.rds_master_password_secret.id
}

resource "aws_db_instance" "nextcloud" {
  identifier            = "${lower(var.project_name)}-db-instance"
  engine                = "postgres"
  engine_version        = var.rds_pg_version
  instance_class        = var.rds_instance_class
  allocated_storage     = var.rds_allocated_storage
  storage_type          = "gp3"
  max_allocated_storage = 100 # Allows for auto-scaling storage

  username = var.rds_master_username
  password = data.aws_secretsmanager_secret_version.rds_master_password_version.secret_string
  db_name  = var.rds_db_name

  db_subnet_group_name   = aws_db_subnet_group.nextcloud_rds.name
  vpc_security_group_ids = [aws_security_group.rds.id]
  publicly_accessible    = false

  multi_az                 = var.rds_multi_az_enabled
  backup_retention_period  = 7
  delete_automated_backups = true # For easy cleanup in a dev environment
  skip_final_snapshot      = true # For easy cleanup in a dev environment

  parameter_group_name = aws_db_parameter_group.nextcloud_postgres.name

  deletion_protection = false

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-rds-instance"
    }
  )

  depends_on = [
    aws_db_subnet_group.nextcloud_rds,
  ]
}

==================================================
File: .\terraform\security_group.tf
==================================================
resource "aws_security_group" "rds" {
  name        = "${var.project_name}-rds-sg"
  description = "Allow traffic from EKS worker nodes to RDS"
  vpc_id      = aws_vpc.main.id

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-rds-sg"
    }
  )
}

resource "aws_security_group_rule" "eks_to_rds" {
  type                     = "ingress"
  from_port                = 5432
  to_port                  = 5432
  protocol                 = "tcp"
  description              = "Allow EKS nodes to connect to PostgreSQL"
  security_group_id        = aws_security_group.rds.id
  source_security_group_id = aws_eks_cluster.main.vpc_config[0].cluster_security_group_id
}

==================================================
File: .\terraform\variable.tf
==================================================
# ---------------------------------------------------------------------------------------------------------------------
# General Project Variables
# ---------------------------------------------------------------------------------------------------------------------

variable "aws_region" {
  description = "The AWS region where all resources for the project will be deployed."
  type        = string
  default     = "eu-central-1"
}

variable "project_name" {
  description = "A name for the project, used as a prefix for tagging and naming resources."
  type        = string
  default     = "Nextcloud"
}

# ---------------------------------------------------------------------------------------------------------------------
# Networking Variables
# ---------------------------------------------------------------------------------------------------------------------

variable "vpc_cidr_block" {
  description = "The main CIDR block for the Virtual Private Cloud (VPC)."
  type        = string
  default     = "10.0.0.0/16"
}

variable "availability_zones" {
  description = "A list of Availability Zones to use for creating subnets, ensuring high availability."
  type        = list(string)
  default     = ["eu-central-1a", "eu-central-1b"]

  validation {
    condition     = length(var.availability_zones) >= 2
    error_message = "At least two Availability Zones must be specified for high availability."
  }
}

variable "public_subnet_cidrs" {
  description = "A list of CIDR blocks for the public subnets, one for each Availability Zone."
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.2.0/24"]
}

variable "private_subnet_cidrs" {
  description = "A list of CIDR blocks for the private subnets, one for each Availability Zone."
  type        = list(string)
  default     = ["10.0.101.0/24", "10.0.102.0/24"]
}

# ---------------------------------------------------------------------------------------------------------------------
# EKS Cluster Variables
# ---------------------------------------------------------------------------------------------------------------------

variable "eks_cluster_version" {
  description = "The desired Kubernetes version for the EKS cluster control plane."
  type        = string
  default     = "1.29" # As of mid-2024, 1.29 is a stable and supported version.
}

variable "eks_node_instance_types" {
  description = "A list of EC2 instance types to use for the EKS worker nodes."
  type        = list(string)
  default     = ["t3.medium"] # A good balance of cost and performance for this project.
}

# ---------------------------------------------------------------------------------------------------------------------
# ECR Repository Variables
# ---------------------------------------------------------------------------------------------------------------------

variable "ecr_repository_name" {
  description = "The name for the ECR repository. Although we use a public image, this is for demonstration."
  type        = string
  default     = "nextcloud-app"
}

# ---------------------------------------------------------------------------------------------------------------------
# RDS Database Variables
# ---------------------------------------------------------------------------------------------------------------------

variable "rds_pg_version" {
  description = "The PostgreSQL engine version for the RDS instance."
  type        = string
  default     = "16.2" # Latest stable version of PostgreSQL 16.
}

variable "rds_instance_class" {
  description = "The instance class for the RDS instance. db.t3.micro is eligible for the AWS Free Tier."
  type        = string
  default     = "db.t3.micro"
}

variable "rds_allocated_storage" {
  description = "The initial allocated storage for the RDS instance (in GB)."
  type        = number
  default     = 20
}

variable "rds_master_username" {
  description = "The master username for the RDS database."
  type        = string
  default     = "nextcloudadmin"
}

variable "rds_db_name" {
  description = "The name of the database to be created within the RDS instance."
  type        = string
  default     = "nextclouddb"
}

variable "rds_multi_az_enabled" {
  description = "Specifies if the RDS instance should be deployed in a Multi-AZ configuration for high availability."
  type        = bool
  default     = true
}

==================================================
File: .\terraform\versions.tf
==================================================
terraform {
  required_version = ">= 1.12.1"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.99.1"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "~> 4.1"
    }
  }
}
